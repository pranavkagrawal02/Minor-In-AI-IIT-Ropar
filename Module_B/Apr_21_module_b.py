# -*- coding: utf-8 -*-
"""module-b.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QkhCGUTHjWt-Opif83sJejNjMwg8Ran0

**Batch 04 - Module B**

Topic Covered: (so far)
1. Linear Regression
2. Polynomial Regression
3. Logistic Regression
4. Regularization
5. Dimensionality
6. Gradient Descent
7. Page Rank
8. Support Vector Machines
9. Neural Networks and Types
10. Clustering
11. Image Processing
12. CNN


**AI Agent Releases:**
05 Versions

Link: https://docs.google.com/spreadsheets/d/1St1tl1W_H7bkwufHhh5rtiaIVVv_KvEA0HHU3xbLuLw/edit?gid=0#gid=0
"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression

# Sample data (input X and output Y)
X = np.array([1, 2, 3, 4, 5]).reshape(-1, 1)  # Input (features)
Y = np.array([2, 4, 6, 8, 10])  # Output (labels)

# Create and train the model
model = LinearRegression()
model.fit(X, Y)

# Plot the results
plt.scatter(X, Y, color='blue', label="Training Data")
plt.plot(X, model.predict(X), color='red', label="Learned Pattern")
plt.xlabel("Input (X)")
plt.ylabel("Output (Y)")
plt.legend()
plt.title("Simple Linear Regression: Learning Input-Output Relationship")
plt.show()

# Basic code
from sklearn.linear_model import LinearRegression
import matplotlib.pyplot as plt

# years of exp. and salary of 5 employees
X = [[1], [2], [3], [4], [5]]
Y = [30000, 35000, 45000, 55000, 70000]

model = LinearRegression().fit(X, Y)
plt.scatter(X, Y)
plt.plot(X, model.predict(X))
plt.show()

import numpy as np
import matplotlib.pyplot as plt

# Define different values of b (intercept) with a fixed m
b_values = [0, 2, 5]  # Different intercepts
m = 1  # Fixed slope
X = np.linspace(-5, 5, 100)  # Generate 100 values from -5 to 5

# Plot different intercepts
plt.figure(figsize=(8,6))
for b in b_values:
    Y = m * X + b
    plt.plot(X, Y, label=f"y = {m}x + {b}")

# Highlight origin and axes
plt.axhline(0, color='black', linewidth=1)
plt.axvline(0, color='black', linewidth=1)

# Labels and legend
plt.xlabel("X values")
plt.ylabel("Y values")
plt.title("Effect of Different Intercepts (b) on a Line")
plt.legend()
plt.grid()
plt.show()

import numpy as np
import matplotlib.pyplot as plt

# Define different values of m (slope) with a fixed b
m_values = [0.5, 1, 2]  # Different slopes
b = 0  # Fixed intercept
X = np.linspace(-5, 5, 100)  # Generate 100 values from -5 to 5

# Plot different slopes
plt.figure(figsize=(8,6))
for m in m_values:
    Y = m * X + b
    plt.plot(X, Y, label=f"y = {m}x + {b}")

# Highlight origin and axes
plt.axhline(0, color='black', linewidth=1)
plt.axvline(0, color='black', linewidth=1)

# Labels and legend
plt.xlabel("X values")
plt.ylabel("Y values")
plt.title("Effect of Different Slopes (m) on a Line")
plt.legend()
plt.grid()
plt.show()

import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression

# Sample data (input X and output Y)
X = np.array([1, 2, 3, 4, 5]).reshape(-1, 1)  # Input (features)
Y = np.array([2, 4, 6, 8, 10])  # Output (labels) - follows Y = 2X

# Create and train the model
model = LinearRegression()
model.fit(X, Y)

# Predict values
X_test = np.array([6, 7, 8]).reshape(-1, 1)
predictions = model.predict(X_test)

# Plot the results
plt.scatter(X, Y, color='blue', label="Training Data")
plt.plot(X, model.predict(X), color='red', label="Learned Pattern (Y=2X)")
plt.scatter(X_test, predictions, color='green', marker='x', label="Predictions")
plt.xlabel("Input (X)")
plt.ylabel("Output (Y)")
plt.legend()
plt.title("Simple Linear Regression: Learning Input-Output Relationship")
plt.show()

# Print predictions
for i, x in enumerate(X_test.flatten()):
    print(f"For input {x}, predicted output is {predictions[i]:.2f}")

import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error, mean_squared_error

# Generate synthetic data
np.random.seed(42)
X = np.random.rand(20, 1) * 10  # 20 random values between 0 and 10
y = 2 * X + 3 + np.random.randn(20, 1) * 2  # Linear function with noise

# Train a Linear Regression Model
model = LinearRegression()
model.fit(X, y)
y_pred = model.predict(X)

# Calculate MAE and MSE
mae =
mse =

# Print Results
print(f"Mean Absolute Error (MAE): {mae:.2f}")
print(f"Mean Squared Error (MSE): {mse:.2f}")

# Plotting the results
plt.scatter(X, y, color='blue', label="Actual Data")
plt.plot(X, y_pred, color='red', linewidth=2, label="Regression Line")
plt.xlabel("X (Input Feature)")
plt.ylabel("y (Target Output)")
plt.legend()
plt.title("Linear Regression with MAE & MSE Calculation")
plt.show()

# Given data points
x = [1, 2, 3, 4, 5]
y = [2, 3, 5, 4, 6]

# Compute mean of x and y
x_mean = sum(x) / len(x)
y_mean = sum(y) / len(y)

# Compute slope (m)
numerator = sum((xi - x_mean) * (yi - y_mean) for xi, yi in zip(x, y))
denominator = sum((xi - x_mean) ** 2 for xi in x)
m = numerator / denominator

# Compute intercept (c)
c = y_mean - m * x_mean

# Print results
print(f"m = {m}")
print(f"c = {c}")

import numpy as np
import matplotlib.pyplot as plt

# Given data points
x = np.array([1, 2, 3, 4, 5])
y_actual = np.array([2, 3, 5, 4, 6])

# Regression equation: y = 0.9x + 1.3
m = 0.9
c = 1.3
y_predicted = m * x + c  # Calculate predicted y values

# Calculate errors (absolute differences)
errors = abs(y_actual - y_predicted)

# Plot actual points
plt.scatter(x, y_actual, color='blue', label="Actual Points")

# Plot regression line
plt.plot(x, y_predicted, color='red', label="Regression Line (y=0.9x+1.3)")

# Draw vertical lines for errors
for i in range(len(x)):
    plt.plot([x[i], x[i]], [y_actual[i], y_predicted[i]], 'g--', alpha=0.6)

# Labels and legend
plt.xlabel("x")
plt.ylabel("y")
plt.title("Linear Regression with Error Visualization")
plt.legend()
plt.grid(True)
plt.show()

import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error, mean_squared_error

# Generate synthetic data
np.random.seed(42)
X = np.random.rand(20, 1) * 10  # 20 random values between 0 and 10
y = 2 * X + 3 + np.random.randn(20, 1) * 2  # Linear function with noise

# Train a Linear Regression Model
model = LinearRegression()
model.fit(X, y)
y_pred = model.predict(X)

# Calculate MAE and MSE
mae = mean_absolute_error(y, y_pred)
mse = mean_squared_error(y, y_pred)

# Print Results
print(f"Mean Absolute Error (MAE): {mae:.2f}")
print(f"Mean Squared Error (MSE): {mse:.2f}")

# Plotting the results
plt.scatter(X, y, color='blue', label="Actual Data")
plt.plot(X, y_pred, color='red', linewidth=2, label="Regression Line")
plt.xlabel("X (Input Feature)")
plt.ylabel("y (Target Output)")
plt.legend()
plt.title("Linear Regression with MAE & MSE Calculation")
plt.show()

import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score

# Generate nonlinear data (quadratic relationship)
np.random.seed(42)
x = np.linspace(-5, 5, 50).reshape(-1, 1)  # 50 points between -5 and 5
y = 3 * x**2 + 2 + np.random.normal(0, 3, size=x.shape)  # Quadratic with noise

# Fit a Linear Regression Model
model = LinearRegression()
model.fit(x, y)
y_pred = model.predict(x)

# Calculate R² Score
r2 = r2_score(y, y_pred)

# Plot the data and the linear fit
plt.scatter(x, y, label='True Data', color='blue')
plt.plot(x, y_pred, label='Linear Fit', color='red')
plt.title(f'Linear Regression Fit (Low R² Value: {r2:.4f})')
plt.xlabel('X')
plt.ylabel('Y')
plt.legend()
plt.show()

print(f"R² Value: {r2:.4f}")

import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score

# Real-time example: Age vs. Reaction Time
age = np.array([10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70]).reshape(-1, 1)
reaction_time = np.array([500, 480, 450, 430, 420, 415, 420, 430, 450, 480, 520, 570, 630])  # Reaction time in ms

# Fit a Linear Regression Model
model = LinearRegression()
model.fit(age, reaction_time)
reaction_time_pred = model.predict(age)

# Calculate R² Score
r2 = r2_score(reaction_time, reaction_time_pred)

# Plot the data and the linear fit
plt.scatter(age, reaction_time, label='True Data', color='blue')
plt.plot(age, reaction_time_pred, label='Linear Fit', color='red')
plt.title(f'Linear Regression Fit (Low R² Value: {r2:.4f})')
plt.xlabel('Age (years)')
plt.ylabel('Reaction Time (ms)')
plt.legend()
plt.show()

print(f"R² Value: {r2:.4f}")

import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score

# Real-time example: Age vs. Reaction Time
age = np.array([10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70]).reshape(-1, 1)
reaction_time = np.array([500, 480, 450, 430, 420, 415, 420, 430, 450, 480, 520, 570, 630])  # Reaction time in ms

# Fit a Linear Regression Model
linear_model = LinearRegression()
linear_model.fit(age, reaction_time)
reaction_time_pred_linear = linear_model.predict(age)
linear_r2 = r2_score(reaction_time, reaction_time_pred_linear)

# Transform features for Polynomial Regression (Degree = 2)
poly = PolynomialFeatures(degree=2)
age_poly = poly.fit_transform(age)
polynomial_model = LinearRegression()
polynomial_model.fit(age_poly, reaction_time)
reaction_time_pred_poly = polynomial_model.predict(age_poly)
poly_r2 = r2_score(reaction_time, reaction_time_pred_poly)

# Plot Linear and Polynomial Regression
plt.scatter(age, reaction_time, label='True Data', color='blue')
plt.plot(age, reaction_time_pred_linear, label=f'Linear Fit (R²: {linear_r2:.4f})', color='red')
plt.plot(age, reaction_time_pred_poly, label=f'Polynomial Fit (R²: {poly_r2:.4f})', color='green')
plt.title('Linear vs. Polynomial Regression')
plt.xlabel('Age (years)')
plt.ylabel('Reaction Time (ms)')
plt.legend()
plt.show()

print(f"Linear R² Value: {linear_r2:.4f}")
print(f"Polynomial R² Value: {poly_r2:.4f}")

import numpy as np
import matplotlib.pyplot as plt

# Define x values (range from -10 to 10)
x = np.linspace(-10, 10, 100)

# Generate random coefficients for each polynomial degree
np.random.seed(42)  # For reproducibility
coefficients = {
    1: np.random.uniform(-5, 5, 2),  # Degree 1 (Linear)
    2: np.random.uniform(-5, 5, 3),  # Degree 2 (Quadratic)
    3: np.random.uniform(-5, 5, 4),  # Degree 3 (Cubic)
    4: np.random.uniform(-5, 5, 5),  # Degree 4 (Quartic)
    5: np.random.uniform(-5, 5, 6),  # Degree 5 (Quintic)
}

# Create separate plots for each polynomial degree
for degree, coeffs in coefficients.items():
    y = np.polyval(coeffs, x)  # Compute y values

    plt.figure(figsize=(6, 4))  # Create a new figure for each plot
    plt.plot(x, y, label=f'Degree {degree}', color='b')

    # Plot settings
    plt.axhline(0, color='black', linewidth=0.5, linestyle="--")
    plt.axvline(0, color='black', linewidth=0.5, linestyle="--")
    plt.xlabel('x')
    plt.ylabel('y')
    plt.title(f'Polynomial Regression (Degree {degree})')
    plt.legend()
    plt.grid(True)

    plt.show()  # Show each plot separately

import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures
from sklearn.metrics import r2_score

# Day Number
x = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10]).reshape(-1, 1)

# Plant Growth
y = np.array([1, 2, 3, 5, 15, 34, 48, 70, 136, 185])

poly = PolynomialFeatures(degree = 8)
x_poly = poly.fit_transform(x)


# Create and fit the model
model = LinearRegression()
model.fit(x_poly, y)

# Predict values
y_pred = model.predict(x_poly)

# Calculate R² score
r2 = r2_score(y, y_pred)

# Print model details
print(f"Slope (coefficient): {model.coef_[0]:.2f}")
print(f"Intercept: {model.intercept_:.2f}")
print(f"R² Score: {r2:.4f}")

# Plotting
plt.scatter(x, y, color='green', label='Actual Growth')
plt.plot(x, y_pred, color='blue', linestyle='--', label=f'Fitted Line (R² = {r2:.2f})')
plt.xlabel('Day Number')
plt.ylabel('Plant Growth')
plt.title('Regression - Plant Growth Over Time')
plt.legend()
plt.grid(True)
plt.show()

# prompt: I have data of GRE score and admitted or not. fit me a polynomial regression. Assume around 10 GRE scores and status. plot the graph. compute r2

import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures
from sklearn.metrics import r2_score

# Sample data (replace with your actual data)
gre_scores = np.array([300, 310, 320, 330, 340, 350, 360, 370, 380, 390]).reshape(-1, 1)
admitted = np.array([0, 0, 0, 1, 0, 1, 1, 1, 1, 1])  # 0: not admitted, 1: admitted

# Create polynomial features
poly_features = PolynomialFeatures(degree=8)
gre_poly = poly_features.fit_transform(gre_scores)

# Fit the polynomial regression model
model = LinearRegression()
model.fit(gre_poly, admitted)

# Make predictions
gre_pred = np.linspace(min(gre_scores), max(gre_scores), 100).reshape(-1, 1)
gre_pred_poly = poly_features.fit_transform(gre_pred)
admitted_pred = model.predict(gre_pred_poly)

# Calculate R-squared
r2 = r2_score(admitted, model.predict(gre_poly))

# Plot the results
plt.scatter(gre_scores, admitted, label='Data')
plt.plot(gre_pred, admitted_pred, color='red', label=f'Polynomial Regression (degree={8})')
plt.xlabel('GRE Score')
plt.ylabel('Admitted (0 or 1)')
plt.title(f'Polynomial Regression (R-squared = {r2:.2f})')
plt.legend()
plt.show()

import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score

# Generate nonlinear data (quadratic relationship)
np.random.seed(42)
x = np.linspace(-5, 5, 50).reshape(-1, 1)  # 50 points between -5 and 5
y = 3 * x**2 + 2 + np.random.normal(0, 3, size=x.shape)  # Quadratic with noise

# Fit a Linear Regression Model
model = LinearRegression()
model.fit(x, y)
y_pred = model.predict(x)

# Calculate R² Score
r2 = r2_score(y, y_pred)

# Plot the data and the linear fit
plt.scatter(x, y, label='True Data', color='blue')
plt.plot(x, y_pred, label='Linear Fit', color='red')
plt.title(f'Linear Regression Fit (Low R² Value: {r2:.4f})')
plt.xlabel('X')
plt.ylabel('Y')
plt.legend()
plt.show()

print(f"R² Value: {r2:.4f}")

import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score

# Real-time example: Age vs. Reaction Time (Nonlinear relationship)
age = np.array([10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70]).reshape(-1, 1)
reaction_time = np.array([500, 480, 450, 430, 420, 415, 420, 430, 450, 480, 520, 570, 630])  # Reaction time in ms

# Fit a Linear Regression Model
model = LinearRegression()
model.fit(age, reaction_time)
reaction_time_pred = model.predict(age)

# Calculate R² Score
r2 = r2_score(reaction_time, reaction_time_pred)

# Plot the data and the linear fit
plt.scatter(age, reaction_time, label='True Data', color='blue')
plt.plot(age, reaction_time_pred, label='Linear Fit', color='red')
plt.title(f'Linear Regression Fit (Low R² Value: {r2:.4f})')
plt.xlabel('Age (years)')
plt.ylabel('Reaction Time (ms)')
plt.legend()
plt.show()

print(f"R² Value: {r2:.4f}")

import numpy as np
import matplotlib.pyplot as plt

# Step 1: Define the sigmoid function
def sigmoid(z):
    return 1 / (1 + np.exp(-z))

# Step 2: Generate input values from -10 to 10
z_values = np.linspace(-10, 10, 200)
sigmoid_values = sigmoid(z_values)

# Step 3: Plot the sigmoid function
plt.figure(figsize=(8, 5))
plt.plot(z_values, sigmoid_values, label="Sigmoid Function", color="darkblue")
plt.title("Sigmoid Function Curve")
plt.xlabel("Input (z)")
plt.ylabel("Sigmoid(z)")
plt.grid(True)
plt.axvline(0, color='gray', linestyle='--', alpha=0.5)
plt.axhline(0.5, color='red', linestyle='--', alpha=0.6, label='Threshold at 0.5')
plt.legend()
plt.show()

import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression

#  Create a simple dataset (Hours Studied vs Pass/Fail)
X = np.array([[1], [2], [3], [4], [5], [6], [7], [8], [9]])  # Hours studied
y = np.array([0, 0, 0, 0, 1, 1, 1, 1, 1])  # 0 = Fail, 1 = Pass

# Train a Logistic Regression model
model =
model.fit(X, y)

# Predict pass/fail for a new student who studied for given hours
# new_hours = np.array([[]])
# prediction = model.predict(new_hours)
# probability = model.predict_proba(new_hours)[0][1]  # Probability of passing

# print(f"Predicted Outcome: {'Pass' if prediction[0] == 1 else 'Fail'}")
# print(f"Probability of Passing: {probability:.2f}")

# Plot the decision boundary
X_test = np.linspace(0, 10, 100).reshape(-1, 1)  # Generate values from 0 to 10
y_prob = model.predict_proba(X_test)[:, 1]  # Get probabilities

plt.scatter(X, y, color="blue", label="Training Data")
plt.plot(X_test, y_prob, color="red", label="Sigmoid Curve")
plt.axhline(0.5, linestyle="--", color="gray", label="Decision Boundary")
plt.xlabel("Hours Studied")
plt.ylabel("Probability of Passing")
plt.legend()
plt.title("Logistic Regression: Predicting Exam Pass/Fail")
plt.show()

import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score

# Real-time example: Age vs. Reaction Time
age = np.array([10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70]).reshape(-1, 1)
reaction_time = np.array([500, 480, 450, 430, 420, 415, 420, 430, 450, 480, 520, 570, 630])  # Reaction time in ms

# Fit a Linear Regression Model
linear_model = LinearRegression()
linear_model.fit(age, reaction_time)
reaction_time_pred_linear = linear_model.predict(age)
linear_r2 = r2_score(reaction_time, reaction_time_pred_linear)

# Transform features for Polynomial Regression (Degree = 2)
poly = PolynomialFeatures(degree=2)
age_poly = poly.fit_transform(age)
polynomial_model = LinearRegression()
polynomial_model.fit(age_poly, reaction_time)
reaction_time_pred_poly = polynomial_model.predict(age_poly)
poly_r2 = r2_score(reaction_time, reaction_time_pred_poly)

# Plot Linear and Polynomial Regression
plt.scatter(age, reaction_time, label='True Data', color='blue')
plt.plot(age, reaction_time_pred_linear, label=f'Linear Fit (R²: {linear_r2:.4f})', color='red')
plt.plot(age, reaction_time_pred_poly, label=f'Polynomial Fit (R²: {poly_r2:.4f})', color='green')
plt.title('Linear vs. Polynomial Regression')
plt.xlabel('Age (years)')
plt.ylabel('Reaction Time (ms)')
plt.legend()
plt.show()

print(f"Linear R² Value: {linear_r2:.4f}")
print(f"Polynomial R² Value: {poly_r2:.4f}")

import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score

# Real-time example: Age vs. Reaction Time
age = np.array([10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70]).reshape(-1, 1)
reaction_time = np.array([500, 480, 450, 430, 420, 415, 420, 430, 450, 480, 520, 570, 630])  # Reaction time in ms

# Fit a Linear Regression Model
linear_model = LinearRegression()
linear_model.fit(age, reaction_time)
reaction_time_pred_linear = linear_model.predict(age)
linear_r2 = r2_score(reaction_time, reaction_time_pred_linear)

# Fit Polynomial Regression Models with Different Degrees
degrees = [2, 4, 6]
plt.scatter(age, reaction_time, label='True Data', color='blue')
colors = ['green', 'purple', 'orange']

for i, degree in enumerate(degrees):
    poly = PolynomialFeatures(degree=degree)
    age_poly = poly.fit_transform(age)
    polynomial_model = LinearRegression()
    polynomial_model.fit(age_poly, reaction_time)
    reaction_time_pred_poly = polynomial_model.predict(age_poly)
    poly_r2 = r2_score(reaction_time, reaction_time_pred_poly)

    plt.plot(age, reaction_time_pred_poly, label=f'Poly Deg {degree} (R²: {poly_r2:.4f})', color=colors[i])
    print(f"Polynomial Degree {degree} R² Value: {poly_r2:.4f}")

# Plot Linear Regression
plt.plot(age, reaction_time_pred_linear, label=f'Linear Fit (R²: {linear_r2:.4f})', color='red')

plt.title('Model Complexity: Underfitting vs. Overfitting')
plt.xlabel('Age (years)')
plt.ylabel('Reaction Time (ms)')
plt.legend()
plt.show()

print(f"Linear R² Value: {linear_r2:.4f}")

import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

# Sample Nonlinear Data
X = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10]).reshape(-1, 1)
y = np.array([2, 6, 15, 30, 55, 90, 140, 210, 300, 410])  # Quadratic pattern

# Transform X for Polynomial Regression (Degree 2)
poly = PolynomialFeatures(degree=2)
X_poly = poly.fit_transform(X)

# Train the Polynomial Regression Model
model = LinearRegression().fit(X_poly, y)

# Predictions
y_pred = model.predict(X_poly)

# Evaluation Metrics
mse = mean_squared_error(y, y_pred)
mae = mean_absolute_error(y, y_pred)
r2 = r2_score(y, y_pred)

# Print Metrics
print("Mean Squared Error (MSE):", mse)
print("Mean Absolute Error (MAE):", mae)
print("R² Score:", r2)

# Plot the Results
plt.scatter(X, y, color='blue', label="Actual Data")
plt.plot(X, y_pred, color='red', label="Polynomial Regression")
plt.xlabel("X")
plt.ylabel("y")
plt.title("Polynomial Regression with Performance Metrics")
plt.legend()
plt.show()

import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression

#  Create a simple dataset (Hours Studied vs Pass/Fail)
X = np.array([[1], [2], [3], [4], [5], [6], [7], [8], [9]])  # Hours studied
y = np.array([0, 0, 0, 0, 1, 1, 1, 1, 1])  # 0 = Fail, 1 = Pass

# Train a Logistic Regression model
model = LinearRegression()
model.fit(X, y)

y_pred = model.predict(X)

# Plotting
plt.scatter(X, y, color='red', label='Actual Growth')
plt.plot(X, y_pred, color='blue', linestyle='--')
plt.xlabel('Hours Studies')
plt.ylabel('Pass/Fail')
plt.title('Student Results')
plt.legend()
plt.grid(True)
plt.show()

import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression
from sklearn.datasets import make_classification

# Step 1: Create a simple dataset (2 features, 2 classes)
X, y = make_classification(n_samples=100, n_features=2, n_redundant=0,
                           n_informative=2, n_clusters_per_class=1, random_state=1)

# Step 2: Fit logistic regression model
model = LogisticRegression()
model.fit(X, y)

# Step 3: Plot the data points
plt.figure(figsize=(8, 6))
plt.scatter(X[:, 0], X[:, 1], c=y, cmap='bwr', edgecolors='k')
plt.title("Logistic Regression Demo")
plt.xlabel("Category 1")
plt.ylabel("Category 2")

# Step 4: Plot the decision boundary
x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),
                     np.linspace(y_min, y_max, 200))

Z = model.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)

plt.contourf(xx, yy, Z, alpha=0.3, cmap='bwr')
plt.grid(True)
plt.show()

import numpy as np
import matplotlib.pyplot as plt

# Step 1: Define the sigmoid function
def sigmoid(z):
  return 1/(1 + np.exp(-z))

# Step 2: Generate input values from -10 to 10
z_values = np.linspace(-10, 10, 200)
sigmoid_values = sigmoid(z_values)

# Step 3: Plot the sigmoid function
plt.figure(figsize=(8, 5))
plt.plot(z_values, sigmoid_values, label="Sigmoid Function", color="darkblue")
plt.title("Sigmoid Function Curve")
plt.xlabel("Input (z)")
plt.ylabel("Sigmoid(z)")
plt.grid(True)
plt.axvline(0, color='gray', linestyle='--', alpha=0.5)
plt.axhline(0.5, color='red', linestyle='--', alpha=0.6, label='Threshold at 0.5')
plt.legend()
plt.show()

import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression, LogisticRegression

# Sample data: Hours studied vs marks (for linear) and pass/fail (for logistic)
X = np.array([[1], [2], [3], [4], [5], [6], [7], [8], [9], [10]])
y_regression = np.array([35, 40, 50, 55, 60, 65, 70, 75, 85, 90])  # marks
y_classification = np.array([0, 0, 0, 0, 0, 1, 1, 1, 1, 1])        # pass/fail

# Train the model
linear_model = LinearRegression()
linear_model.fit(X, y_regression)
y_linear_pred = linear_model.predict(X)

# Train the model
logistic_model = LogisticRegression()
logistic_model.fit(X, y_classification)
x_range = np.linspace(0, 11, 300).reshape(-1, 1)
y_logistic_prob = logistic_model.predict_proba(x_range)[:, 1]

# Plotting
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))

# Linear Regression Plot
ax1.scatter(X, y_regression, color='blue', label='Actual Marks')
ax1.plot(X, y_linear_pred, color='red', label='Linear Fit')
ax1.set_title('Linear Regression: Hours vs Marks')
ax1.set_xlabel('Hours Studied')
ax1.set_ylabel('Marks Scored')
ax1.legend()
ax1.grid(True)

# Logistic Regression Plot
ax2.scatter(X, y_classification, color='green', label='Actual Pass/Fail')
ax2.plot(x_range, y_logistic_prob, color='purple', label='Logistic Curve')
ax2.axhline(0.5, color='gray', linestyle='--', label='Threshold = 0.5')
ax2.set_title('Logistic Regression: Hours vs Pass Probability')
ax2.set_xlabel('Hours Studied')
ax2.set_ylabel('Probability of Passing')
ax2.legend()
ax2.grid(True)

plt.tight_layout()
plt.show()

import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression
import ipywidgets as widgets
from IPython.display import display

# Data
gre_scores = np.array([290, 300, 310, 320, 330, 340, 295, 305, 315, 325])
admissions = np.array([0, 0, 0, 1, 1, 1, 0, 0, 1, 1])

X = gre_scores.reshape(-1, 1)
y = admissions

# Model
model = LogisticRegression()
model.fit(X, y)

# Widget
gre_input = widgets.IntSlider(
    value=310,
    min=280,
    max=350,
    step=1,
    description='GRE Score:',
    continuous_update=False
)

# Function to update plot and prediction
def update_plot(gre_val):
    prediction = model.predict([[gre_val]])[0]
    prob = model.predict_proba([[gre_val]])[0][1]

    print(f"\nGRE Score: {gre_val}")
    print(f"Prediction: {'Admitted' if prediction else 'Not Admitted'}")
    print(f"Probability of Admission: {prob:.2f}")

    # Plot sigmoid
    gre_range = np.linspace(280, 350, 300).reshape(-1, 1)
    predicted_probs = model.predict_proba(gre_range)[:, 1]

    plt.figure(figsize=(8, 5))
    plt.scatter(gre_scores, admissions, color='red', edgecolors='k', label='Training Data')
    plt.plot(gre_range, predicted_probs, color='blue', label='Sigmoid Curve')
    plt.axvline(gre_val, color='green', linestyle='--', label='Your GRE')
    plt.xlabel("GRE Score")
    plt.ylabel("Probability of Admission")
    plt.title("Logistic Regression Prediction")
    plt.legend()
    plt.grid(True)
    plt.show()

# Display the widget
widgets.interact(update_plot, gre_val=gre_input)

import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression

# Sample data: GRE, CGPA, Admission (1 = Admitted, 0 = Not Admitted)
data = np.array([
    [310, 8.0, 0],
    [320, 8.5, 1],
    [330, 9.0, 1],
    [300, 7.5, 0],
    [340, 9.5, 1],
    [305, 6.5, 0],
    [315, 7.8, 0],
    [325, 8.7, 1],
    [310, 9.0, 1],
    [295, 6.8, 0]
])

# Split features and labels
X = data[:, :2]  # GRE and CGPA
y = data[:, 2]   # Admission

# Train logistic regression model
model = LogisticRegression()
model.fit(X, y)

# Take user input
gre_input = float(input("Enter your GRE score: "))
cgpa_input = float(input("Enter your CGPA: "))
user_input = np.array([[gre_input, cgpa_input]])

# Predict admission
prediction = model.predict(user_input)
probability = model.predict_proba(user_input)[0][1]

# Output result
print("\nPrediction Result:")
print("Admitted" if prediction[0] == 1 else "Not Admitted ")
print(f"Probability of Admission: {probability:.2f}")

# Plotting
plt.figure(figsize=(8,6))
for admitted in [0, 1]:
    subset = X[y == admitted]
    label = 'Admitted' if admitted == 1 else 'Not Admitted'
    color = 'green' if admitted == 1 else 'red'
    plt.scatter(subset[:, 0], subset[:, 1], label=label, c=color, edgecolor='k')

# User point
plt.scatter(gre_input, cgpa_input, c='blue', s=100, edgecolors='k', label="You")

plt.xlabel("GRE Score")
plt.ylabel("CGPA")
plt.title("University Admission Prediction")
plt.legend()
plt.grid(True)
plt.show()

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression

# Sample data: GRE, CGPA, Admitted
data = [
    [310, 8.0, 1],
    [300, 7.5, 0],
    [320, 8.6, 1],
    [315, 8.2, 1],
    [299, 7.0, 0],
    [325, 9.0, 1],
    [280, 6.5, 0],
    [330, 9.1, 1],
    [340, 9.5, 1],
    [290, 7.2, 0],
]

df = pd.DataFrame(data, columns=["GRE", "CGPA", "Admitted"])

# Features and label
X = df[["GRE", "CGPA"]]
y = df["Admitted"]

# Train logistic regression model
model = LogisticRegression()
model.fit(X, y)

# Extract weights
w1, w2 = model.coef_[0]
b = model.intercept_[0]

print(f"\nModel Weights:")
print(f"w1 (GRE): {w1:.4f}")
print(f"w2 (CGPA): {w2:.4f}")
print(f"Intercept (b): {b:.4f}")

# Take user input
gre_input = float(input("\nEnter GRE score: "))
cgpa_input = float(input("Enter CGPA: "))
user_input = pd.DataFrame({"GRE": [gre_input], "CGPA": [cgpa_input]})

# Prediction
probability = model.predict_proba(user_input)[0][1]
prediction = model.predict(user_input)[0]

print(f"\nPredicted probability of admission: {probability:.4f}")
print("Prediction:", "Admitted ✅" if prediction == 1 else "Not Admitted ❌")

# Plot decision boundary
gre_vals = np.linspace(280, 340, 100)
cgpa_vals = np.linspace(6.0, 10.0, 100)
GRE_grid, CGPA_grid = np.meshgrid(gre_vals, cgpa_vals)
grid_df = pd.DataFrame({
    "GRE": GRE_grid.ravel(),
    "CGPA": CGPA_grid.ravel()
})
Z = model.predict_proba(grid_df)[:, 1].reshape(GRE_grid.shape)

plt.contourf(GRE_grid, CGPA_grid, Z, levels=[0, 0.5, 1], alpha=0.3, colors=["red", "green"])
plt.scatter(df["GRE"], df["CGPA"], c=df["Admitted"], cmap="bwr", edgecolors='k')
plt.scatter(gre_input, cgpa_input, color='gold', s=100, edgecolors='black', label="Your Input")
plt.xlabel("GRE Score")
plt.ylabel("CGPA")
plt.title("Admission Prediction using Logistic Regression")
plt.legend()
plt.grid(True)
plt.show()

# REGULARIZATION
#===============

# The Obervations we made in the spreadsheet, how does it boil down in the code?
# How do we compute the lasso and ridge values using inbuilt libraries?

import numpy as np
from sklearn.linear_model import LinearRegression, Ridge, Lasso

# X has two features (like hours of study and sleep)
X = np.array([[1, 5], [2, 5], [3, 5], [4, 6]])
# y is the score
y = np.array([2, 4, 7, 8])

# Linear Regression (no regularization)
lr = LinearRegression()
lr.fit(X, y)

# Ridge Regression (L2 Regularization)
ridge = Ridge(alpha=1.0)
ridge.fit(X, y)

# Lasso Regression (L1 Regularization)
lasso = Lasso(alpha=1.0)
lasso.fit(X, y)

print("Linear Regression Coefficients:", lr.coef_)
print("Ridge Regression Coefficients :", ridge.coef_)
print("Lasso Regression Coefficients :", lasso.coef_)

# Working on our case study and finding R2 score

import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures
from sklearn.metrics import r2_score

# Input features: hours of study and sleep
X = np.array([[1, 5], [2, 5], [3, 5], [4, 6]])
y = np.array([2, 4, 7, 8])  # Target: score


# 1. Linear Regression (no regularization)
lr = LinearRegression()
lr.fit(X,y)
y_pred_linear = lr.predict(X)
print("Linear Regression R² Score:", r2_score(y, y_pred_linear))


# 2. Polynomial Regression
poly = PolynomialFeatures(degree=2)
X_poly = poly.fit_transform(X)
poly_lr = LinearRegression()
poly_lr.fit(X_poly, y)
y_pred_poly = poly_lr.predict(X_poly)
print("Polynomial Regression R² Score:", r2_score(y, y_pred_poly))

# Predicting from our case study

import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures
from sklearn.metrics import r2_score

# Input features: hours of study and sleep
X = np.array([[1, 5], [2, 5], [3, 5], [4, 6]])
y = np.array([2, 4, 7, 8])  # Target: score

# 1. Linear Regression (no regularization)
lr = LinearRegression()
lr.fit(X, y)
y_pred_linear = lr.predict(X)

print("Linear Regression Coefficients:", lr.coef_)
print("Linear Regression R² Score:", r2_score(y, y_pred_linear))

# 2. Polynomial Regression
poly = PolynomialFeatures(degree=2)
X_poly = poly.fit_transform(X)

poly_lr = LinearRegression()
poly_lr.fit(X_poly, y)
y_pred_poly = poly_lr.predict(X_poly)

print("Polynomial Regression R² Score:", r2_score(y, y_pred_poly))

# 3. Predict on a new sample
new_sample = np.array([[5,6]])
new_sample_poly = poly.transform(new_sample)

print("Prediction on new sample:")
print("- Linear:", lr.predict(new_sample))
print("- Polynomial:", poly_lr.predict(new_sample_poly))

# Working with all possilble cases
import numpy as np
from sklearn.linear_model import LinearRegression, Lasso, Ridge
from sklearn.preprocessing import PolynomialFeatures
from sklearn.metrics import r2_score

# Input features: hours of study and sleep
X = np.array([[1, 5], [2, 5], [3, 5], [4, 6]])
y = np.array([2, 4, 7, 8])  # Target: score

# 1. Linear Regression (no regularization)
lr = LinearRegression()
lr.fit(X, y)
y_pred_linear = lr.predict(X)

# 2. Ridge Regression (linear features)
ridge_linear = Ridge(alpha=1.0)
ridge_linear.fit(X, y)
y_pred_ridge_linear = ridge_linear.predict(X)

# 3. Lasso Regression (linear features)
lasso_linear = Lasso(alpha=1.0)
lasso_linear.fit(X, y)
y_pred_lasso_linear = lasso_linear.predict(X)

print("=== Linear Features ===")
print("Linear Regression Coefficients:", lr.coef_)
print("Linear R² Score:", r2_score(y, y_pred_linear))
print("Ridge Coefficients:", ridge_linear.coef_)
print("Ridge R² Score:", r2_score(y, y_pred_ridge_linear))
print("Lasso Coefficients:", lasso_linear.coef_)
print("Lasso R² Score:", r2_score(y, y_pred_lasso_linear))

# 4. Polynomial Regression (degree 2)
poly = PolynomialFeatures(degree=2)
X_poly = poly.fit_transform(X)

# 4a. Linear Regression on Polynomial Features
poly_lr = LinearRegression()
poly_lr.fit(X_poly, y)
y_pred_poly = poly_lr.predict(X_poly)

# 4b. Ridge Regression on Polynomial Features
ridge_poly = Ridge(alpha=1.0)
ridge_poly.fit(X_poly, y)
y_pred_ridge_poly = ridge_poly.predict(X_poly)

# 4c. Lasso Regression on Polynomial Features
lasso_poly = Lasso(alpha=1.0, max_iter=10000)
lasso_poly.fit(X_poly, y)
y_pred_lasso_poly = lasso_poly.predict(X_poly)

print("\n=== Polynomial Features ===")
print("Polynomial Regression Coefficients:", poly_lr.coef_)
print("Polynomial R² Score:", r2_score(y, y_pred_poly))
print("Ridge Coefficients:", ridge_poly.coef_)
print("Ridge R² Score:", r2_score(y, y_pred_ridge_poly))
print("Lasso Coefficients:", lasso_poly.coef_)
print("Lasso R² Score:", r2_score(y, y_pred_lasso_poly))

# 5. Prediction on new sample [5, 6]
new_sample = np.array([[5, 6]])
new_sample_poly = poly.transform(new_sample)

print("\n=== Prediction on [5, 6] ===")
print("Linear:", lr.predict(new_sample))
print("Linear Ridge:", ridge_linear.predict(new_sample))
print("Linear Lasso:", lasso_linear.predict(new_sample))
print("Polynomial:", poly_lr.predict(new_sample_poly))
print("Poly Ridge:", ridge_poly.predict(new_sample_poly))
print("Poly Lasso:", lasso_poly.predict(new_sample_poly))

# What do we take away from this?
#---------------------------------

from sklearn.linear_model import LinearRegression, Lasso, Ridge
from sklearn.metrics import mean_squared_error
import numpy as np

# Sample dataset (6 data points, 2 features)
X = np.array([
    [1, 1],
    [2, 2],
    [3, 3],
    [4, 4],
    [5, 5],
    [6, 6]
])

# Output (some linear relation + noise)
y = np.array([3, 5, 7, 9, 11, 13])

# Model 1: Linear Regression (no regularization)
lr = LinearRegression()
lr.fit(X, y)
y_pred_lr = lr.predict(X)
print("Linear Regression Coefficients:", lr.coef_)
print("MSE:", mean_squared_error(y, y_pred_lr))

# Model 2: Ridge Regression
ridge = Ridge(alpha=1.0)
ridge.fit(X, y)
y_pred_ridge = ridge.predict(X)
print("\nRidge Coefficients:", ridge.coef_)
print("MSE (Ridge):", mean_squared_error(y, y_pred_ridge))

# Model 3: Lasso Regression
lasso = Lasso(alpha=1.0)
lasso.fit(X, y)
y_pred_lasso = lasso.predict(X)
print("\nLasso Coefficients:", lasso.coef_)
print("MSE (Lasso):", mean_squared_error(y, y_pred_lasso))

# Visualization of given problem

import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.metrics import mean_squared_error

# Sample dataset (6 points, 2 features)
X = np.array([
    [1, 1],
    [2, 2],
    [3, 3],
    [4, 4],
    [5, 5],
    [6, 6]
])

# Target variable (roughly y = 1*x1 + 1*x2)
y = np.array([3, 5, 7, 9, 11, 13])

# Train all three models
lr = LinearRegression().fit(X, y)
ridge = Ridge(alpha=1.0).fit(X, y)
lasso = Lasso(alpha=1.0).fit(X, y)

# Predictions
y_pred_lr = lr.predict(X)
y_pred_ridge = ridge.predict(X)
y_pred_lasso = lasso.predict(X)

# Print coefficients and errors
print("Linear Regression Coefficients:", lr.coef_, "MSE:", mean_squared_error(y, y_pred_lr))
print("Ridge Regression Coefficients:", ridge.coef_, "MSE:", mean_squared_error(y, y_pred_ridge))
print("Lasso Regression Coefficients:", lasso.coef_, "MSE:", mean_squared_error(y, y_pred_lasso))

# Plotting predictions
plt.figure(figsize=(10, 6))
plt.plot(y, label='True Values', marker='o', linewidth=2)
plt.plot(y_pred_lr, label='Linear Regression', marker='s')
plt.plot(y_pred_ridge, label='Ridge (α=1.0)', marker='^')
plt.plot(y_pred_lasso, label='Lasso (α=1.0)', marker='x')

plt.title("Comparison of Linear, Ridge, and Lasso Regression")
plt.xlabel("Sample Index")
plt.ylabel("Predicted Value")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

# Lets add R2
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.metrics import mean_squared_error, r2_score

# Sample dataset
X = np.array([
    [1, 1],
    [2, 2],
    [3, 3],
    [4, 4],
    [5, 5],
    [6, 6]
])

y = np.array([3, 5, 7, 9, 11, 13])

# Fit models
lr = LinearRegression().fit(X, y)
ridge = Ridge(alpha=1.0).fit(X, y)
lasso = Lasso(alpha=1.0).fit(X, y)

# Predictions
y_pred_lr = lr.predict(X)
y_pred_ridge = ridge.predict(X)
y_pred_lasso = lasso.predict(X)

# Print metrics
print("=== Linear Regression ===")
print("Coefficients:", lr.coef_)
print("MSE:", mean_squared_error(y, y_pred_lr))
print("R² Score:", r2_score(y, y_pred_lr))

print("\n=== Ridge Regression ===")
print("Coefficients:", ridge.coef_)
print("MSE:", mean_squared_error(y, y_pred_ridge))
print("R² Score:", r2_score(y, y_pred_ridge))

print("\n=== Lasso Regression ===")
print("Coefficients:", lasso.coef_)
print("MSE:", mean_squared_error(y, y_pred_lasso))
print("R² Score:", r2_score(y, y_pred_lasso))

# Lets add prediction

import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.metrics import mean_squared_error, r2_score

# === Step 1: Create Dataset ===
X = np.array([
    [1, 1],
    [2, 2],
    [3, 3],
    [4, 4],
    [5, 5],
    [6, 6]
])
y = np.array([3, 5, 7, 9, 11, 13])  # Simple linear target: y = x1 + x2 + 1

# === Step 2: Train Models ===
lr = LinearRegression().fit(X, y)
ridge = Ridge(alpha=1.0).fit(X, y)
lasso = Lasso(alpha=1.0).fit(X, y)

# === Step 3: Make Predictions on Training Data ===
y_pred_lr = lr.predict(X)
y_pred_ridge = ridge.predict(X)
y_pred_lasso = lasso.predict(X)

# === Step 4: Show Coefficients, MSE, and R² Scores ===
print("=== Linear Regression ===")
print("Coefficients:", lr.coef_)
print("Intercept:", lr.intercept_)
print("MSE:", mean_squared_error(y, y_pred_lr))
print("R² Score:", r2_score(y, y_pred_lr))

print("\n=== Ridge Regression ===")
print("Coefficients:", ridge.coef_)
print("Intercept:", ridge.intercept_)
print("MSE:", mean_squared_error(y, y_pred_ridge))
print("R² Score:", r2_score(y, y_pred_ridge))

print("\n=== Lasso Regression ===")
print("Coefficients:", lasso.coef_)
print("Intercept:", lasso.intercept_)
print("MSE:", mean_squared_error(y, y_pred_lasso))
print("R² Score:", r2_score(y, y_pred_lasso))

# === Step 5: Plot the Predictions ===
plt.figure(figsize=(10, 6))
plt.plot(y, label='True Values', marker='o', linewidth=2)
plt.plot(y_pred_lr, label='Linear Regression', marker='s')
plt.plot(y_pred_ridge, label='Ridge (α=1.0)', marker='^')
plt.plot(y_pred_lasso, label='Lasso (α=1.0)', marker='x')
plt.title("Predictions: Linear vs Ridge vs Lasso")
plt.xlabel("Sample Index")
plt.ylabel("Predicted Value")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

# === Step 6: Predict on New Input ===
new_sample = np.array([[7, 7]])

pred_lr = lr.predict(new_sample)
pred_ridge = ridge.predict(new_sample)
pred_lasso = lasso.predict(new_sample)

print("\n=== Prediction for New Input [7, 7] ===")
print("Linear Regression Prediction:", pred_lr[0])
print("Ridge Regression Prediction:", pred_ridge[0])
print("Lasso Regression Prediction:", pred_lasso[0])

# Where is the lowest Point?
import numpy as np
import matplotlib.pyplot as plt

# Bowl-shaped curve
x = np.linspace(-3, 3, 100)
y = x**2

# Plot curve
plt.plot(x, y, label="y = x²", color='darkgreen')

# Mark the lowest point
plt.plot(0, 0, 'ro')
plt.text(0, 0.5, "Lowest point", ha='center', color='red')

# Add question
plt.title("Where’s the Lowest Point?")

plt.xlabel("x")
plt.ylabel("y")
plt.grid(True)
plt.legend()
plt.show()

import numpy as np
import matplotlib.pyplot as plt
import matplotlib.animation as animation
from IPython.display import HTML

# Define function and its gradient
def f(x):
    return x**2

def grad_f(x):
    return 2 * x

# Hiker A
x0_a = 3.0
alpha_a = 0.1

# Hiker B
x0_b = -2.5
alpha_b = 0.05

steps = 30

# Gradient Descent Paths
x_vals_a = [x0_a]
x_vals_b = [x0_b]

for _ in range(steps):
    x_next_a = x_vals_a[-1] - alpha_a * grad_f(x_vals_a[-1])
    x_next_b = x_vals_b[-1] - alpha_b * grad_f(x_vals_b[-1])
    x_vals_a.append(x_next_a)
    x_vals_b.append(x_next_b)

y_vals_a = [f(x) for x in x_vals_a]
y_vals_b = [f(x) for x in x_vals_b]

# Set up the figure
fig, ax = plt.subplots(figsize=(8, 6))
x_curve = np.linspace(-4, 4, 400)
y_curve = f(x_curve)
ax.plot(x_curve, y_curve, 'gray', label="y = x²")

point_a, = ax.plot([], [], 'ro', label='Hiker A (α=0.1)')
point_b, = ax.plot([], [], 'bo', label='Hiker B (α=0.05)')

text_a = ax.text(-3.5, 10, '', fontsize=9, color='red')
text_b = ax.text(-3.5, 8.5, '', fontsize=9, color='blue')

ax.set_xlim(-4, 4)
ax.set_ylim(0, 12)
ax.set_xlabel("x")
ax.set_ylabel("y")
ax.set_title("Gradient Descent: Two Hikers Going Down the Valley")
ax.grid(True)
ax.legend()

# Animation update function
def update(i):
    point_a.set_data([x_vals_a[i]], [y_vals_a[i]])
    point_b.set_data([x_vals_b[i]], [y_vals_b[i]])
    text_a.set_text(f"Hiker A Step {i}: x={x_vals_a[i]:.2f}")
    text_b.set_text(f"Hiker B Step {i}: x={x_vals_b[i]:.2f}")
    return point_a, point_b, text_a, text_b

ani = animation.FuncAnimation(fig, update, frames=steps+1, interval=400, blit=False)

# Show animation inline in Colab
HTML(ani.to_jshtml())

import numpy as np
import matplotlib.pyplot as plt

# Function and its derivative
def f(x):
    return x**2

def grad_f(x):
    return 2 * x

# Starting point
x_start = 2.5

# Learning rates to compare
learning_rates = [0.01, 0.1, 0.5]
colors = ['blue', 'green', 'red']
labels = ['Small Step Hiker (0.01)', 'Medium Step Hiker (0.1)', 'Large Step Hiker (0.5)']

# Plot the function
x = np.linspace(-3, 3, 400)
y = f(x)

plt.figure(figsize=(10, 6))
plt.plot(x, y, 'k-', label='y = x²')
plt.axhline(0, color='gray', lw=0.5)
plt.axvline(0, color='gray', lw=0.5)

# Show each learning rate path with arrows
for lr, color, label in zip(learning_rates, colors, labels):
    x_vals = [x_start]
    for _ in range(5):
        x_new = x_vals[-1] - lr * grad_f(x_vals[-1])
        x_vals.append(x_new)

    y_vals = [f(xi) for xi in x_vals]
    plt.plot(x_vals, y_vals, 'o-', color=color, label=label)

    # Draw arrows showing steps
    for i in range(len(x_vals) - 1):
        dx = x_vals[i+1] - x_vals[i]
        dy = y_vals[i+1] - y_vals[i]
        plt.arrow(x_vals[i], y_vals[i], dx, dy, color=color, head_width=0.1, length_includes_head=True)

# Final formatting
plt.xlabel("x")
plt.ylabel("y = x²")
plt.title("Step Size and Direction")
plt.legend()
plt.grid(True)
plt.show()

# A quadratic cost function: y = (x - 3)^2
import matplotlib.pyplot as plt

def func(x):
    return (x - 3)**2

def grad(x):
    return 2*(x - 3)

x = 0  # starting point
lr = 0.15
steps = []
for i in range(20):
    x = x - lr * grad(x)
    steps.append(x)

xs = [i/10 for i in range(-10, 60)]
ys = [func(i) for i in xs]

plt.plot(xs, ys, label='Function')
plt.plot(steps, [func(i) for i in steps], 'ro--', label='Steps')
plt.legend()
plt.title("Gradient Descent on a Simple Function")
plt.xlabel("x")
plt.ylabel("f(x)")
plt.grid()
plt.show()

import numpy as np
import matplotlib.pyplot as plt
from ipywidgets import interact
import ipywidgets as widgets

# Function and its gradient
def func(x):
    return (x - 3)**2

def grad(x):
    return 2*(x - 3)

# Main plot function
def plot_gradient_descent(lr=0.1):
    x = 0
    steps = [x]
    for _ in range(20):
        x = x - lr * grad(x)
        steps.append(x)

    xs = np.linspace(-1, 6, 100)
    ys = func(xs)

    plt.figure(figsize=(8, 5))
    plt.plot(xs, ys, label="Function: (x - 3)^2")
    plt.plot(steps, [func(s) for s in steps], 'ro--', label="Gradient Descent Steps")
    plt.title(f"Gradient Descent | Learning Rate: {lr}")
    plt.xlabel("x")
    plt.ylabel("f(x)")
    plt.legend()
    plt.grid(True)
    plt.show()

# Create slider
interact(plot_gradient_descent, lr=widgets.FloatSlider(value=0.1, min=0.01, max=1.0, step=0.01))

import numpy as np
import matplotlib.pyplot as plt

# Function and derivative
def f(x):
    return (x - 3)**2

def f_prime(x):
    return 2 * (x - 3)

# x range for plotting the function
x_vals = np.linspace(-1, 7, 300)
y_vals = f(x_vals)

# Points of interest
points = [1.5, 3, 4.5]  # x < 3, x = 3, x > 3

# Plot the function
plt.figure(figsize=(10, 6))
plt.plot(x_vals, y_vals, label='$f(x) = (x - 3)^2$', color='blue')
plt.axvline(3, linestyle='--', color='gray', alpha=0.6)

# Plot tangent lines at selected points
for x0 in points:
    slope = f_prime(x0)
    y0 = f(x0)
    tangent_x = np.linspace(x0 - 1, x0 + 1, 10)
    tangent_y = slope * (tangent_x - x0) + y0
    plt.plot(tangent_x, tangent_y, linestyle='--', label=f'Slope at x={x0:.1f}: {slope:.1f}')
    plt.scatter(x0, y0, color='red')

    # Annotate slope direction
    direction = '↓ (negative)' if slope < 0 else ('↑ (positive)' if slope > 0 else '→ (zero)')
    plt.annotate(f"Slope = {slope:.1f}\n{direction}", (x0, y0 + 0.5), fontsize=10, ha='center')

# Plot settings
plt.title("Visualizing Slope of $f(x) = (x - 3)^2$")
plt.xlabel("x")
plt.ylabel("f(x)")
plt.legend()
plt.grid(True)
plt.show()

import numpy as np
import matplotlib.pyplot as plt

# Simple data
x = np.array([1, 2, 3])
y = np.array([2, 4, 6])

# Mean Squared Error Loss
def compute_loss(w):
    y_pred = w * x
    return np.mean((y - y_pred) ** 2)

# Try various w values
w_values = np.linspace(0, 4, 100)
loss_values = [compute_loss(w) for w in w_values]

# Plotting
plt.figure(figsize=(10, 6))
plt.plot(w_values, loss_values, label="Loss vs Weight", color='purple')
plt.axvline(x=2, linestyle='--', color='green', label='Minimum Loss at w=2')
plt.scatter([2], [compute_loss(2)], color='red', zorder=5)
plt.title("Mapping Model Parameter (w) to Error (Loss)")
plt.xlabel("Weight (w)")
plt.ylabel("Loss (MSE)")
plt.grid(True)
plt.legend()
plt.show()

from sklearn.linear_model import SGDRegressor
import numpy as np

x = np.array([[1], [2], [3], [4]])
y = np.array([2, 4, 6, 8])

model = SGDRegressor(max_iter = 1000, learning_rate = 'constant', eta0=0.01)
model.fit(x,y)

print("Coefficient:", model.coef_[0])

import numpy as np
from sklearn.linear_model import Lasso, Ridge, SGDRegressor
from sklearn.metrics import mean_squared_error

# Simple dataset
x = np.array([[1], [2], [3], [4], [5]])
y = np.array([1.2, 2.4, 3.1, 4.3, 5.1])

# Lasso Regression
lasso = Lasso(alpha=0.1)
lasso.fit(x, y)
print("Lasso Coefficient:", lasso.coef_[0])

# Ridge Regression
ridge = Ridge(alpha=0.1)
ridge.fit(x, y)
print("Ridge Coefficient:", ridge.coef_[0])

# SGDRegressor
sgd = SGDRegressor(penalty='l2', alpha=0.1, max_iter=1000, learning_rate='constant', eta0=0.01)
sgd.fit(x, y)
print("SGD Coefficient:", sgd.coef_[0])



# prompt: Write me a code for page rank. use a simple data set of 3 to 4 pages. show me the page ranks of each iteration. don't approximate the rank values.

import numpy as np

def page_rank(adj_matrix, damping_factor=0.85, iterations=100):
  """
  Calculates PageRank for a given adjacency matrix.

  Args:
    adj_matrix: A NumPy array representing the adjacency matrix of the web graph.
    damping_factor: The damping factor (probability of following a link).
    iterations: The number of iterations to perform.

  Returns:
    A NumPy array containing the PageRank values for each page.
  """
  num_pages = len(adj_matrix)
  page_ranks = np.ones(num_pages) / num_pages  # Initialize PageRank values

  for iteration in range(iterations):
    print(f"Iteration {iteration + 1}:")
    new_page_ranks = np.zeros(num_pages)
    for i in range(num_pages):
        for j in range(num_pages):
            if adj_matrix[j][i] == 1:
                out_degree = np.sum(adj_matrix[j])
                if out_degree > 0 :
                    new_page_ranks[i] += damping_factor * (page_ranks[j] / out_degree)
                else:
                    new_page_ranks[i] += damping_factor * (page_ranks[j] / num_pages)

        new_page_ranks[i] += (1 - damping_factor) / num_pages

    page_ranks = new_page_ranks
    print(page_ranks)

  return page_ranks

# Example usage:
# Adjacency matrix for a simple web graph with 4 pages
# Page 1 links to pages 2 and 3
# Page 2 links to page 1
# Page 3 links to page 4
# Page 4 links to page 3

adjacency_matrix = np.array([
    [0, 1, 1, 0],
    [1, 0, 0, 0],
    [0, 0, 0, 1],
    [0, 0, 1, 0]
])

final_ranks = page_rank(adjacency_matrix)

print("\nFinal PageRanks:", final_ranks)

# The first Implementation of PageRank
# !pip install python-igraph
from igraph import Graph

g = Graph(directed=True)
g.add_vertices(3)
g.add_edges([(0,1), (0,2), (1,2), (2,0)])
pr = g.pagerank()
print(pr)

# Twitter Followers
# Install igraph if not already installed
#!pip install python-igraph

from igraph import Graph
from igraph import plot
import matplotlib.pyplot as plt

# Step 1: Create a directed graph (Twitter-style follow graph)
# Each directed edge (A, B) means "A follows B"
users = ["Deepanshu", "Zoya", "Tinku", "Aryan", "Laxman"]
edges = [
    ("Deepanshu", "Zoya"),
    ("Deepanshu", "Tinku"),
    ("Deepanshu", "Aryan"), ("Aryan", "Deepanshu"),
    ("Zoya", "Tinku"),
    ("Tinku", "Aryan"),
    ("Aryan", "Laxman")]

# Step 2: Build the igraph Graph
g = Graph(directed=True)
g.add_vertices(users)
g.add_edges(edges)

# Step 3: Compute PageRank (higher = more influential)
page_ranks = g.pagerank(damping = 0.85)

# Step 4: Display results
print("Influence Scores (PageRank):")
for user, score in zip(users, page_ranks):
    print(f"{user}: {score:.4f}")

# Step 5: Visualize the graph
layout = g.layout("fr")  # Force-directed layout
fig, ax = plt.subplots(figsize=(8,6))
g.vs["label"] = users
g.vs["size"] = [20 + 100 * pr for pr in page_ranks]  # Size nodes by rank
g.vs["color"] = "skyblue"
g.es["arrow_size"] = 0.5
plot(g, layout=layout, vertex_label=g.vs["label"], target=ax)
plt.title("Twitter-style Follower Network with PageRank")
plt.show()

# Zachary's Karate Club - Relationship Graph
import networkx as nx
import matplotlib.pyplot as plt

# Load the built-in Karate Club graph from NetworkX
G = nx.karate_club_graph()

# Draw the graph
plt.figure(figsize=(10, 8))
pos = nx.spring_layout(G, seed=42)  # For consistent layout

# Draw nodes
nx.draw_networkx_nodes(G, pos, node_size=700, node_color='skyblue', edgecolors='black')

# Draw edges (relationships)
nx.draw_networkx_edges(G, pos, width=2)

# Draw node labels
nx.draw_networkx_labels(G, pos, font_size=12, font_color='black')

# Title and display
plt.title("Zachary's Karate Club - Relationship Graph", fontsize=16)
plt.axis('off')
plt.show()

import networkx as nx
import numpy as np
import matplotlib.pyplot as plt

# Load the graph
G = nx.karate_club_graph()

# Parameters
damping_factor = 0.85
max_iter = 100
tol = 1e-6

# Initialize
N = G.number_of_nodes()
pr = np.ones(N) / N  # Initial PageRank values
adj = nx.to_numpy_array(G)  # Adjacency matrix

# Normalize the adjacency matrix
out_degree = adj.sum(axis=1)
transition_matrix = adj / out_degree[:, None]

# PageRank iteration
history = []
for i in range(max_iter):
    new_pr = (1 - damping_factor) / N + damping_factor * transition_matrix.T @ pr
    delta = np.abs(new_pr - pr).sum()
    history.append((i + 1, new_pr.copy(), delta))
    pr = new_pr
    if delta < tol:
        print(f"Converged at iteration {i+1} with total change {delta:.6f}")
        break

# Display results
print("\nIteration-wise PageRank Scores:")
for i, scores, delta in history:
    print(f"Iter {i:2d} | Δ = {delta:.6f} | Top 3 nodes: {np.argsort(scores)[-3:][::-1]}")

# Plot convergence
deltas = [h[2] for h in history]
plt.figure(figsize=(8, 4))
plt.plot(range(1, len(deltas)+1), deltas, marker='o')
plt.xlabel('Iteration')
plt.ylabel('Total Change (Δ)')
plt.title('Convergence of PageRank on Karate Club Graph')
plt.grid(True)
plt.show()

import networkx as nx

# Load the Karate Club dataset (built into NetworkX)
G = nx.karate_club_graph()

# Compute the PageRank
pagerank = nx.pagerank(G, alpha=0.85)  # alpha is the damping factor (default is 0.85)

# Display the nodes with the highest PageRank scores
sorted_pagerank = sorted(pagerank.items(), key=lambda x: x[1], reverse=True)
top_n = sorted_pagerank[:34]
print("All nodes by PageRank:")
for node, score in top_n:
    print(f"Node {node}: {score}")

# Optional: Visualize the graph
import matplotlib.pyplot as plt

plt.figure(figsize=(8, 8))
nx.draw(G, with_labels=True, node_size=500, font_size=10, font_color='red', node_color='pink')
plt.title("Karate Club Graph with PageRank")
plt.show()

import networkx as nx
import matplotlib.pyplot as plt

# Step 1: Create a directed graph
G =

# Step 2: Add endorsements (A -> B means A endorsed B)
endorsements = [

]

G.add_edges_from()

# Step 3: Compute PageRank


# Step 4: Display results (without f-strings)
print("LinkedIn-style Endorsement PageRank Scores:")
for user, score in pageranks.items():
    print(user + ": " + str(round(score, 4)))

# Step 5: Visualize the graph
plt.figure(figsize=(8, 6))
pos = nx.spring_layout(G)
node_sizes = [5000 * pageranks[node] for node in G.nodes()]

nx.draw_networkx_nodes(G, pos, node_size=node_sizes, node_color="lightgreen")
nx.draw_networkx_edges(G, pos, arrows=True)
nx.draw_networkx_labels(G, pos)

plt.title("LinkedIn Endorsement Network (Node size = Influence)")
plt.show()

# The curious Case - What do you take away?
import networkx as nx
import matplotlib.pyplot as plt

# Create the "failing" graph: A → B → C → D
G = nx.DiGraph()
edges = [('A', 'B'), ('B', 'C'), ('C', 'D')]
G.add_edges_from(edges)

# Run PageRank
pagerank = nx.pagerank(G, alpha=0.85)

# Show PageRank values
print("PageRank Scores:")
for node, score in pagerank.items():
    print(f"{node}: {score:.4f}")

# Visualize
plt.figure(figsize=(8, 4))
pos = nx.spring_layout(G, seed=42)

# Draw nodes with PageRank-based size
node_sizes = [pagerank[n] * 5000 for n in G.nodes()]
nx.draw(G, pos, with_labels=True, node_size=node_sizes, node_color='lightcoral', arrows=True, edge_color='gray')

# Annotate PageRank values
for node, (x, y) in pos.items():
    plt.text(x, y + 0.08, f"{pagerank[node]:.4f}", ha='center', fontsize=10, color='black')

plt.title(" PageRank Behavior on a Dangling Chain (A → B → C → D)")
plt.axis('off')
plt.show()

# The story of Apples and Oranges
# Where are we heading at?

# What does that look like in code?
import matplotlib.pyplot as plt
import numpy as np
from sklearn.svm import SVC

# Red and orange points
red_points = np.array([
    [2, 4], [3, 3], [4, 5], [3, 5], [2.5, 4], [3.2, 4.8], [4.1, 4.5], [2.8, 3.7]
])
orange_points = np.array([
    [6, 1], [7, 2], [8, 2], [7, 1], [6.5, 2], [7.5, 1.8], [8.1, 1.9], [6.8, 1.5]
])

# Combine data
X = np.vstack((red_points, orange_points))
y = np.array([0]*len(red_points) + [1]*len(orange_points))

# Train linear SVM
model = SVC(kernel='linear', C=1)
model.fit(X, y)

# Extract model parameters
w = model.coef_[0]
b = model.intercept_[0]
slope = -w[0] / w[1]
intercept = -b / w[1]

# Create a grid to draw hyperplane and margins
xx = np.linspace(1, 9, 100)
yy = slope * xx + intercept

# Margin calculation
margin = 1 / np.sqrt(np.sum(w ** 2))
yy_down = yy - np.sqrt(1 + slope**2) * margin
yy_up = yy + np.sqrt(1 + slope**2) * margin

# Plotting
plt.figure(figsize=(8, 6))
plt.scatter(red_points[:, 0], red_points[:, 1], color='red')
plt.scatter(orange_points[:, 0], orange_points[:, 1], color='orange')
plt.plot(xx, yy, 'k-')
plt.plot(xx, yy_down, 'k--')
plt.plot(xx, yy_up, 'k--')
plt.title('The apples and oranges')
plt.xlabel('X-axis')
plt.ylabel('Y-axis')
plt.grid(True)
plt.show()

# Code with Labels
import matplotlib.pyplot as plt
import numpy as np
from sklearn.svm import SVC

# More red and orange points
red_points = np.array([
    [2, 4], [3, 3], [4, 5], [3, 5], [2.5, 4], [3.2, 4.8], [4.1, 4.5], [2.8, 3.7]
])
orange_points = np.array([
    [6, 1], [7, 2], [8, 2], [7, 1], [6.5, 2], [7.5, 1.8], [8.1, 1.9], [6.8, 1.5]
])

# Combine data
X = np.vstack((red_points, orange_points))
y = np.array([0]*len(red_points) + [1]*len(orange_points))

# Train linear SVM
model = SVC(kernel='linear', C=1)
model.fit(X, y)

# Extract model parameters
w = model.coef_[0]
b = model.intercept_[0]
slope = -w[0] / w[1]
intercept = -b / w[1]

# Create a grid to draw hyperplane and margins
xx = np.linspace(1, 9, 100)
yy = slope * xx + intercept

# Margin calculation
margin = 1 / np.sqrt(np.sum(w ** 2))
yy_down = yy - np.sqrt(1 + slope**2) * margin
yy_up = yy + np.sqrt(1 + slope**2) * margin

# Plotting
plt.figure(figsize=(8, 6))
plt.scatter(red_points[:, 0], red_points[:, 1], color='red', label='Apples')
plt.scatter(orange_points[:, 0], orange_points[:, 1], color='orange', label='Oranges')
plt.plot(xx, yy, 'k-', label='Hyperplane')
plt.plot(xx, yy_down, 'k--', label='Margin')
plt.plot(xx, yy_up, 'k--')
plt.legend()
plt.title('SVM: Hyperplane and Margins')
plt.xlabel('X-axis')
plt.ylabel('Y-axis')
plt.grid(True)
plt.show()

from sklearn import datasets
from sklearn.svm import SVC
import matplotlib.pyplot as plt

# Load a small dataset with only two features
X, y = datasets.make_blobs(n_samples=20, centers=2, random_state=6)

# Train a linear SVM
model = SVC(kernel='linear')
model.fit(X, y)

# Plot the data points
plt.scatter(X[:, 0], X[:, 1], c=y, cmap='autumn', edgecolors='k')

# Plot the decision boundary (hyperplane)
ax = plt.gca()
xlim = ax.get_xlim()
ylim = ax.get_ylim()

# Create a grid to evaluate the model
xx = np.linspace(xlim[0], xlim[1], 30)
yy = np.linspace(ylim[0], ylim[1], 30)
YY, XX = np.meshgrid(yy, xx)
xy = np.vstack([XX.ravel(), YY.ravel()]).T
Z = model.decision_function(xy).reshape(XX.shape)

# Plot decision boundary and margins
ax.contour(XX, YY, Z, colors='k', levels=[-1, 0, 1], linestyles=['--', '-', '--'])

# Plot support vectors
ax.scatter(model.support_vectors_[:, 0], model.support_vectors_[:, 1],
           s=100, linewidth=1, facecolors='none', edgecolors='k', label='Support Vectors')

plt.title("Simple SVM Demo")
plt.legend()
plt.show()

import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets

# Generate non-linear sample data (circles)
X, y = datasets.make_circles(n_samples=200, factor=0.3, noise=0.05, random_state=42)

# Plot the raw data only
plt.figure(figsize=(6, 6))
plt.scatter(X[:, 0], X[:, 1], c=y, cmap='autumn', edgecolors='k')
plt.title("Sample Circular Data")
plt.xlabel("Feature 1")
plt.ylabel("Feature 2")
plt.grid(True)
plt.show()

# What if we visualize it in multiple dimensions?
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_circles
from mpl_toolkits.mplot3d import Axes3D

# Generate the same circular data
X, y = make_circles(n_samples=200, factor=0.3, noise=0.05, random_state=42)

# Apply a manual transformation to lift the data into 3D
# Let's use: z = x^2 + y^2 (a common feature map for circles)
x1 = X[:, 0]
x2 = X[:, 1]
z = x1**2 + x2**2

# Plot in 3D
fig = plt.figure(figsize=(8, 6))
ax = fig.add_subplot(111, projection='3d')

# Color map: red (0) and orange (1)
colors = np.where(y == 0, 'red', 'orange')

ax.scatter(x1, x2, z, c=colors, edgecolor='k')
ax.set_xlabel('X1')
ax.set_ylabel('X2')
ax.set_zlabel('Z = X1² + X2²')
ax.set_title('Data Transformed to 3D (Z = X1² + X2²)')
plt.tight_layout()
plt.show()

# Understanding the 4 kernels
import numpy as np
import matplotlib.pyplot as plt
from sklearn import svm, datasets

# Generate non-linear sample data
X, y = datasets.make_circles(n_samples=200, factor=0.3, noise=0.05, random_state=42)

# Set up the SVM classifiers with different kernels
kernels = {
    'Linear': svm.SVC(kernel='linear'),
    'Polynomial': svm.SVC(kernel='poly', degree = 5),
    'Radial Basis': svm.SVC(kernel='rbf'),
    'Sigmoid': svm.SVC(kernel='sigmoid')
}

# Train all models
models = {}
for name, clf in kernels.items():
    clf.fit(X, y)
    models[name] = clf

# Plotting
plt.figure(figsize=(12, 10))
for i, (name, model) in enumerate(models.items(), 1):
    plt.subplot(2, 2, i)

    # Plot decision boundary
    ax = plt.gca()
    xlim = ax.get_xlim()
    ylim = ax.get_ylim()

    xx, yy = np.meshgrid(np.linspace(-1.5, 1.5, 500),
                         np.linspace(-1.5, 1.5, 500))
    Z = model.decision_function(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)

    plt.contourf(xx, yy, Z, levels=np.linspace(Z.min(), 0, 20), cmap='autumn', alpha=0.5)
    plt.contourf(xx, yy, Z, levels=[0, Z.max()], colors='lightblue', alpha=0.5)
    plt.contour(xx, yy, Z, levels=[0], linewidths=2, colors='k')  # decision boundary

    plt.scatter(X[:, 0], X[:, 1], c=y, cmap='autumn', edgecolors='k')
    plt.title(name)
    plt.xticks(())
    plt.yticks(())

plt.tight_layout()
plt.suptitle("SVM Decision Boundaries with Different Kernels", fontsize=16, y=1.02)
plt.show()

# Working with IRIS data set
import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report

# Load Iris dataset
iris = datasets.load_iris()
X = iris.data[:, :2]  # Use only first two features for easy 2D visualization
y = iris.target

# Split into train/test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)

# Standardize features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Train SVM
model = SVC(kernel='rbf', gamma='auto')
model.fit(X_train, y_train)

# Predict
y_pred = model.predict(X_test)

# Evaluation
print("Classification Report:\n", classification_report(y_test, y_pred))

# Plot decision boundaries
def plot_decision_boundary(X, y, model, title):
    h = .02
    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                         np.arange(y_min, y_max, h))
    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)

    plt.figure(figsize=(8, 6))
    plt.contourf(xx, yy, Z, alpha=0.3)
    plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k', cmap='viridis')
    plt.title(title)
    plt.xlabel('Sepal length (standardized)')
    plt.ylabel('Sepal width (standardized)')
    plt.grid(True)
    plt.show()

plot_decision_boundary(X_train, y_train, model, "SVM with RBF Kernel (Iris Dataset)")

# SVM with penguin data set

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from matplotlib.cbook import get_sample_data
import seaborn as sns

# Load penguin dataset
penguins = sns.load_dataset("penguins")

# Drop rows with missing values
penguins.dropna(inplace=True)

# Select features and labels
X = penguins[['bill_length_mm', 'flipper_length_mm']].values
y = penguins['species'].values

# Encode species labels
le = LabelEncoder()
y = le.fit_transform(y)

# Split into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Standardize the features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Train the SVM model
model = SVC(kernel='rbf', gamma='scale')
model.fit(X_train, y_train)

# Function to plot decision boundaries
def plot_decision_boundary(X, y, model, title):
    h = 0.01
    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                         np.arange(y_min, y_max, h))
    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)

    plt.figure(figsize=(8, 6))
    plt.contourf(xx, yy, Z, alpha=0.3, cmap='coolwarm')
    scatter = plt.scatter(X[:, 0], X[:, 1], c=y, cmap='coolwarm', edgecolors='k')
    handles, _ = scatter.legend_elements()
    plt.legend(handles=list(handles), labels=list(le.classes_))
    plt.title(title)
    plt.xlabel("Bill Length (standardized)")
    plt.ylabel("Flipper Length (standardized)")
    plt.grid(True)
    plt.show()

# Plotting
plot_decision_boundary(X_train, y_train, model, "SVM on Penguin Dataset (RBF Kernel)")

import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.svm import SVC
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split

# Load dataset and filter for only 2 species
penguins = sns.load_dataset("penguins").dropna()
penguins = penguins[penguins['species'].isin(['Adelie', 'Gentoo'])]

# Features and target
X = penguins[['bill_length_mm', 'flipper_length_mm']].values
y = penguins['species'].values

# Encode target labels (Adelie=0, Gentoo=1)
le = LabelEncoder()
y = le.fit_transform(y)

# Standardize features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Train/test split
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)

# Train SVM with RBF kernel
model = SVC(kernel='rbf', gamma='scale')
model.fit(X_train, y_train)

# Plotting decision boundary with support vectors
def plot_decision_boundary(X, y, model, title):
    h = 0.01
    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                         np.arange(y_min, y_max, h))
    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)

    plt.figure(figsize=(8, 6))
    plt.contourf(xx, yy, Z, alpha=0.3, cmap='coolwarm')
    scatter = plt.scatter(X[:, 0], X[:, 1], c=y, cmap='coolwarm', edgecolors='k')

    # Highlight the support vectors
    plt.scatter(model.support_vectors_[:, 0], model.support_vectors_[:, 1],
                facecolors='none', edgecolors='k', s=100, linewidths=1.5, label="Support Vectors")

    # Separate legends: one for the data points and one for the support vectors
    handles, labels = scatter.legend_elements()
    plt.legend(handles=handles, labels=list(le.classes_))
    plt.legend(handles=[plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='k', markersize=10, label="Support Vectors")],
               loc='upper right')

    plt.title(title)
    plt.xlabel("Bill Length")
    plt.ylabel("Flipper Length")
    plt.grid(True)
    plt.show()

# Plot decision boundary with support vectors
plot_decision_boundary(X_train, y_train, model, "SVM with RBF Kernel (Adelie vs Gentoo)")

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler

# Load and clean penguins dataset
penguins = sns.load_dataset('penguins')
penguins = penguins.dropna()
penguins = penguins[penguins['species'].isin(['Adelie', 'Gentoo'])]

# Features and labels
X = penguins[['bill_length_mm', 'flipper_length_mm']].values
y = penguins['species'].values
le = LabelEncoder()
y = le.fit_transform(y)  # 0 = Adelie, 1 = Gentoo

# Standardize
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, random_state=42)

# Train SVM with RBF kernel
model = SVC(kernel='rbf', C=1.0, gamma='scale')
model.fit(X_train, y_train)

# Plotting function with margins
def plot_svm_with_margins(X, y, model, title):
    plt.figure(figsize=(8, 6))

    # Create mesh grid
    h = 0.02
    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                         np.arange(y_min, y_max, h))

    # Compute decision function over mesh
    Z = model.decision_function(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)

    # Plot decision boundary and margins
    plt.contourf(xx, yy, Z > 0, alpha=0.3, cmap=plt.cm.coolwarm)
    contour = plt.contour(xx, yy, Z, levels=[-1, 0, 1], linestyles=['--', '-', '--'],
                          colors='k', linewidths=1.5)

    # Plot data points
    scatter = plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm, edgecolors='k')

    # Plot support vectors
    sv = model.support_vectors_
    plt.scatter(sv[:, 0], sv[:, 1], s=100, facecolors='none',
                edgecolors='black', linewidth=1.5, label='Support Vectors')

    plt.xlabel('Bill Length (scaled)')
    plt.ylabel('Flipper Length (scaled)')
    plt.title(title)
    plt.legend()
    plt.show()

plot_svm_with_margins(X_train, y_train, model, "SVM with Decision Boundary, Margins & Support Vectors")

# Class Data on Purchase Pattern
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.metrics import classification_report

# Load CSV
df = pd.read_csv('batch-data.csv')
X = df[['age', 'cost']].values
y = df['purchase'].values

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)

# Feature scaling
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# SVM model with RBF kernel
model = SVC(kernel='rbf', C=1.0, gamma='auto')
model.fit(X_train_scaled, y_train)

# Plot with decision boundary, margins, and support vectors
def plot_svm_margins(X, y, model, title):
    h = 0.01
    x_min, x_max = X[:, 0].min()-1, X[:, 0].max()+1
    y_min, y_max = X[:, 1].min()-1, X[:, 1].max()+1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                         np.arange(y_min, y_max, h))

    Z = model.decision_function(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)

    plt.figure(figsize=(8, 6))
    plt.contourf(xx, yy, Z > 0, alpha=0.3, cmap='coolwarm')
    plt.contour(xx, yy, Z, levels=[-1, 0, 1], linestyles=['--', '-', '--'], colors='k')

    plt.scatter(X[:, 0], X[:, 1], c=y, cmap='coolwarm', edgecolors='k')
    plt.scatter(model.support_vectors_[:, 0], model.support_vectors_[:, 1],
                s=100, facecolors='none', edgecolors='blue', label='Support Vectors')
    plt.title(title)
    plt.xlabel('Age (scaled)')
    plt.ylabel('Cost (scaled)')
    plt.legend()
    plt.show()

plot_svm_margins(X_train_scaled, y_train, model, "SVM with RBF Kernel\n(Support Vectors & Margins)")

# Classification report
y_pred = model.predict(X_test_scaled)
print("Classification Report:\n", classification_report(y_test, y_pred))

# Purchase Pattern and Prediction
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.metrics import classification_report

# Load CSV
df = pd.read_csv('batch-data.csv')
X = df[['age', 'cost']].values
y = df['purchase'].values

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)

# Feature scaling
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# SVM model with RBF kernel
model = SVC(kernel='rbf', C=1.0, gamma='auto')
model.fit(X_train_scaled, y_train)

# New data point to predict
new_tuple = np.array([[23,4000]])
new_tuple_scaled = scaler.transform(new_tuple)
predicted_class = model.predict(new_tuple_scaled)[0]

# Plot function with decision boundary, margins, support vectors, and new point
def plot_svm_margins_with_new_point(X, y, model, new_point_scaled, title):
    h = 0.01
    x_min, x_max = X[:, 0].min()-1, X[:, 0].max()+1
    y_min, y_max = X[:, 1].min()-1, X[:, 1].max()+1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                         np.arange(y_min, y_max, h))

    Z = model.decision_function(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)

    plt.figure(figsize=(8, 6))
    plt.contourf(xx, yy, Z > 0, alpha=0.3, cmap='coolwarm')
    plt.contour(xx, yy, Z, levels=[-1, 0, 1], linestyles=['--', '-', '--'], colors='k')

    plt.scatter(X[:, 0], X[:, 1], c=y, cmap='coolwarm', edgecolors='k', label='Train Data')
    plt.scatter(model.support_vectors_[:, 0], model.support_vectors_[:, 1],
                s=100, facecolors='none', edgecolors='blue', label='Support Vectors')

    # New point marker
    plt.scatter(new_point_scaled[0][0], new_point_scaled[0][1],
                color='green', marker='X', s=150, label='New Tuple')

    plt.title(title)
    plt.xlabel('Age (scaled)')
    plt.ylabel('Cost (scaled)')
    plt.legend()
    plt.show()

# Plot
plot_svm_margins_with_new_point(X_train_scaled, y_train, model, new_tuple_scaled,
                                "SVM with RBF Kernel\n(Support Vectors, Margins, and New Tuple)")

# Classification report
y_pred = model.predict(X_test_scaled)
print("Classification Report:\n", classification_report(y_test, y_pred))

print(f"\nPrediction: {predicted_class} (0 = Not Purchase, 1 = Purchase)")

# Evaluation Metrics

import pandas as pd
from sklearn.metrics import confusion_matrix, classification_report

# Load CSV
df = pd.read_csv('classification_results.csv')
y_true = df['actual']
y_pred = df['predicted']

# Confusion matrix and metrics
print("Confusion Matrix:")
cm = confusion_matrix(y_true, y_pred)
print(cm)

# Calculating Each Term

tn, fp, fn, tp = cm.ravel()
accuracy = (tp + tn) / (tp + tn + fp + fn)
precision = tp / (tp + fp)
recall = tp / (tp + fn)
f1 = 2 * (precision * recall) / (precision + recall)
print("Accuracy", accuracy)
print("Precision", precision)
print("Recall", recall)
print("F1 Score", f1)

#Bank Data Set from UCI

# Importing libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score

# Load the Banknote dataset from UCI
df = pd.read_csv('data_banknote_authentication.txt')
df.columns = ['variance', 'skewness', 'curtosis', 'entropy', 'class']

# We'll use only two features for visualization
X = df[['variance', 'skewness']].values
y = df['class'].values

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)

# Standardize features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Train SVM with RBF kernel
model = SVC(kernel='rbf', C=1.0, gamma='auto')
model.fit(X_train_scaled, y_train)

# Predictions
y_pred = model.predict(X_test_scaled)

# Metrics
print("Accuracy:", accuracy_score(y_test, y_pred))
print("Precision:", precision_score(y_test, y_pred))
print("Recall:", recall_score(y_test, y_pred))
print("F1 Score:", f1_score(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))

# Plotting decision boundaries, margins, and support vectors
def plot_svm_margins(X, y, model, title):
    h = 0.01
    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                         np.arange(y_min, y_max, h))

    Z = model.decision_function(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)

    plt.figure(figsize=(8, 6))
    plt.contourf(xx, yy, Z > 0, alpha=0.3, cmap='coolwarm')
    plt.contour(xx, yy, Z, levels=[-1, 0, 1], linestyles=['--', '-', '--'], colors='k')

    plt.scatter(X[:, 0], X[:, 1], c=y, cmap='coolwarm', edgecolors='k')
    plt.scatter(model.support_vectors_[:, 0], model.support_vectors_[:, 1],
                s=100, facecolors='none', edgecolors='blue', label='Support Vectors')

    plt.xlabel('Variance (scaled)')
    plt.ylabel('Skewness (scaled)')
    plt.title(title)
    plt.legend()
    plt.show()

# Visualize
plot_svm_margins(X_train_scaled, y_train, model, "SVM with RBF Kernel on Banknote Data")

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Load the updated bank loan dataset
# Load CSV
df = pd.read_csv('/content/bank_loan_data.csv')  # Update path if needed

# Feature selection and target
X = df[['Annual_Income', 'Credit_Score']].values
y = df['Loan_Approved'].values

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)

# Feature scaling
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Define parameter ranges
C_values = [0.01, 0.1, 1, 10, 100]
gamma_values = [0.01, 0.1, 1]

# Store results
results = []

for gamma in gamma_values:
    for C in C_values:
        model = SVC(kernel='rbf', C=C, gamma=gamma)
        model.fit(X_train_scaled, y_train)
        y_pred = model.predict(X_test_scaled)

        acc = accuracy_score(y_test, y_pred)
        prec = precision_score(y_test, y_pred, zero_division=0)
        rec = recall_score(y_test, y_pred, zero_division=0)
        f1 = f1_score(y_test, y_pred, zero_division=0)

        results.append({
            'C': C,
            'gamma': gamma,
            'Accuracy': acc,
            'Precision': prec,
            'Recall': rec,
            'F1 Score': f1
        })

# Convert to DataFrame for display
results_df = pd.DataFrame(results)
results_df = results_df[['C', 'gamma', 'Accuracy', 'Precision', 'Recall', 'F1 Score']]
results_df.sort_values(by=['gamma', 'C'], inplace=True)
results_df.reset_index(drop=True, inplace=True)
results_df.round(3)

# Regularization Revision
import pandas as pd
from sklearn.linear_model import LinearRegression

# 1. Create the dataset
data = {
    'individual_study_hours': [1, 2, 3, 4, 5, 6],
    'group_study_hours': [3, 3, 3, 3, 4, 4],
    'marks_scored': [56, 60, 65, 66, 75, 87]
}

df = pd.DataFrame(data)
print(df)

# 2. Define X and y
X = df[['individual_study_hours', 'group_study_hours']]
y = df['marks_scored']

# 3. Fit Linear Regression
lr = LinearRegression()
lr.fit(X, y)

# 4. Coefficients and intercept
print("\nCoefficients:", lr.coef_)
print("Intercept:", lr.intercept_)

# Regularization Revision
import pandas as pd
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score

# 1. Create the dataset
data = {
    'individual_study_hours': [1,2,3,4,5,6],
    'group_study_hours': [3,3,3,3,4,4],
    'marks_scored': [56, 60, 65, 66, 75, 87]
}

df = pd.DataFrame(data)
print(df)

# 2. Define X and y
X = df[['individual_study_hours', 'group_study_hours']]
y = df['marks_scored']

# 3. Fit Linear Regression
lr = LinearRegression()
lr.fit(X, y)

# 4. Coefficients and intercept
print("\nCoefficients:", lr.coef_)
print("Intercept:", lr.intercept_)

# 5. Predict and compute R²
y_pred = lr.predict(X)
r2 = r2_score(y, y_pred)
print("\nR2 Score:", r2 )

# 6. Show predictions
print("\nPredictions:", y_pred)

import numpy as np
import numpy as np

coefs = lr.coef_
alpha = 1

# Lasso Penalty with alpha
lasso_penalty = alpha * np.sum(np.abs(coefs))

# Ridge Penalty with alpha
ridge_penalty = alpha * np.sum(coefs**2)

print("Lasso Penalty (with alpha=1):", lasso_penalty)
print("Ridge Penalty (with alpha=1):", ridge_penalty)

# Regularization Revision
import pandas as pd
from sklearn.linear_model import LinearRegression, Lasso, Ridge
from sklearn.metrics import r2_score

# 1. Create the dataset
data = {
    'individual_study_hours': [1,2,3,4,5,6],
    'group_study_hours': [3,3,3,3,4,4],
    'marks_scored': [56, 60, 65, 66, 75, 87]
}

df = pd.DataFrame(data)
print(df)

# 2. Define X and y
X = df[['individual_study_hours', 'group_study_hours']]
y = df['marks_scored']

# 3. Fit Linear Regression
lr = LinearRegression()
lr.fit(X, y)

# 4. Coefficients and intercept for Linear Regression
print("\nLinear Regression Coefficients:", lr.coef_)
print("Linear Regression Intercept:", lr.intercept_)

# 5. Fit Lasso Regression
lasso = Lasso(alpha=1)
lasso.fit(X, y)

# 6. Coefficients and intercept for Lasso Regression
print("\nLasso Regression Coefficients:", lasso.coef_)
print("Lasso Regression Intercept:", lasso.intercept_)

# 7. Fit Ridge Regression
ridge = Ridge(alpha=1)
ridge.fit(X, y)

# 8. Coefficients and intercept for Ridge Regression
print("\nRidge Regression Coefficients:", ridge.coef_)
print("Ridge Regression Intercept:", ridge.intercept_)

# 9. Make predictions with the models
y_pred_lr = lr.predict(X)
y_pred_lasso = lasso.predict(X)
y_pred_ridge = ridge.predict(X)

# 10. Compute R² for each model
r2_lr = r2_score(y, y_pred_lr)
r2_lasso = r2_score(y, y_pred_lasso)
r2_ridge = r2_score(y, y_pred_ridge)

print("\nR² Score for Linear Regression:", r2_lr)
print("R² Score for Lasso Regression:", r2_lasso)
print("R² Score for Ridge Regression:", r2_ridge)

import pandas as pd
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score

# 1. Create the dataset
data = {
    'individual_study_hours': [1,2,3,4,5,6],
    'group_study_hours': [3,3,3,3,4,4],
    'marks_scored': [56, 60, 65, 66, 75, 87]
}

df = pd.DataFrame(data)
print(df)

# 2. Define X and y
X = df[['individual_study_hours', 'group_study_hours']]
y = df['marks_scored']

# 3. Fit Linear Regression (No Regularization)
lr = LinearRegression()
lr.fit(X, y)

# 4. Coefficients and intercept for Linear Regression
print("\nLinear Regression Coefficients:", lr.coef_)
print("Linear Regression Intercept:", lr.intercept_)

# 5. Make a prediction for a new data point
new_data_point = pd.DataFrame([[7,3]], columns=['individual_study_hours', 'group_study_hours'])

# Predict the score for the new data point
predicted_score = lr.predict(new_data_point)

# 6. Print the predicted score
print("\nPredicted Marks:", predicted_score[0])

# 7. Compute R² to evaluate the model's performance
y_pred = lr.predict(X)
r2 = r2_score(y, y_pred)
print("\nR² Score:", r2)

import pandas as pd
from sklearn.linear_model import LinearRegression, Lasso, Ridge
from sklearn.metrics import r2_score

# 1. Create the dataset
data = {
    'individual_study_hours': [1,2,3,4,5,6],
    'group_study_hours': [3,3,3,3,4,4],
    'marks_scored': [56, 60, 65, 66, 75, 87]
}

df = pd.DataFrame(data)
print(df)

# 2. Define X and y
X = df[['individual_study_hours', 'group_study_hours']]
y = df['marks_scored']

# 3. Fit Linear Regression (No Regularization)
lr = LinearRegression()
lr.fit(X, y)

# 4. Coefficients and intercept for Linear Regression
print("\nLinear Regression Coefficients:", lr.coef_)
print("Linear Regression Intercept:", lr.intercept_)

# 5. Fit Lasso Regression
lasso = Lasso()
lasso.fit(X, y)
print("\nLasso Regression Coefficients:", lasso.coef_)
print("Lasso Regression Intercept:", lasso.intercept_)

# 6. Fit Ridge Regression
ridge = Ridge()
ridge.fit(X, y)
print("\nRidge Regression Coefficients:", ridge.coef_)
print("Ridge Regression Intercept:", ridge.intercept_)

# 7. Make a prediction for a new data point
new_data_point = pd.DataFrame([[7,3]], columns=['individual_study_hours', 'group_study_hours'])

# Predict the score for the new data point using Linear Regression
lr_pred = lr.predict(new_data_point)
# Predict the score for the new data point using Lasso
lasso_pred = lasso.predict(new_data_point)
# Predict the score for the new data point using Ridge
ridge_pred = ridge.predict(new_data_point)

# 8. Print the predicted scores for the new data point
print("\nPredicted Marks:")
print(f"Linear Regression Prediction: {lr_pred[0]}")
print(f"Lasso Regression Prediction: {lasso_pred[0]}")
print(f"Ridge Regression Prediction: {ridge_pred[0]}")

# 9. Compute R² for each model
lr_r2 = r2_score(y, lr.predict(X))
lasso_r2 = r2_score(y, lasso.predict(X))
ridge_r2 = r2_score(y, ridge.predict(X))

print("\nR² Score for Linear Regression:", lr_r2)
print("R² Score for Lasso Regression:", lasso_r2)
print("R² Score for Ridge Regression:", ridge_r2)

# Revision

import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.metrics import mean_squared_error, r2_score

# === Step 1: Create Dataset ===
X = np.array([
    [1, 1],
    [2, 2],
    [3, 3],
    [4, 4],
    [5, 5],
    [6, 6]
])
y = np.array([3, 5, 7, 9, 11, 13])  # Simple linear target: y = x1 + x2 + 1

# === Step 2: Train Models ===
lr = LinearRegression().fit(X, y)
ridge = Ridge(alpha=1.0).fit(X, y)
lasso = Lasso(alpha=1.0).fit(X, y)

# === Step 3: Make Predictions on Training Data ===
y_pred_lr = lr.predict(X)
y_pred_ridge = ridge.predict(X)
y_pred_lasso = lasso.predict(X)

# === Step 4: Show Coefficients, MSE, and R² Scores ===
print("=== Linear Regression ===")
print("Coefficients:", lr.coef_)
print("Intercept:", lr.intercept_)
print("MSE:", mean_squared_error(y, y_pred_lr))
print("R² Score:", r2_score(y, y_pred_lr))

print("\n=== Ridge Regression ===")
print("Coefficients:", ridge.coef_)
print("Intercept:", ridge.intercept_)
print("MSE:", mean_squared_error(y, y_pred_ridge))
print("R² Score:", r2_score(y, y_pred_ridge))

print("\n=== Lasso Regression ===")
print("Coefficients:", lasso.coef_)
print("Intercept:", lasso.intercept_)
print("MSE:", mean_squared_error(y, y_pred_lasso))
print("R² Score:", r2_score(y, y_pred_lasso))

# === Step 5: Plot the Predictions ===
plt.figure(figsize=(10, 6))
plt.plot(y, label='True Values', marker='o', linewidth=2)
plt.plot(y_pred_lr, label='Linear Regression', marker='s')
plt.plot(y_pred_ridge, label='Ridge (α=1.0)', marker='^')
plt.plot(y_pred_lasso, label='Lasso (α=1.0)', marker='x')
plt.title("Predictions: Linear vs Ridge vs Lasso")
plt.xlabel("Sample Index")
plt.ylabel("Predicted Value")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

# === Step 6: Predict on New Input ===
new_sample = np.array([[7,7]])

pred_lr = lr.predict(new_sample)
pred_ridge = ridge.predict(new_sample)
pred_lasso = lasso.predict(new_sample)

print("\n=== Prediction for New Input ===")
print("Linear Regression Prediction:", pred_lr[0])
print("Ridge Regression Prediction:", pred_ridge[0])
print("Lasso Regression Prediction:", pred_lasso[0])

#-------------------Neurons------------------------
#               - the story
#--------------------------------------------------

# Inputs are about: Ear, Whisker and Fur
inputs = [1, 0, 1]
weights = [0.5, 0.7, 0.6]
threshold = 1.2

weighted_sum = (inputs[0] * weights[0]) + (inputs[1] * weights[1]) + (inputs[2] * weights[2])

if weighted_sum >= threshold:
  print("Neurons fired: Cat")
else:
  print("Not a cat")

"""**Session: Perceptrons**

Perceptron is a single-layer neural network linear or a Machine Learning algorithm used for supervised learning of various binary classifiers.

**Sheet Link:**
https://docs.google.com/spreadsheets/d/1KcfcvPQ40HLCK85LeahIZboLufVGVl0mpSxshLISM00/edit?gid=0#gid=0

Perceptron is a linear model. A perceptron learns to separate data using a straight line (in 2D), a plane (in 3D), or a hyperplane (in higher dimensions). It can only solve linearly separable problems — that is, problems where the data classes can be divided by a straight line (like AND or OR gates). It cannot learn non-linearly separable problems like XOR.

It applies a step function to decide the output.
*output = 1 if z ≥ 0 else 0*
"""

import numpy as np

# Inputs and target outputs (AND gate)
X = np.array([
    [0, 0],
    [0, 1],
    [1, 0],
    [1, 1]
])
y = np.array([0, 0, 0, 1])

# Match spreadsheet: initialize weights and bias
weights = np.zeros(2)
bias = 0
learning_rate = 1
epochs = 10

# Step activation function
def step(z):
    return 1 if z >= 0 else 0

# Training
for epoch in range(epochs):
    print(f"Epoch {epoch + 1}")
    for i in range(len(X)):
        x = X[i]
        y_expected = y[i]

        z = np.dot(x, weights) + bias
        prediction = step(z)
        error = y_expected - prediction

        # Update weights and bias
        weights += learning_rate * error * x
        bias += learning_rate * error

        print(f"Input: {x}, Predicted: {prediction}, Error: {error}, Weights: {weights}, Bias: {bias}")

# Perceptron cannot learn non-linearly separable problems. Below is the proof.
# Even if you change epoch to 100 or 1000, error still remains.
import numpy as np

# XOR input: 2 features
X = np.array([
    [0, 0],
    [0, 1],
    [1, 0],
    [1, 1]
])

# XOR output
y = np.array([0, 1, 1, 0])

# Initialize weights and bias
weights = np.zeros(2)
bias = 0
learning_rate = 1
epochs = 10

# Step activation function
def step(x):
    return 1 if x >= 0 else 0

# Training loop
for epoch in range(epochs):
    print(f"\nEpoch {epoch+1}")
    total_error = 0
    for i in range(len(X)):
        z = np.dot(X[i], weights) + bias
        y_pred = step(z)
        error = y[i] - y_pred

        # Update weights and bias
        weights += learning_rate * error * X[i]
        bias += learning_rate * error
        total_error += abs(error)

        print(f"Input: {X[i]}, Predicted: {y_pred}, Error: {error}, Weights: {weights}, Bias: {bias}")

    if total_error == 0:
        print("\nTraining converged.")
        break

# Sigmoid for AND
# Gradient descent and how a basic neural network learns using a single neuron with sigmoid activation.

'''
This program trains a single-neuron neural network using sigmoid activation to model an AND gate.
It uses gradient descent to iteratively update weights and bias by minimizing the mean squared error between predicted
and expected outputs. The sigmoid function squashes outputs between 0 and 1, while its derivative is used to compute
gradients during backpropagation. The model runs for 10,000 epochs, updating parameters at each step.
Every 1000 epochs, it prints the loss to show learning progress. Finally, it prints the network’s predictions for all
input combinations, demonstrating how the neuron learns the logic of an AND gate.
'''
import numpy as np

# Input data (AND gate)
X = np.array([
    [0, 0],
    [0, 1],
    [1, 0],
    [1, 1]
])

# Expected output
y = np.array([[0], [0], [0], [1]])

# Sigmoid activation function
def sigmoid(z):
    return 1 / (1 + np.exp(-z))

# Derivative of sigmoid (for backprop)
def sigmoid_derivative(z):
    return sigmoid(z) * (1 - sigmoid(z))

# Initialize weights and bias
weights = np.random.randn(2, 1)
bias = 0
learning_rate = 0.1
epochs = 10000

# Training loop using gradient descent
for epoch in range(epochs):
    # Forward pass
    z = np.dot(X, weights) + bias
    y_pred = sigmoid(z)

    # Compute loss (MSE)
    loss = np.mean((y - y_pred) ** 2)

    # Backward pass (gradients)
    error = y_pred - y
    d_weights = np.dot(X.T, error * sigmoid_derivative(z))
    d_bias = np.sum(error * sigmoid_derivative(z))

    # Update weights and bias
    weights -= learning_rate * d_weights
    bias -= learning_rate * d_bias

    # Print every 1000 epochs
    if epoch % 1000 == 0:
        print(f"Epoch {epoch}, Loss: {loss:.4f}")

# Final output
print("\nFinal predictions:")
for i in range(len(X)):
    z = np.dot(X[i], weights) + bias
    y_pred = sigmoid(z)
    print(f"Input: {X[i]}, Output: {y_pred[0]:.4f}")

# Understanding Backpropogation

# Inputs: Ears, Whiskers, Fur
inputs = [1, 1, 1]

# Hidden Layer Weights (2 neurons, each with 3 inputs)
hidden_weights = [
    [0.6, 0.5, 0.4],  # Neuron 1
    [0.3, 0.7, 0.8]   # Neuron 2
]
hidden_thresholds = [0.9, 1.2]

# Output Layer Weights (2 inputs from hidden layer)
output_weights = [0.6, 0.9]
output_threshold = 1.0

# Step 1: Compute hidden layer outputs
hidden_outputs = []
for i in range(2):
    sum_hidden = sum([inputs[j] * hidden_weights[i][j] for j in range(3)])
    output = 1 if sum_hidden >= hidden_thresholds[i] else 0
    hidden_outputs.append(output)

# Step 2: Compute output layer result
final_sum = sum([hidden_outputs[i] * output_weights[i] for i in range(2)])
final_output = 1 if final_sum >= output_threshold else 0

# Step 3: Print result
if final_output == 1:
    print("Mini-brain says: It's a cat!")
else:
    print("Mini-brain says: Not a cat.")

# House price prediction example:
# Inputs and true output
# area in sqft and price in lakhs
area = 1000
true_price = 75

# Initial weight
weight = 0.01

# Predicted price
predicted_price = area * weight

# Error
error = (true_price - predicted_price) ** 2

print("Predicted Price:", predicted_price)
print("Error:", error)

loss = 0.5 * (true_price - predicted_price)**2
print("Loss:", loss)

# Learning rate
lr = 0.0001

# Gradient computation: dl/dw
'''
l = 1/2 (y-y1)^2
  = 0.5 (y - wx)^2

dl/dw =  d  [ 0.5 (y - wx)^2]
        ---
         dw

      = (y - wx) (-x)
      =  - (y - y1) * x
'''
gradient = - (true_price - predicted_price) * area
weight = weight - lr * gradient

# New prediction after update
predicted_price = area * weight
print("Updated Weight:", weight)
print("New Predicted Price:", predicted_price)

"""**PyTorch**

PyTorch is an open-source machine learning library used for deep learning tasks.

A tensor is a fundamental concept in PyTorch (and deep learning in general). It's a multi-dimensional array or matrix that can hold data and be manipulated for mathematical operations. Tensors are very similar to NumPy arrays but can run on GPUs (using CUDA) for accelerated computation, making them efficient for deep learning tasks.
"""

import torch

# Create a tensor
tensor = torch.rand(3, 3)
print(tensor)

# 1D, 2D and 3D in tensor
import torch

# 1D Tensor (Vector)
tensor_1d = torch.tensor([1, 2, 3, 4, 5])
print("1D Tensor (shape={}):\n{}".format(tensor_1d.shape, tensor_1d))

# 2D Tensor (Matrix)
tensor_2d = torch.tensor([[1, 2], [3, 4], [5,6]])
print("\n2D Tensor (shape={}):\n{}".format(tensor_2d.shape, tensor_2d))

# 3D Tensor (Stack of Matrices or "Tensor cube")
tensor_3d = torch.tensor([
    [[1,2], [3, 4]],
    [[5, 6], [7, 8]]
])
print("\n3D Tensor (shape={}):\n{}".format(tensor_3d.shape, tensor_3d))

# Tensor Opeerations

import torch

# Scalar
a = torch.tensor(2.0)

# Vector
v = torch.tensor([1.0, 2.0, 3.0])

# Matrix
m = torch.tensor([[1.0, 2.0], [3.0, 4.0]])

# Arithmetic Operatios
x = torch.tensor([2.0, 4.0])
y = torch.tensor([1.0, 3.0])

# Element-wise operations
print(x + y)      # Addition
print(x * y)      # Multiplication
print(x - y)      # Subtraction
print(x / y)      # Division


# Matrix Multiplication
A = torch.tensor([[1.0, 2.0], [3.0, 4.0]])
B = torch.tensor([[5.0, 6.0], [7.0, 8.0]])

# Matrix multiplication
C = torch.matmul(A, B)
print(C)

# Case Study: Predicting Output using y = wx + b
# Inputs and parameters
x = torch.tensor([2.0])                       # Input
w = torch.tensor([3.0], requires_grad=True)  # Weight
b = torch.tensor([1.0], requires_grad=True)  # Bias

# Forward pass
y = w * x + b
print("Prediction y:", y)

import torch

# Step 1: Data
area = torch.tensor(1.0)         # e.g., 1 unit of area
true_price = torch.tensor(3.0)   # e.g., Rs. 3 lakhs

# Step 2: Initialize weight with gradient tracking
weight = torch.tensor(0.1, requires_grad=True)

# Step 3: Learning rate
lr = 0.01

# Step 4: Model function
def model(x):
    return x * weight

# Step 5: Training loop
for epoch in range(1000):
    # Forward pass
    predicted_price = model(area)
    loss = (true_price - predicted_price) ** 2

    # Backward pass
    loss.backward()

    # Update weight
    with torch.no_grad():
        weight -= lr * weight.grad

    # Zero gradients - reset to zero
    weight.grad.zero_()

    # Print progress
    if epoch % 100 == 0 or loss.item() < 1e-6:
        print(f"Epoch {epoch}: Weight = {weight.item():.4f}, Predicted = {predicted_price.item():.4f}, Loss = {loss.item():.6f}")

    if loss.item() < 1e-6:
        print("Converged.")
        break

# Final prediction
print("\nFinal Trained Weight:", weight.item())
print("Prediction for area=2:", model(torch.tensor(2.0)).item())

"""**Smart Hiring System**

Let us build an application for the smart hiring system.


"""

# Loading the data of smart hiring system
import pandas as pd

# Candidate data
data = {
    'GPA': [8.9, 7.5, 6.2, 9.1, 7.0, 5.8, 8.5, 6.5, 5.2, 7.8],
    'Intern': [6, 3, 0, 5, 1, 0, 4, 2, 0, 3],
    'Projects': [5, 4, 2, 6, 3, 1, 5, 3, 0, 4],
    'CommSkill': [9, 7, 5, 8, 6, 4, 9, 6, 3, 8],
    'Label': ['Strong Fit', 'Maybe', 'Not a Fit', 'Strong Fit', 'Maybe','Not a Fit', 'Strong Fit', 'Maybe', 'Not a Fit', 'Maybe']
}

df = pd.DataFrame(data)
print(df)

import numpy as np

# Input for Candidate A
x = np.array([8.9, 6, 5, 9])  # Features: GPA, Intern, Projects, CommSkill

# Manually defined weights and bias
w = np.array([0.3, 0.2, 0.5, 0.1])
b = -3

# Linear combination
z = np.dot(x, w) + b

print("Weighted sum (Linear Output):", z)

# Nature of Graphs - Sigmoid, relu and tanh
import numpy as np
import matplotlib.pyplot as plt

# Generate input values from -5 to 5
z = np.linspace(-5, 5, 500)

# Activation functions
def sigmoid(z): return 1 / (1 + np.exp(-z))
def relu(z): return np.maximum(0, z)
def tanh(z): return np.tanh(z)

# Compute outputs
sigmoid_vals = sigmoid(z)
relu_vals = relu(z)
tanh_vals = tanh(z)

# Plot
plt.figure(figsize=(10, 6))
plt.plot(z, sigmoid_vals, label='Sigmoid', color='blue')
plt.plot(z, tanh_vals, label='Tanh', color='green')
plt.plot(z, relu_vals, label='ReLU', color='red')

plt.title('Activation Functions: Sigmoid vs Tanh vs ReLU')
plt.xlabel('Input z')
plt.ylabel('Activated Output')
plt.axhline(0, color='gray', linestyle='--', linewidth=0.5)
plt.axvline(0, color='gray', linestyle='--', linewidth=0.5)
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

# What relu does
import numpy as np

# Input: Candidate profile
x = np.array([8.5, 4, 5, 9])  # Example candidate

# Simulate weights for 3 neurons (shape: 3 x 4)
W = np.array([
    [0.2, 0.5, -0.3, 0.8],   # Neuron 1
    [-0.5, 0.1, 0.4, 0.3],   # Neuron 2
    [0.6, -0.4, 0.2, 0.1]    # Neuron 3
])

# Biases for 3 neurons
b = np.array([2, -1, 0.5])

# ReLU function
def relu(z):
    return np.maximum(0, z)

# Step 1: Weighted sum (dot product + bias)
z = np.dot(W, x) + b
print("Weighted sums (z):", z)

# Step 2: Activation
a = relu(z)
print("Activated outputs (ReLU):", a)

# Vanishing Gradient Example

import numpy as np

def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def sigmoid_derivative(x):
    s = sigmoid(x)
    return s * (1 - s)

# Simulate passing a gradient through 10 layers
layers = 15
initial_gradient = 1
x = 8

gradient = initial_gradient
print("Initial gradient:", gradient)

for i in range(1, layers + 1):
    gradient *= sigmoid_derivative(x)  # multiply by derivative at each layer
    print(f"After layer {i}, gradient: {gradient}")

# Fitting all activation functions
import numpy as np
import pandas as pd

# Candidate data
data = {
    'GPA': [8.9, 7.5, 6.2, 9.1, 7.0, 5.8, 8.5, 6.5, 5.2, 7.8],
    'Intern': [6, 3, 0, 5, 1, 0, 4, 2, 0, 3],
    'Projects': [5, 4, 2, 6, 3, 1, 5, 3, 0, 4],
    'CommSkill': [9, 7, 5, 8, 6, 4, 9, 6, 3, 8],
    'Label': ['Strong Fit', 'Maybe', 'Not a Fit', 'Strong Fit', 'Maybe','Not a Fit', 'Strong Fit', 'Maybe', 'Not a Fit', 'Maybe']
}

df = pd.DataFrame(data)

# Manually defined weights and bias
w = np.array([0.3, 0.2, 0.5, 0.1])
b = -3

# Feature matrix
features = df[['GPA', 'Intern', 'Projects', 'CommSkill']].values

# Linear output z = w.x + b
z_values = np.dot(features, w) + b

# Define activation functions
def sigmoid(z):
  return 1 / (1 + np.exp(-z))

def relu(z):
  return np.maximum(0, z)

def tanh(z):
  return np.tanh(z)

# Apply activations
df['z'] = z_values
df['Sigmoid'] = sigmoid(z_values)
df['ReLU'] = relu(z_values)
df['Tanh'] = tanh(z_values)

# Display key outputs
print(df[['z', 'Sigmoid']])
print(df[['z', 'ReLU']])
print(df[['z', 'Tanh']])

# Demo of softmax

import numpy as np

def softmax(z):
  exp_values = np.exp(z)
  return exp_values / np.sum(exp_values)


# Example: Scores predicted by a model for one candidate
# Let's assume the model predicts 3 raw scores for a candidate:
# [Strong Fit, Maybe, Not a Fit]
logits = np.array([2.0, 1.0, 0.1])

probs = softmax(logits)

print("Raw scores (logits):", logits)
print("Softmax probabilities:", probs)
print("Predicted class:", np.argmax(probs))  # index of highest probability

# Applying it to our example
import numpy as np

# Candidate data: GPA, Intern, Projects, CommSkill
X = np.array([
    [8.9, 6, 5, 9],
    [7.5, 3, 4, 7],
    [6.2, 0, 2, 5],
    [9.1, 5, 6, 8],
    [7.0, 1, 3, 6],
    [5.8, 0, 1, 4],
    [8.5, 4, 5, 9],
    [6.5, 2, 3, 6],
    [5.2, 0, 0, 3],
    [7.8, 3, 4, 8]
])

candidates = ["A","B","C","D","E","F","G","H","I","J"]

# Let's define weights for each class: [Strong Fit, Maybe, Not a Fit]
W = np.array([
    [0.4, 0.1, -0.3],  # GPA weights
    [0.2, 0.1, -0.2],  # Intern
    [0.3, 0.2, -0.1],  # Projects
    [0.5, 0.2, -0.4]   # CommSkill
])

b = np.array([0.5, 0, -0.5])  # Bias for each class

def softmax(z):
    exp_vals = np.exp(z - np.max(z))
    return exp_vals / np.sum(exp_vals)

labels = ["Strong Fit", "Maybe", "Not a Fit"]

for i in range(len(X)):
    x = X[i]
    z = np.dot(x, W) + b
    probs = softmax(z)
    prediction = np.argmax(probs)

    print(f"Candidate {candidates[i]}:")
    print(f"  Raw scores: {z}")
    print(f"  Probabilities: {probs}")
    print(f"  Predicted Class: {labels[prediction]}")
    print("-" * 40)

# Simple fully connected neural network (MLP) using torch.nn.Module.
# MLP - Multi-Layer Perceptron

# Import PyTorch and modules for neural networks
import torch
import torch.nn as nn
import torch.nn.functional as F

# -------------------------------
# Step 1: Define Sample Input
# -------------------------------

# A mini-batch of 2 samples, each with 4 input features
# Format: [GPA, Intern months, Projects, Communication Skill] or similar
x = torch.tensor([[0.5, -1.2, 0.3, 0.8],
                  [0.1,  0.0, -0.5, 0.9]],
                 dtype=torch.float32)


# -------------------------------
# Step 2: Define MLP Architecture
# -------------------------------

# Subclassing nn.Module to define a custom neural network
class SimpleMLP(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(SimpleMLP, self).__init__()

        # First fully connected layer: input → hidden
        self.fc1 = nn.Linear(input_size, hidden_size)

        # Second fully connected layer: hidden → output
        self.fc2 = nn.Linear(hidden_size, output_size)

        # Hidden layer activation: ReLU (can be replaced with Tanh, Sigmoid, etc.)
        self.activation_hidden = nn.ReLU()

        # Output layer activation: Softmax to get class probabilities
        # dim=1 means softmax is applied across columns (classes) for each sample
        self.activation_output = nn.Softmax(dim=1)

    def forward(self, x):
        # First layer transformation: linear + activation
        x = self.fc1(x)
        x = self.activation_hidden(x)

        # Second layer transformation: linear + activation
        x = self.fc2(x)
        x = self.activation_output(x)

        # Final output is a probability distribution over 3 classes
        return x

# -------------------------------
# Step 3: Create Model and Predict
# -------------------------------

# Instantiate the model
model = SimpleMLP(input_size=4, hidden_size=5, output_size=3)

# Forward pass: send input through the model
output = model(x)

# Display output probabilities for each class
print("Model output:\n", output)

# Iris Data Set - with forward and back propogation
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# 1. Load and prepare the Iris dataset
data = load_iris()
X = data.data
y = data.target

# Normalize input features
scaler = StandardScaler()
X = scaler.fit_transform(X)

# Convert to tensors
X = torch.tensor(X, dtype=torch.float32)
y = torch.tensor(y, dtype=torch.long)

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 2. Define MLP model
class IrisMLP(nn.Module):
    def __init__(self):
        super(IrisMLP, self).__init__()
        self.model = nn.Sequential(
          nn.Linear(4, 10),
          nn.ReLU(),
          nn.Linear(10, 3)
        )

    def forward(self, x):
        return self.model(x)

model = IrisMLP()

# 3. Define loss function and SGD optimizer
criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(model.parameters(), lr = 0.1)

# 4. Training loop
for epoch in range(100):
    outputs = model(X_train)
    loss = criterion(outputs, y_train)

    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    if epoch % 10 == 0:
        print(f"Epoch {epoch}, Loss: {loss.item():.4f}")

# 5. Testing
with torch.no_grad():
    test_outputs = model(X_test)
    _, predictions = torch.max(test_outputs, 1)
    accuracy = (predictions == y_test).sum().item() / y_test.size(0)

print(f"\nTest Accuracy: {accuracy * 100:.2f}%")

# Working with MNIST data set
# Wikipedia Link: https://en.wikipedia.org/wiki/MNIST_database
# 70,000 grayscale images of handwritten digits

import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms

# 1. Load and transform MNIST
transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])
train_data = datasets.MNIST(root='./data', train=True, download=True, transform=transform)
test_data = datasets.MNIST(root='./data', train=False, download=True, transform=transform)

train_loader = torch.utils.data.DataLoader(train_data, batch_size=64, shuffle=True)
test_loader = torch.utils.data.DataLoader(test_data, batch_size=64, shuffle=False)

# 2. Define the MLP Model
class MLP(nn.Module):
    def __init__(self):
        super(MLP, self).__init__()
        self.model = nn.Sequential(
            nn.Flatten(),
            nn.Linear(28*28, 128),
            nn.ReLU(),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, 10)
        )

    def forward(self, x):
        return self.model(x)

model = MLP()

# 3. Define loss and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(model.parameters(), lr=0.01)

# 4. Training loop
for epoch in range(5):
    running_loss = 0
    for images, labels in train_loader:
        optimizer.zero_grad()
        outputs = model(images)
        loss = criterion(outputs, labels)
        loss.backward()       # Backpropagation
        optimizer.step()      # Update weights
        running_loss += loss.item()
    print(f"Epoch {epoch+1} - Loss: {running_loss/len(train_loader)}")

# 5. Test accuracy
correct, total = 0, 0
with torch.no_grad():
    for images, labels in test_loader:
        outputs = model(images)
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

print(f"Test Accuracy: {100 * correct / total:.2f}%")

"""**Types of NN**

Feedforward Neural Networks (FNNs) are the simplest type, where data flows in one direction from input to output, ideal for image and pattern recognition. Recurrent Neural Networks (RNNs) include loops, enabling memory of past inputs, making them suitable for sequential data like text, speech, and time-series predictions.
"""

'''
Feedforward Neural Network (FNN)
How it works:
Treats input as a single flat vector: [1, 2, 3]
No memory of the order in which numbers came
Works okay for simple patterns but doesn’t scale well to sequences or time-dependent data
'''
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
import numpy as np

# Training data
X_fnn = np.array([[1, 2, 3]])
y_fnn = np.array([4])

# FNN model
fnn = Sequential([
    Dense(10, input_shape=(3,), activation='relu'),
    Dense(1)
])

fnn.compile(optimizer='adam', loss='mse')
fnn.fit(X_fnn, y_fnn, epochs=200, verbose=0)

# Test
print("FNN prediction for [2, 3, 4]:", fnn.predict(np.array([[2, 3, 4]])))

'''
Recurrent Neural Network (RNN)
How it works:
Treats input as a sequence: [[1], [2], [3]]
Processes input one step at a time
Maintains memory of past values using hidden states
Better suited for sequence tasks like language, stock prices, time series, etc.
'''

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import SimpleRNN, Dense
import numpy as np

# Training data reshaped for RNN
X_rnn = np.array([[[1], [2], [3]]])  # shape: (batch, time, features)
y_rnn = np.array([4])

# RNN model
rnn = Sequential([
    SimpleRNN(10, input_shape=(3, 1)),  # 3 time steps, 1 feature per step
    Dense(1)
])

rnn.compile(optimizer='adam', loss='mse')
rnn.fit(X_rnn, y_rnn, epochs=200, verbose=0)

# Test
test_input = np.array([[[2], [3], [4]]])  # shape: (1, 3, 1)
print("RNN prediction for [2, 3, 4]:", rnn.predict(test_input))

# Using Keras

'''
This below code:
===================

import tensorflow as tf

# Define weights and biases manually for each layer
class SimpleModel(tf.Module):
    def __init__(self):
        super().__init__()

        # Layer 1: Dense(64, activation='relu')
        self.w1 = tf.Variable(tf.random.normal([10, 64]), name='w1')
        self.b1 = tf.Variable(tf.zeros([64]), name='b1')

        # Layer 2: Dense(1)
        self.w2 = tf.Variable(tf.random.normal([64, 1]), name='w2')
        self.b2 = tf.Variable(tf.zeros([1]), name='b2')

    def __call__(self, x):
        # First dense layer with ReLU activation
        z1 = tf.matmul(x, self.w1) + self.b1
        a1 = tf.nn.relu(z1)

        # Output layer (no activation)
        z2 = tf.matmul(a1, self.w2) + self.b2
        return z2

# Instantiate the model
model = SimpleModel()

# Example input
x = tf.random.normal([5, 10])  # batch of 5 samples, each with 10 features
output = model(x)

print("Model output:", output)
'''


# Changes to

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Input

model = Sequential([
    Input(shape=(10,)),
    Dense(64, activation='relu'),
    Dense(1)
])

"""**Clustering**

Clustering is an **unsupervised learning** technique used to group similar data points based on their attributes. It helps identify patterns or structures in data without using predefined labels. The goal is to ensure items within a cluster are more similar to each other than to those in other clusters.

Sheet Link: https://docs.google.com/spreadsheets/d/1vjT_HV30xx-DUS30IzVh9maqagWZvJJAMqf6dUPp4S8/edit?gid=0#gid=0
"""

# Implementing the case study example (Iterations in spreadhseet)
import math

# Data
customers = {
    "C1": (2, 500),
    "C2": (10, 800),
    "C3": (4, 300),
    "C4": (11, 1200),
    "C5": (3, 350),
    "C6": (9, 1000)
}

# Initialize centroids manually
centroid_A = customers["C1"]
centroid_B = customers["C4"]

def euclidean(p1, p2):
    return math.sqrt((p1[0] - p2[0]) ** 2 + (p1[1] - p2[1]) ** 2)

def compute_new_centroid(cluster):
    freq = sum(c[0] for c in cluster) / len(cluster)
    spend = sum(c[1] for c in cluster) / len(cluster)
    return (freq, spend)

# Run until convergence
iteration = 1
prev_assignment = {}
while True:
    print(f"\n--- Iteration {iteration} ---")

    cluster_A = []
    cluster_B = []
    assignment = {}

    for cid, (f, s) in customers.items():
        dA = euclidean((f, s), centroid_A)
        dB = euclidean((f, s), centroid_B)

        if dA < dB:
            cluster_A.append((f, s))
            assignment[cid] = 'A'
        else:
            cluster_B.append((f, s))
            assignment[cid] = 'B'

        print(f"{cid}: Dist to A={dA:.2f}, B={dB:.2f} => Cluster {assignment[cid]}")

    if assignment == prev_assignment:
        print("\n Converged.")
        break

    prev_assignment = assignment
    centroid_A = compute_new_centroid(cluster_A)
    centroid_B = compute_new_centroid(cluster_B)

    print(f"\nNew Centroid A: {centroid_A}")
    print(f"New Centroid B: {centroid_B}")

    iteration += 1

# Final Output
print("\n Final Clusters:")
print("Cluster A:", [cid for cid, grp in assignment.items() if grp == 'A'])
print("Cluster B:", [cid for cid, grp in assignment.items() if grp == 'B'])

# Implementing the same using library functions

import numpy as np
from sklearn.cluster import KMeans

# Data
customer_ids = ["C1", "C2", "C3", "C4", "C5", "C6"]
data = np.array([
    [2, 500],
    [10, 800],
    [4, 300],
    [11, 1200],
    [3, 350],
    [9, 1000]
])

# Initial centroids from C1 and C4
initial_centroids = np.array([
    [2, 500],    # C1
    [11, 1200]   # C4
])

# KMeans model with manually specified initial centroids
kmeans = KMeans(n_clusters = 2, init = initial_centroids, n_init = 1, max_iter = 25)
kmeans.fit(data)

# Results
labels = kmeans.labels_

# Display results
for cid, label in zip(customer_ids, labels):
    print(f"{cid} → Cluster {label}")

print("\nCluster Centers:")
print(kmeans.cluster_centers_)

# Make random initializations
import numpy as np
from sklearn.cluster import KMeans

# Data
customer_ids = ["C1", "C2", "C3", "C4", "C5", "C6"]
data = np.array([
    [2, 500],
    [10, 800],
    [4, 300],
    [11, 1200],
    [3, 350],
    [9, 1000]
])

# KMeans model without manually specified initial centroids
kmeans = KMeans(n_clusters = 2, n_init = 10, max_iter = 25)
kmeans.fit(data)

# Results
labels = kmeans.labels_

# Display results
for cid, label in zip(customer_ids, labels):
    print(f"{cid} → Cluster {label}")

print("\nCluster Centers:")
print(kmeans.cluster_centers_)

# With a graph
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans

# Data
customer_ids = ["C1", "C2", "C3", "C4", "C5", "C6"]
data = np.array([
    [2, 500],
    [10, 800],
    [4, 300],
    [11, 1200],
    [3, 350],
    [9, 1000]
])

# Initial centroids from C1 and C4
initial_centroids = np.array([
    [2, 500],    # C1
    [11, 1200]   # C4
])

# Fit KMeans
kmeans = KMeans(n_clusters=2, init=initial_centroids, n_init=1, max_iter=100, random_state=42)
kmeans.fit(data)
labels = kmeans.labels_
centroids = kmeans.cluster_centers_

# Plot
colors = ['red', 'blue']
for i in range(2):
    cluster_points = data[labels == i]
    plt.scatter(cluster_points[:, 0], cluster_points[:, 1], c=colors[i], label=f'Cluster {i}', s=100)

# Plot centroids
plt.scatter(centroids[:, 0], centroids[:, 1], c='black', marker='X', s=200, label='Centroids')

# Add customer IDs as labels
for i, txt in enumerate(customer_ids):
    plt.annotate(txt, (data[i, 0] + 0.1, data[i, 1] + 10))

plt.xlabel('Frequency')
plt.ylabel('Spend')
plt.title('Customer Clusters using K-Means')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
import pandas as pd
import numpy as np

# Example data
data = {
    'Frequency': [2, 10, 4, 11, 3, 9, 3, 7, 9, 4, 12, 5, 8, 5],
    'Spend': [500, 800, 300, 1200, 350, 1000, 750, 800, 1000, 1200, 450, 300, 200, 600]
}
df = pd.DataFrame(data)

# Scale the features
scaler = StandardScaler()
scaled_data = scaler.fit_transform(df)

# Try KMeans for different values of k
inertia = []
K = range(1, 11)
for k in K:
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(scaled_data)
    inertia.append(kmeans.inertia_)

# Plot the Elbow
plt.figure(figsize=(8, 4))
plt.plot(K, inertia, 'bo-')
plt.xlabel('Number of clusters (k)')
plt.ylabel('Inertia')
plt.title('Elbow Method For Optimal k')
plt.grid(True)
plt.show()

# Dataset Link: https://archive.ics.uci.edu/dataset/352/online+retail

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score

# Step 1: Load the dataset
df = pd.read_excel('Online Retail.xlsx')

# Step 2: Data Preprocessing
# Remove the rows with missing customer ids
df = df[pd.notnull(df['CustomerID'])]

# Remove canceled orders (InvoiceNo starting with 'C')
df = df[~df['InvoiceNo'].astype(str).str.startswith('C')]

# Remove negative quantities
df = df[df['Quantity'] > 0]

# Step 3: Feature Engineering
# Calculate TotalPrice
df['TotalPrice'] = df['Quantity'] * df['UnitPrice']

# Aggregate data by CustomerID
customer_df = df.groupby('CustomerID').agg({
    'InvoiceNo': 'nunique',  # Frequency
    'Quantity': 'sum',
    'TotalPrice': 'sum'
}).rename(columns={
    'InvoiceNo': 'Frequency',
    'Quantity': 'TotalQuantity',
    'TotalPrice': 'Monetary'
}).reset_index()

# Step 4: Feature Scaling
features = ['Frequency', 'TotalQuantity', 'Monetary']
scaler = StandardScaler()
scaled_features = scaler.fit_transform(customer_df[features])

# Step 5: Determine the optimal number of clusters using the Elbow Method
sse = []
k_range = range(1, 11)
for k in k_range:
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(scaled_features)
    sse.append(kmeans.inertia_)

print(sse)
print("\n")

# Plot the Elbow Curve
plt.figure(figsize=(8, 5))
plt.plot(k_range, sse, marker='o')
plt.title('Elbow Method')
plt.xlabel('Number of clusters')
plt.ylabel('Sum of squared distances (Inertia)')
plt.show()


# Step 6: Evaluate clustering with Silhouette Score
silhouette_scores = []
for k in range(2, 11):
    kmeans = KMeans(n_clusters=k, random_state=42)
    labels = kmeans.fit_predict(scaled_features)
    score =  silhouette_score(scaled_features, labels)
    silhouette_scores.append(score)
    print(f'For n_clusters = {k}, the Silhouette Score is {score:.4f}')

# Plot Silhouette Scores
plt.figure(figsize=(8, 5))
plt.plot(range(2, 11), silhouette_scores, marker='o')
plt.title('Silhouette Scores for Various Clusters')
plt.xlabel('Number of clusters')
plt.ylabel('Silhouette Score')
plt.show()



# Step 7: Apply KMeans with the optimal number of clusters (e.g., k=5)
optimal_k = 4
kmeans = KMeans(n_clusters=optimal_k, random_state=42)
customer_df['Cluster'] = kmeans.fit_predict(scaled_features)

# Step 8: Visualize the clusters
# For visualization, we'll use the first two principal components
from sklearn.decomposition import PCA

pca = PCA(n_components=2)
principal_components = pca.fit_transform(scaled_features)
customer_df['PC1'] = principal_components[:, 0]
customer_df['PC2'] = principal_components[:, 1]

plt.figure(figsize=(8, 5))
sns.scatterplot(data=customer_df, x='PC1', y='PC2', hue='Cluster', palette='Set2')
plt.title('Customer Segments')
plt.show()

# Hierarchical Clustering
from sklearn.cluster import AgglomerativeClustering
import matplotlib.pyplot as plt
import numpy as np

# Sample data: 2D points
X = np.array([[1, 2], [2, 3], [5, 6], [6, 6], [3, 6], [2, 5], [3, 4], [5,4], [9, 10]])

# Create Agglomerative Clustering model
agg = AgglomerativeClustering(n_clusters=4, linkage='ward')
labels = agg.fit_predict(X)

# Plotting
plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='rainbow')
plt.title("Agglomerative Clustering")
plt.show()

from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
import numpy as np

# Sample data
X = np.array([[1, 2], [2, 3], [5, 6], [6, 6], [3, 6], [2, 5], [3, 4], [5,4], [9, 10]])

# Recursive divisive function
def divisive_clustering(data, max_clusters=2):
    clusters = [data]
    labels = np.zeros(len(data), dtype=int)

    while len(clusters) < max_clusters:
        # Find the largest cluster to split
        sizes = [len(c) for c in clusters]
        idx = np.argmax(sizes)
        to_split = clusters.pop(idx)

        # Apply KMeans with 2 clusters
        kmeans = KMeans(n_clusters=3, n_init=10)
        kmeans.fit(to_split)
        split_labels = kmeans.labels_

        # Split the cluster
        cluster1 = to_split[split_labels == 0]
        cluster2 = to_split[split_labels == 1]
        clusters.extend([cluster1, cluster2])

    # Assign final labels
    label_id = 0
    for cluster in clusters:
        for point in cluster:
            index = np.where((X == point).all(axis=1))[0][0]
            labels[index] = label_id
        label_id += 1

    return labels

# Apply divisive clustering
labels = divisive_clustering(X, max_clusters=2)

# Plotting
plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='rainbow')
plt.title("Divisive Clustering (via KMeans)")
plt.show()

# Reading a Dendrogram
from sklearn.datasets import make_blobs
from scipy.cluster.hierarchy import dendrogram, linkage
import matplotlib.pyplot as plt

# Step 1: Generate synthetic data
X, _ = make_blobs(n_samples=30, centers=3, random_state=42)

# Step 2: Compute the linkage matrix
Z = linkage(X, method='complete')  # You can try 'ward, 'single', 'complete', 'average'

# Step 3: Plot dendrogram
plt.figure(figsize=(10, 5))
dendrogram(Z)
plt.title("Hierarchical Clustering Dendrogram")
plt.xlabel("Data Point Index")
plt.ylabel("Cluster Distance")
plt.show()

import matplotlib.pyplot as plt
from sklearn.datasets import make_blobs
from scipy.cluster.hierarchy import linkage, dendrogram

# Step 1: Generate synthetic dataset
X, _ = make_blobs(n_samples=30, centers=3, random_state=42)

# Step 2: List of linkage methods
methods = ['ward', 'single', 'complete', 'average']

# Step 3: Set up a 2x2 plot grid
fig, axs = plt.subplots(2, 2, figsize=(14, 10))
axs = axs.flatten()

# Step 4: Loop through each method and plot the dendrogram
for i, method in enumerate(methods):
    Z = linkage(X, method=method)
    dendrogram(Z, ax=axs[i])
    axs[i].set_title(f'Dendrogram - {method.capitalize()} Linkage')
    axs[i].set_xlabel('Sample Index')
    axs[i].set_ylabel('Distance')

plt.tight_layout()
plt.show()

# Applying DBSCAN on 2-moons data set
from sklearn.datasets import make_moons
import matplotlib.pyplot as plt
from sklearn.cluster import DBSCAN
import numpy as np
from sklearn.metrics import silhouette_score
from sklearn.cluster import KMeans


# Understand the dataset
X, _ = make_moons(n_samples=200, noise=0.05, random_state=0)
plt.scatter(X[:, 0], X[:, 1])
plt.title("Two Moons Dataset")
plt.xlabel("X1")
plt.ylabel("X2")
plt.show()


# What would k-means do to this?
kmeans = KMeans(n_clusters=2, random_state=0)
kmeans_labels = kmeans.fit_predict(X)

plt.scatter(X[:, 0], X[:, 1], c=kmeans_labels, cmap='rainbow', s=40)
plt.title("K-Means Clustering")
plt.xlabel("X1")
plt.ylabel("X2")
plt.show()




# DBSCAN with chosen parameters
db = DBSCAN(eps=0.2, min_samples = 5)
db.fit(X)

# Get labels (-1 means noise)
labels = db.labels_

# Plotting
plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='rainbow', s=40)
plt.title("DBSCAN Clustering")
plt.xlabel("X1")
plt.ylabel("X2")
plt.show()


# Filter out noise points
core_samples_mask = labels != -1
if np.sum(core_samples_mask) > 1:
    score = silhouette_score(X[core_samples_mask], labels[core_samples_mask])
    print(f"Silhouette Score: {score:.2f}")
else:
    print("Not enough core samples for silhouette score.")

# Measuring Accuracy
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_moons
from sklearn.cluster import DBSCAN
from sklearn.metrics import silhouette_score, davies_bouldin_score

# Step 1: Generate two-moons dataset
X, _ = make_moons(n_samples=200, noise=0.05, random_state=0)

# Step 2: Apply DBSCAN
dbscan = DBSCAN(eps=0.2, min_samples=5)
labels = dbscan.fit_predict(X)

# Step 3: Plot clustering result
plt.figure(figsize=(6, 5))
plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='rainbow', s=40)
plt.title("DBSCAN Clustering Result")
plt.xlabel("X1")
plt.ylabel("X2")
plt.show()

# Step 4: Filter out noise for evaluation (-1 are noise points)
mask = labels != -1
X_core = X[mask]
labels_core = labels[mask]

# Step 5: Evaluation
if len(set(labels_core)) > 1:
    sil_score = silhouette_score(X_core, labels_core)
    db_score = davies_bouldin_score(X_core, labels_core)
    print(f"Silhouette Score: {sil_score:.2f}")
    print(f"Davies-Bouldin Index: {db_score:.2f}")
else:
    print("Not enough clusters (or too many noise points) for evaluation.")

import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_blobs
from sklearn.cluster import DBSCAN
from sklearn.metrics import silhouette_score, davies_bouldin_score

# Step 1: Generate synthetic clustered data with noise
X, _ = make_blobs(n_samples=400, centers=3, cluster_std=0.6, random_state=42)
noise = np.random.uniform(low=-10, high=10, size=(40, 2))  # Add some outliers
X = np.vstack([X, noise])  # Combine clusters and outliers

# Step 2: Try DBSCAN with different eps values
# In DBSCAN, eps (epsilon) defines the radius of a point's neighborhood. It controls how close points must be to be considered part of the same cluster.
# A point is labeled a core point if it has enough neighbors (min_samples) within this eps radius.
# Small eps values can lead to many outliers or fragmented clusters, while large values may merge distinct clusters.
# It directly affects the number and shape of clusters detected. Choosing the right eps is crucial and often done through visual inspection or a k-distance plot.
#DBSCAN is powerful for detecting clusters of arbitrary shape and noise.
eps_values = [0.2, 0.4, 0.6, 0.8]

# Step 3: Plot the results
plt.figure(figsize=(16, 8))
for i, eps in enumerate(eps_values, 1):
    dbscan = DBSCAN(eps=eps, min_samples=5)
    labels = dbscan.fit_predict(X)

    plt.subplot(2, 2, i)
    plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='Paired', s=30)
    plt.title(f"DBSCAN with eps={eps} | Clusters: {len(set(labels)) - (1 if -1 in labels else 0)} | Outliers: {np.sum(labels==-1)}")
    plt.xlabel("X1")
    plt.ylabel("X2")

plt.tight_layout()
plt.show()

"""Image Processing

Sheet Link: https://docs.google.com/spreadsheets/d/1EAzPQG5aDgp0aHrYeQNOml5SeIZXXCFFHIf3NHaehsQ/edit?gid=818603946#gid=818603946
"""

import cv2
import numpy as np
import matplotlib.pyplot as plt

# Load and convert to RGB
image = cv2.imread('iit-ropar.jpg')
image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

# Split RGB channels
r = image_rgb[:, :, 0]
g = image_rgb[:, :, 1]
b = image_rgb[:, :, 2]

# Sum of pixel values in each channel
total_r = np.sum(r)
total_g = np.sum(g)
total_b = np.sum(b)

# Plotting
colors = ['Red', 'Green', 'Blue']
values = [total_r, total_g, total_b]

plt.figure()
plt.imshow(image)
plt.title("Original RGB Image")
plt.axis('off')
plt.show()

plt.figure(figsize=(6, 4))
plt.bar(colors, values, color=['red', 'green', 'blue'])
plt.title('Total Color Intensity in Image')
plt.ylabel('Sum of Pixel Values')
plt.tight_layout()
plt.show()

# Motivation Case Study
# Why do you see what do you see?
# Why is this blurry?
# But let us stitch the story better


import torch
import torchvision
import torchvision.transforms as transforms
import matplotlib.pyplot as plt
import matplotlib.patches as patches

# Load an image from CIFAR-10 dataset
transform = transforms.Compose([transforms.ToTensor()])
dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)

image, label = dataset[0]  # Get the first image and label

# Simulate a bounding box — format: (x_min, y_min, width, height)
bbox = [10, 10, 20, 15]  # You can change this to any values

# Plot the image with a bounding box
fig, ax = plt.subplots(1)
ax.imshow(image.permute(1, 2, 0), interpolation='nearest')  # Convert from (C, H, W) to (H, W, C) for plotting

# Create a Rectangle patch
rect = patches.Rectangle((bbox[0], bbox[1]), bbox[2], bbox[3], linewidth=2, edgecolor='r', facecolor='none')
ax.add_patch(rect)

# Optional: add label text
ax.text(bbox[0], bbox[1] - 2, 'Object', color='red', fontsize=10, backgroundcolor='white')

plt.axis('off')
plt.show()

import cv2
import matplotlib.pyplot as plt

# Step 1: Load the image using OpenCV (this loads in BGR format)
image_bgr = cv2.imread('iit-ropar.jpg')

# Step 2: Convert the image to RGB format
image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB )

# Step 3: Display both images side by side using matplotlib
plt.figure(figsize=(10, 5))

# Display BGR image (will look odd)
plt.subplot(1, 2, 1)
plt.imshow(image_bgr)  # matplotlib expects RGB, so this will show wrong colors
plt.title("Loaded Image (BGR)")
plt.axis('off')

# Display RGB image (correct colors)
plt.subplot(1, 2, 2)
plt.imshow(image_rgb)
plt.title("Converted Image (RGB)")
plt.axis('off')

plt.tight_layout()
plt.show()

import cv2
import numpy as np
import matplotlib.pyplot as plt

# Load an image
image = cv2.imread('iit-ropar.jpg')
image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

# Display shape and dtype
print("Shape:", image_rgb.shape)  # e.g., (height, width, channels)
print("Data Type:", image_rgb.dtype)

# Plot RGB channels separately
r, g, b = image_rgb[:,:,0], image_rgb[:,:,1], image_rgb[:,:,2]

plt.figure(figsize=(10,3))
for i, channel in enumerate([r, g, b]):
    plt.subplot(1, 3, i+1)
    plt.imshow(channel, cmap='gray')
    plt.title(['Red', 'Green', 'Blue'][i])
    plt.axis('off')
plt.tight_layout()
plt.show()

import cv2
import numpy as np
import matplotlib.pyplot as plt

# Load an image
image = cv2.imread('iit-ropar.jpg')
image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

# Split channels
r, g, b = image_rgb[:,:,0], image_rgb[:,:,1], image_rgb[:,:,2]

# Create zero channel
zeros = np.zeros_like(r)

# Stack each channel with two zeroed channels to isolate color
red_image   = np.stack([r, zeros, zeros], axis=2)
green_image = np.stack([zeros, g, zeros], axis=2)
blue_image  = np.stack([zeros, zeros, b], axis=2)

# Plot all three
plt.figure(figsize=(10, 3))
for i, channel_img in enumerate([red_image, green_image, blue_image]):
    plt.subplot(1, 3, i+1)
    plt.imshow(channel_img)
    plt.title(['Red Channel', 'Green Channel', 'Blue Channel'][i])
    plt.axis('off')

plt.tight_layout()
plt.show()

# Image Processing

from torchvision import transforms
from PIL import Image

img = Image.open("iit-ropar.jpg")

transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.RandomHorizontalFlip(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
])

processed_img = transform(img)
print("Transformed shape:", processed_img.shape)  # (C, H, W)

# Working with a sample convolution
import torch
import torch.nn as nn

# Build the example image
input = torch.tensor([[[
                        [1., 2., 1., 0., 0.],
                        [0., 1., 2., 1., 0.],
                        [1., 2., 1., 0., 0.],
                        [0., 1., 2., 1., 0.],
                        [0., 0., 1., 2., 1.]
                        ]]])  # shape = (1, 1, 5, 5)

# Convolution filter (e.g., edge detection)
conv = nn.Conv2d(in_channels = 1,out_channels = 1,kernel_size = 3, stride =1, padding = 1 )
conv.weight.data = torch.tensor([[[
                                [1., 0., -1.],
                                [1., 0., -1.],
                                [1., 0., -1.]
                                ]]])

output = conv(input)
print(output)

import torch
import torch.nn as nn
import torch.nn.functional as F

# Define a convolutional layer manually
conv = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3, bias=False)

# Assign the custom vertical edge detection kernel
conv.weight.data = torch.tensor([[[[1., 0., -1.],
                                   [1., 0., -1.],
                                   [1., 0., -1.]]]])

# Create a dummy 5x5 image
image = torch.tensor([[[
    [10., 10., 20., 30., 30.],
    [10., 10., 20., 30., 30.],
    [10., 10., 20., 30., 30.],
    [10., 10., 20., 30., 30.],
    [10., 10., 20., 30., 30.]
]]])  # shape = (1, 1, 5, 5)

# Apply the filter
output = conv(image)

print(output)

import torch
import torch.nn as nn

# Single layer example
layer = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, stride=1, padding=1)
input_image = torch.randn(1, 1, 28, 28)  # batch_size x channels x height x width
output = layer(input_image)
print(output.shape)

# Working with CIFAR-10 dataset
# Link: https://www.kaggle.com/c/cifar-10/m

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
import matplotlib.pyplot as plt

# 1. Load and Normalize CIFAR-10 dataset
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # 3-channel mean/std
])

trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)
testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)

trainloader = torch.utils.data.DataLoader(trainset, batch_size=32, shuffle=True)
testloader = torch.utils.data.DataLoader(testset, batch_size=32, shuffle=False)

# 2. Define a Simple CNN
class SimpleCNN(nn.Module):
    def __init__(self):
        super(SimpleCNN, self).__init__()
        self.network = nn.Sequential(
            nn.Conv2d(3, 32, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2),

            nn.Conv2d(32, 64, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2),

            nn.Flatten(),
            nn.Linear(64*8*8, 64),
            nn.ReLU(),
            nn.Linear(64, 10)
        )

    def forward(self, x):
        return self.network(x)

# 3. Initialize Model, Loss, Optimizer
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = SimpleCNN().to(device)

criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# 4. Training Loop
train_losses = []
train_accuracies = []

for epoch in range(5):
    model.train()
    running_loss = 0.0
    correct = 0
    total = 0

    for images, labels in trainloader:
        images, labels = images.to(device), labels.to(device)

        optimizer.zero_grad()
        outputs = model(images)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()

        _, preds = torch.max(outputs, 1)
        correct += (preds == labels).sum().item()
        total += labels.size(0)

    epoch_loss = running_loss / len(trainloader)
    epoch_acc = correct / total

    train_losses.append(epoch_loss)
    train_accuracies.append(epoch_acc)

    print(f"Epoch {epoch+1}: Loss = {epoch_loss:.4f}, Accuracy = {epoch_acc:.4f}")

# 5. Plot Loss and Accuracy
plt.figure(figsize=(10, 4))
plt.subplot(1, 2, 1)
plt.plot(train_losses, label='Loss')
plt.title('Training Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.grid(True)
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(train_accuracies, label='Accuracy')
plt.title('Training Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.grid(True)
plt.legend()

plt.tight_layout()
plt.show()

# Approach 2 leverages transfer learning using a pretrained ResNet18 model.
# By freezing early layers and fine-tuning only the final layer, it efficiently adapts to
# CIFAR-10 image classification. This method reduces training time, improves generalization, and
# demonstrates how pretrained models accelerate deep learning tasks on smaller datasets with limited resources.

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
from torchvision import models
import time

# Check for GPU
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Using device:", device)

# 1. Data Transformation and Loading
transform = transforms.Compose([
    transforms.Resize(224),  # Resize CIFAR images to 224x224 for ResNet18
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
])

trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)
trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True, num_workers=2)

testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)
testloader = torch.utils.data.DataLoader(testset, batch_size=64,shuffle=False, num_workers=2)

# 2. Load Pretrained ResNet18 and Modify Final Layer
resnet18 = models.resnet18(pretrained=True)

# Freeze all layers
for param in resnet18.parameters():
    param.requires_grad = False

# Replace the final fully connected layer (fc)
num_ftrs = resnet18.fc.in_features
resnet18.fc = nn.Linear(num_ftrs, 10)  # CIFAR-10 has 10 classes

resnet18 = resnet18.to(device)

# 3. Define Loss Function and Optimizer
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(resnet18.fc.parameters(), lr=0.001)  # Only train the final layer

# 4. Training Loop
num_epochs = 5
for epoch in range(num_epochs):
    resnet18.train()
    running_loss = 0.0

    for inputs, labels in trainloader:
        inputs, labels = inputs.to(device), labels.to(device)

        optimizer.zero_grad()
        outputs = resnet18(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()

    print(f"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(trainloader):.4f}")

print("Training complete.")

# 5. Testing Accuracy
resnet18.eval()
correct = 0
total = 0
with torch.no_grad():
    for inputs, labels in testloader:
        inputs, labels = inputs.to(device), labels.to(device)
        outputs = resnet18(inputs)
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

print(f"Test Accuracy: {100 * correct / total:.2f}%")

import matplotlib.pyplot as plt
import matplotlib.patches as patches

# Anchor box (predicted) and Ground truth
pred_box = [50, 50, 100, 100]  # x, y, width, height
gt_box = [60, 60, 80, 80]

def draw_box(ax, box, label, color):
    rect = patches.Rectangle((box[0], box[1]), box[2], box[3], linewidth=2, edgecolor=color, facecolor='none')
    ax.add_patch(rect)
    ax.text(box[0], box[1] - 10, label, color=color, fontsize=12)

fig, ax = plt.subplots()
draw_box(ax, pred_box, "Pred", 'blue')
draw_box(ax, gt_box, "GT", 'green')
plt.xlim(0, 200)
plt.ylim(0, 200)
plt.gca().invert_yaxis()
plt.title("Anchor Box (Blue) vs Ground Truth (Green)")
plt.show()

import matplotlib.pyplot as plt
import numpy as np

# Case Study Image: Just a blank placeholder for drawing boxes
image = np.ones((100, 100))

# Ground Truth box: [x1, y1, x2, y2]
gt_box = [30, 30, 70, 70]

# Anchor Boxes (proposed regions)
anchors = [
    [25, 25, 75, 75],   # Good match
    [40, 40, 90, 90],   # Partial overlap
    [10, 10, 50, 50]    # Poor match
]

# Predicted scores for each anchor (confidence)
scores = [0.9, 0.6, 0.3]

# IoU function
def iou(box1, box2):
    xi1, yi1 = max(box1[0], box2[0]), max(box1[1], box2[1])
    xi2, yi2 = min(box1[2], box2[2]), min(box1[3], box2[3])
    inter = max(0, xi2 - xi1) * max(0, yi2 - yi1)
    area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])
    area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])
    union = area1 + area2 - inter
    return inter / union if union != 0 else 0

# Visualize GT and Anchors
fig, ax = plt.subplots()
ax.imshow(image, cmap='gray')
ax.add_patch(plt.Rectangle((gt_box[0], gt_box[1]), gt_box[2] - gt_box[0], gt_box[3] - gt_box[1],
             edgecolor='green', lw=2, fill=False, label='GT'))

for i, box in enumerate(anchors):
    ax.add_patch(plt.Rectangle((box[0], box[1]), box[2]-box[0], box[3]-box[1],
                 edgecolor='red', linestyle='--', lw=1.5, fill=False))
    ax.text(box[0], box[1] - 3, f"A{i+1} ({scores[i]:.1f})", color='red', fontsize=7)

plt.title("GT and Anchor Boxes")
plt.axis('off')
plt.legend()
plt.show()

# Step 1: IoU
print("IoUs with Ground Truth:")
ious = [iou(box, gt_box) for box in anchors]
for i, val in enumerate(ious):
    print(f"Anchor {i+1}: IoU = {val:.2f}")

# Step 2: Sort predictions by score
sorted_preds = sorted(zip(anchors, scores, ious), key=lambda x: x[1], reverse=True)

# Step 3: Calculate TP, FP
tp, fp = [], []
matched = False
for box, score, iou_val in sorted_preds:
    if iou_val >= 0.5 and not matched:
        tp.append(1)
        fp.append(0)
        matched = True  # Only one GT
    else:
        tp.append(0)
        fp.append(1)

# Step 4: Precision, Recall
tp_cum = np.cumsum(tp)
fp_cum = np.cumsum(fp)
precision = tp_cum / (tp_cum + fp_cum)
recall = tp_cum / 1  # Only 1 GT

print("\nPrecision:", precision)
print("Recall:", recall)

# Step 5: Plot PR curve
plt.plot(recall, precision, marker='o')
plt.xlabel("Recall")
plt.ylabel("Precision")
plt.title("Precision-Recall Curve")
plt.grid(True)
plt.show()

# Step 6: mAP = area under PR curve
mAP = np.trapezoid(precision, recall)
print(f"\nmAP: {mAP:.2f}")

"""**Revision Sessions**

The following codes were used during revision sessions.
"""

# Sheet Link
# https://docs.google.com/spreadsheets/d/1JdWoWGBs-Ztj6Ig2KqRz-Afh8xf-QEJmV5e0SMXN1bQ/edit?gid=0#gid=0
# Linear and Polynomial Regression Template

# 1. Import Libraries
numpy, sklearn, matplotlib

# 2. Load Data
X
y

# 3. Fit the Regression
lin_reg = LinearRegression()
lin_reg.fit(X,y)
y_pred_lin = lin_reg.predict(X)

# Polynomial features
poly_features = PolynomialFeatures(degree=)
X_poly = poly_features.fit_transform(X)
poly_reg = LinearRegression()
poly_reg.fit(X_poly, y)
y_pred_poly = poly_reg.predict(X_poly)

# 4. Metrics
r2
mse

# 5. Plot the graph
plt.

# Linear and Polynomial Regression

import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures
from sklearn.metrics import mean_squared_error, r2_score

# Generate simple data
# Traffic jam severity (independent variable)
X = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]).reshape(-1, 1)

# Footfall per hour (dependent variable)
y = np.array([500, 345, 450, 420, 400, 421, 350, 330, 310, 456, 270])

# Linear Regression
lin_reg = LinearRegression()
lin_reg.fit(X, y)
y_pred_lin = lin_reg.predict(X)

# Polynomial Regression
poly_features = PolynomialFeatures(degree=8)
X_poly = poly_features.fit_transform(X)
poly_reg = LinearRegression()
poly_reg.fit(X_poly, y)
y_pred_poly = poly_reg.predict(X_poly)

# Metrics
print("Linear Regression:")
print("R² Score:", r2_score(y, y_pred_lin))
print("MSE:", mean_squared_error(y, y_pred_lin))

print("\nPolynomial Regression:")
print("R² Score:", r2_score(y, y_pred_poly))
print("MSE:", mean_squared_error(y, y_pred_poly))

# Sort for smooth plotting
X_sorted = np.sort(X, axis=0)
y_pred_lin_sorted = lin_reg.predict(X_sorted)
X_poly_sorted = poly_features.transform(X_sorted)
y_pred_poly_sorted = poly_reg.predict(X_poly_sorted)

# Plot 1: Linear Regression
plt.figure()
plt.scatter(X, y, color='blue', label='Original data')
plt.plot(X_sorted, y_pred_lin_sorted, color='green', label='Linear fit')
plt.legend()
plt.xlabel("X")
plt.ylabel("y")
plt.title("Linear Regression")
plt.grid(True)

# Plot 2: Polynomial Regression
plt.figure()
plt.scatter(X, y, color='blue', label='Original data')
plt.plot(X_sorted, y_pred_poly_sorted, color='red', label='Polynomial fit')
plt.legend()
plt.xlabel("X")
plt.ylabel("y")
plt.title("Polynomial Regression")
plt.grid(True)

plt.show()

# Home work Exercise
# Metro Interstate Traffic Volume
# Download the data set here
# https://archive.ics.uci.edu/dataset/492/metro+interstate+traffic+volume

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score

# Load the dataset
df = pd.read_csv('Metro_Interstate_Traffic_Volume.csv')

# Feature engineering: Convert date_time to hour and day of week
# df['date_time'] = pd.to_datetime(df['date_time'])
# df['hour'] = df['date_time'].dt.hour
# df['day_of_week'] = df['date_time'].dt.dayofweek

# Select relevant features
features = ['clouds_all'] # also experiment by adding others and by adding more than one
X = df[features]
y = df['traffic_volume']

# Train/test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Model
model = LinearRegression()
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

# Evaluation
print("R² Score:", r2_score(y_test, y_pred))
print("MSE:", mean_squared_error(y_test, y_pred))

# Visualization
plt.figure(figsize=(8, 6))
plt.scatter(y_test, y_pred, alpha=0.3, label='Predicted vs Actual', color='blue')


plt.xlabel('Actual Traffic Volume')
plt.ylabel('Predicted Traffic Volume')
plt.title('Actual vs Predicted Traffic Volume')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

# PageRank Example
# Link for sheet
# https://docs.google.com/spreadsheets/d/1AEnaRRDHRzVczEuL8RGekGRy94q6gaV7yihjZiWzBGk/edit?gid=827786899#gid=827786899

import networkx as nx

# Create a directed graph
G = nx.DiGraph()

# Add directed edges between nodes
edges = [
    ('A', 'B'),
    ('B', 'C'),
    ('C', 'A'),
    ('D', 'C'),
    ('E', 'C'),
    ('E', 'D')
]
G.add_edges_from(edges)

# Compute PageRank
pagerank_scores = nx.pagerank(G, alpha=0.85)

# Print results
print("PageRank scores:")
for node, score in pagerank_scores.items():
    print(f"{node}: {score:.4f}")

# PageRank
# Stanford Web Graph
# Link: https://snap.stanford.edu/data/web-Stanford.html

import gzip
import networkx as nx

# Initialize directed graph
G = nx.DiGraph()

# Path to the downloaded .gz file
file_path = "web-Stanford.txt.gz"

# Load the graph from file
with gzip.open(file_path, 'rt') as f:
    for line in f:
        if line.startswith("#"):
            continue  # Skip comments
        src, dst = map(int, line.strip().split())
        G.add_edge(src, dst)

print(f"Loaded graph with {G.number_of_nodes()} nodes and {G.number_of_edges()} edges")

# Compute PageRank
pagerank_scores = nx.pagerank(G, alpha=0.85)

# Display top 10 pages by PageRank
top_10 = sorted(pagerank_scores.items(), key=lambda x: x[1], reverse=True)[:10]
print("Top 10 nodes by PageRank:")
for node, score in top_10:
    print(f"Node {node}: {score:.6f}")

# Perceptron
# Working With Mushroom data set
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.metrics import accuracy_score, classification_report

# Load and preprocess Mushroom dataset
url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/mushroom/agaricus-lepiota.data'
columns = [
    'class', 'cap-shape', 'cap-surface', 'cap-color', 'bruises', 'odor',
    'gill-attachment', 'gill-spacing', 'gill-size', 'gill-color',
    'stalk-shape', 'stalk-root', 'stalk-surface-above-ring',
    'stalk-surface-below-ring', 'stalk-color-above-ring',
    'stalk-color-below-ring', 'veil-type', 'veil-color', 'ring-number',
    'ring-type', 'spore-print-color', 'population', 'habitat'
]

df = pd.read_csv(url, header=None, names=columns)
df = df.replace('?', np.nan).dropna()

# Separate features and labels
X = df.drop('class', axis=1)
y = (df['class'] == 'p').astype(int)  # 1 = poisonous, 0 = edible

# One-hot encode all categorical features
encoder = OneHotEncoder(sparse_output=False)
X_encoded = encoder.fit_transform(X)

# Standardize the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_encoded)

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y.values, test_size=0.2, random_state=42)

# Sigmoid function
def sigmoid(z):
    return 1 / (1 + np.exp(-z))

# Binary cross-entropy loss
def compute_loss(y_true, y_pred):
    epsilon = 1e-10  # to avoid log(0)
    return -np.mean(y_true * np.log(y_pred + epsilon) + (1 - y_true) * np.log(1 - y_pred + epsilon))

# Gradient Descent Training
def train_gradient_descent(X, y, lr=0.1, epochs=1000):
    m, n = X.shape
    weights = np.zeros(n)
    bias = 0

    for epoch in range(epochs):
        linear_output = np.dot(X, weights) + bias
        predictions = sigmoid(linear_output)

        # Compute gradients
        dw = np.dot(X.T, (predictions - y)) / m
        db = np.sum(predictions - y) / m

        # Update weights
        weights -= lr * dw
        bias -= lr * db

        # Optionally print loss every 100 epochs
        if epoch % 100 == 0:
            loss = compute_loss(y, predictions)
            print(f"Epoch {epoch}, Loss: {loss:.4f}")

    return weights, bias

# Train the model
weights, bias = train_gradient_descent(X_train, y_train, lr=0.5, epochs=1000)

# Prediction
def predict(X, weights, bias, threshold=0.5):
    return (sigmoid(np.dot(X, weights) + bias) >= threshold).astype(int)

# Evaluate
y_pred = predict(X_test, weights, bias)
print("\nAccuracy:", accuracy_score(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred, target_names=['Edible', 'Poisonous']))

# Crearting a pipeline for Mushroom dataset

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer

# Load mushroom dataset
url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/mushroom/agaricus-lepiota.data'
columns = [
    'class', 'cap-shape', 'cap-surface', 'cap-color', 'bruises', 'odor',
    'gill-attachment', 'gill-spacing', 'gill-size', 'gill-color', 'stalk-shape',
    'stalk-root', 'stalk-surface-above-ring', 'stalk-surface-below-ring',
    'stalk-color-above-ring', 'stalk-color-below-ring', 'veil-type', 'veil-color',
    'ring-number', 'ring-type', 'spore-print-color', 'population', 'habitat'
]
df = pd.read_csv(url, header=None, names=columns)

# Clean data
df = df.replace('?', np.nan).dropna()

# Separate features and label
X = df.drop('class', axis=1)
y = (df['class'] == 'p').astype(int)  # Poisonous = 1, Edible = 0

# One-hot encoding and scaling using a pipeline
categorical_features = X.columns.tolist()
preprocessor = ColumnTransformer([
    ("onehot", OneHotEncoder(handle_unknown="ignore"), categorical_features)
])

# Create full pipeline with logistic regression
pipeline = Pipeline([
    ("preprocessing", preprocessor),
    ("scaling", StandardScaler(with_mean=False)),  # use with_mean=False for sparse output
    ("classifier", LogisticRegression(solver='lbfgs', max_iter=1000))
])

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train
pipeline.fit(X_train, y_train)

# Predict and evaluate
y_pred = pipeline.predict(X_test)

print("Accuracy:", accuracy_score(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred, target_names=["Edible", "Poisonous"]))

"""**Dimensionality Reduction**

Dimensionality reduction involves reducing the number of input features in a dataset to simplify analysis and improve performance. It includes feature selection (choosing key variables) and feature extraction (creating new, compact features). This process helps avoid the curse of dimensionality, reduces overfitting, improves model training time, enhances visualization, and filters out noise. A popular technique is Principal Component Analysis (PCA), which linearly transforms data into uncorrelated components (principal components) ordered by variance. Key ideas in PCA include the covariance matrix, eigenvectors (direction), eigenvalues (variance magnitude), and the explained variance ratio, which shows each component’s contribution to the total variance.

**Sheet Link:**
https://docs.google.com/spreadsheets/d/1_myOs1K_vTUC1EbcjUy9K6fq1iRfdKDTOWZSANLg0dU/edit?gid=0#gid=0
"""

# Part 01: Understanding the need
# =================================

# Learning with a synthetic data

import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA

# Create synthetic 2D dataset
np.random.seed(0)
x = np.random.normal(0, 1, 100)
y = 2 * x + np.random.normal(0, 1, 100)
X = np.vstack((x, y)).T

# Plot original data
plt.scatter(X[:,0], X[:,1], alpha=0.5)
plt.title("Original 2D Data")
plt.xlabel("x")
plt.ylabel("y")
plt.axis('equal')
plt.grid(True)
plt.show()

# What does this output mean? Explain

pca = PCA(n_components=2)
X_pca = pca.fit_transform(X)

print("Explained Variance Ratio:", pca.explained_variance_ratio_)
plt.scatter(X_pca[:,0], X_pca[:,1], alpha=0.5)
plt.title("Data after PCA")
plt.xlabel("PC1")
plt.ylabel("PC2")
plt.grid(True)
plt.axis('equal')
plt.show()

# Part 2 - Asking Questions
# ========================================================
# Dataset Details - Data set from scikit-learn
# Name: Digits Dataset
# Content: 8×8 grayscale images of handwritten digits (0–9)

# What do you infer from below code?

import numpy as np
from sklearn.datasets import load_digits
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Load dataset
digits = load_digits()
X = digits.data
y = digits.target

# Standardize features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# ----------------- Without PCA -----------------
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y)
clf = LogisticRegression(max_iter=1000)
clf.fit(X_train, y_train)
print("Accuracy without PCA:", accuracy_score(y_test, clf.predict(X_test)))

# ----------------- With PCA -----------------
pca = PCA(n_components=20)
X_pca = pca.fit_transform(X_scaled)
X_train_pca, X_test_pca, y_train_pca, y_test_pca = train_test_split(X_pca, y, random_state=42)
clf.fit(X_train_pca, y_train_pca)
print("Accuracy with PCA:", accuracy_score(y_test_pca, clf.predict(X_test_pca)))

# California Housing Dataset
# Link: https://inria.github.io/scikit-learn-mooc/python_scripts/datasets_california_housing.html

import numpy as np
import pandas as pd
import time
from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.metrics import mean_squared_error

# Load dataset
data = fetch_california_housing()
X = pd.DataFrame(data.data, columns=data.feature_names)
y = pd.Series(data.target)

# Standardize features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# ----------------- Without PCA -----------------
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, random_state=42)

start_time_no_pca = time.time()
model_no_pca = LinearRegression()
model_no_pca.fit(X_train, y_train)
end_time_no_pca = time.time()

y_pred_no_pca = model_no_pca.predict(X_test)
mse_no_pca = mean_squared_error(y_test, y_pred_no_pca)
training_time_no_pca = end_time_no_pca - start_time_no_pca

print("Without PCA:")
print("MSE:", mse_no_pca)
print("Training Time:", training_time_no_pca)

# ----------------- With PCA -----------------
pca = PCA(n_components=5)
X_pca = pca.fit_transform(X_scaled)
X_train_pca, X_test_pca, y_train_pca, y_test_pca = train_test_split(X_pca, y, random_state=42)

start_time_pca = time.time()
model_pca = LinearRegression()
model_pca.fit(X_train_pca, y_train_pca)
end_time_pca = time.time()

y_pred_pca = model_pca.predict(X_test_pca)
mse_pca = mean_squared_error(y_test_pca, y_pred_pca)
training_time_pca = end_time_pca - start_time_pca

print("\nWith PCA:")
print("MSE:", mse_pca)
print("Training Time:", training_time_pca)

# Breast Cancer Wisconsin dataset
# Link: https://www.kaggle.com/datasets/uciml/breast-cancer-wisconsin-data

from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
import time

# Load data
data = load_breast_cancer()
X, y = data.data, data.target

# Train/test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Standardize features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# ---- Without PCA ----
start_time = time.time()
lr = LogisticRegression(max_iter = 1000)
lr.fit(X_train_scaled, y_train)
preds = lr.predict(X_test_scaled)
end_time = time.time()
accuracy_no_pca = accuracy_score(y_test, preds)
time_no_pca = end_time - start_time

# ---- With PCA ----
pca = PCA(n_components=10)
X_train_pca = pca.fit_transform(X_train_scaled)
X_test_pca = pca.transform(X_test_scaled)

start_time = time.time()
lr_pca = LogisticRegression(max_iter = 1000)
lr_pca.fit(X_train_pca, y_train)
preds_pca = lr_pca.predict(X_test_pca)
end_time = time.time()
accuracy_pca = accuracy_score(y_test, preds_pca)
time_pca = end_time - start_time

# ---- Results ----
print(f"Without PCA - Accuracy: {accuracy_no_pca:.4f}, Time: {time_no_pca:.4f} seconds")
print(f"With PCA    - Accuracy: {accuracy_pca:.4f}, Time: {time_pca:.4f} seconds")

# IRIS data set visualization
# Can we make it better?

from sklearn.datasets import load_iris
import seaborn as sns
import pandas as pd
import matplotlib.pyplot as plt

# Load dataset
iris = load_iris()
X = iris.data
y = iris.target
feature_names = iris.feature_names
target_names = iris.target_names

# Create DataFrame including labels
df = pd.DataFrame(X, columns=feature_names)
df['Species'] = [target_names[i] for i in y]

# Plot pairwise scatterplots
sns.pairplot(df, hue='Species', palette='Set2', markers=["o", "s", "D"])
plt.suptitle("Pairwise Scatterplots of Iris Dataset Features", y=1.02)
plt.show()

# How can we do better?
# t-SNE?

from sklearn.datasets import load_iris
from sklearn.manifold import TSNE
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Load dataset
iris = load_iris()
X = iris.data
y = iris.target
target_names = iris.target_names

# Standardize the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Apply t-SNE
tsne = TSNE(n_components=2, random_state=42, perplexity = 30, max_iter = 1000)
X_embedded = tsne.fit_transform(X_scaled)

# Create DataFrame for plotting
df_tsne = pd.DataFrame()
df_tsne["Component 1"] = X_embedded[:, 0]
df_tsne["Component 2"] = X_embedded[:, 1]
df_tsne["Label"] = [target_names[i] for i in y]

# Plot
plt.figure(figsize=(8, 6))
sns.scatterplot(data=df_tsne, x="Component 1", y="Component 2", hue="Label", palette="Set2", s=100)
plt.title("t-SNE on Iris Dataset")
plt.show()

# The 20 newsgroups text dataset
# Link: https://scikit-learn.org/0.19/datasets/twenty_newsgroups.html
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.datasets import fetch_20newsgroups
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from torch.utils.data import TensorDataset, DataLoader

# 1. Load text data (subset for speed)
categories = ['rec.sport.baseball', 'sci.med', 'comp.graphics', 'talk.politics.misc']
data = fetch_20newsgroups(subset='all', categories=categories, remove=('headers', 'footers', 'quotes'))

# 2. Vectorize text using TF-IDF
vectorizer = TfidfVectorizer(max_features=2000, stop_words='english')
X = vectorizer.fit_transform(data.data).toarray()
y = data.target

# 3. Train/test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 4. Convert to tensors
X_train_tensor = torch.tensor(X_train, dtype=torch.float32)
X_test_tensor = torch.tensor(X_test, dtype=torch.float32)
y_train_tensor = torch.tensor(y_train, dtype=torch.long)
y_test_tensor = torch.tensor(y_test, dtype=torch.long)

# 5. DataLoader
train_ds = TensorDataset(X_train_tensor, y_train_tensor)
test_ds = TensorDataset(X_test_tensor, y_test_tensor)
train_dl = DataLoader(train_ds, batch_size=32, shuffle=True)
test_dl = DataLoader(test_ds, batch_size=32)

# 6. Feedforward NN model
model = nn.Sequential(
    nn.Linear(2000,512),
    nn.ReLU(),
    nn.Linear(512,128),
    nn.ReLU(),
    nn.Linear(128,len(categories))
)

# 7. Training setup
loss_fn = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# 8. Training loop
for epoch in range(5):
    model.train()
    total_loss, correct = 0, 0
    for xb, yb in train_dl:
        optimizer.zero_grad()
        out = model(xb)
        loss = loss_fn(out, yb)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
        correct += (out.argmax(1) == yb).sum().item()
    acc = correct / len(train_ds)
    print(f"Epoch {epoch+1}: Loss={total_loss:.2f}, Accuracy={acc:.4f}")

# IRIS dataset
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Load and preprocess data
iris = load_iris()
X = iris.data
y = iris.target

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Standardize features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Convert to tensors
X_train = torch.tensor(X_train, dtype=torch.float32)
y_train = torch.tensor(y_train, dtype=torch.long)
X_test = torch.tensor(X_test, dtype=torch.float32)
y_test = torch.tensor(y_test, dtype=torch.long)

# Define model
model = nn.Sequential(
    nn.Linear(4, 10),
    nn.ReLU(),
    nn.Linear(10,3)
)

# Loss and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.01)

# Training loop
for epoch in range(300):
    # Forward
    outputs = model(X_train)
    loss = criterion(outputs, y_train)

    # Backward
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    if epoch % 50 == 0:
        print(f"Epoch {epoch}, Loss: {loss.item():.4f}")

# Evaluation
with torch.no_grad():
    test_outputs = model(X_test)
    predicted = torch.argmax(test_outputs, dim=1)
    accuracy = (predicted == y_test).float().mean()
    print(f"\nTest Accuracy: {accuracy:.2f}")

# Task: Build an efficient Neural Network for Wine Data Set
# Link: https://archive.ics.uci.edu/dataset/109/wine
# Use 50 Epochs at max

import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.datasets import load_wine
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from torch.utils.data import TensorDataset, DataLoader

# Load and preprocess data
data = load_wine()
X = data.data
y = data.target

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

X_train = torch.tensor(X_train, dtype=torch.float32)
X_test = torch.tensor(X_test, dtype=torch.float32)
y_train = torch.tensor(y_train, dtype=torch.long)
y_test = torch.tensor(y_test, dtype=torch.long)

train_ds = TensorDataset(X_train, y_train)
test_ds = TensorDataset(X_test, y_test)

train_loader = DataLoader(train_ds, batch_size=16, shuffle=True)
test_loader = DataLoader(test_ds, batch_size=16)

# Define model
class WineNN(nn.Module):
    def __init__(self):
        super(WineNN, self).__init__()
        self.model = nn.Sequential(
            nn.Linear(13, 16),
            nn.ReLU(),
            nn.Linear(16, 8),
            nn.ReLU(),
            nn.Linear(8, 3)
        )

    def forward(self, x):
        return self.model(x)

model = WineNN()

# Training setup
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.01)
epochs = 50

# Training loop
for epoch in range(1, epochs + 1):
    model.train()
    running_loss = 0.0
    for xb, yb in train_loader:
        preds = model(xb)
        loss = criterion(preds, yb)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        running_loss += loss.item()

    # Print every 10 epochs
    if epoch % 10 == 0:
        model.eval()
        correct = 0
        total = 0
        with torch.no_grad():
            for xb, yb in test_loader:
                preds = model(xb)
                _, predicted = torch.max(preds, 1)
                total += yb.size(0)
                correct += (predicted == yb).sum().item()
        avg_loss = running_loss / len(train_loader)
        accuracy = correct / total
        print(f"Epoch {epoch:2d}: Loss = {avg_loss:.4f}, Accuracy = {accuracy:.4f}")

# Part 1
# Build a CNN using Keras
model = Sequential([
    Conv2D(32, (3, 3), activation = 'relu', input_shape(28, 28, 1)),
    MaxPooling2D(2,2),
    Flatten(),
    Dense(10, activation = 'softmax')
])

# Part 2
# Equivalent PyTorch code
import torch
import torch.nn as nn
import torch.nn.functional as F

class CNNModel(nn.Module):
    def __init__(self):
        super(CNNModel, self).__init__()
        self.conv = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1)  # 28x28 → 28x28
        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)                                 # 28x28 → 14x14
        self.flattened_size = 32 * 14 * 14
        self.fc = nn.Linear(self.flattened_size, 10)

    def forward(self, x):
        x = F.relu(self.conv(x))                      # Conv + ReLU
        x = self.pool(x)                              # MaxPooling
        x = x.view(-1, self.flattened_size)           # Flatten
        x = F.softmax(self.fc(x), dim=1)              # Dense + Softmax
        return x

# Create model
model = CNNModel()

"""**Working with Fashion MNIST dataset**

Link: https://www.tensorflow.org/datasets/catalog/fashion_mnist

**Description:**

The Fashion MNIST dataset is a collection of 70,000 grayscale images of 28×28 pixels, depicting 10 categories of clothing items such as shirts, shoes, bags, and dresses. Designed as a more complex alternative to the classic MNIST handwritten digit dataset, Fashion MNIST offers richer visual features while maintaining the same image size and format, making it ideal for benchmarking machine learning algorithms. Widely used for testing image classification models, it helps researchers evaluate their techniques on realistic yet manageable data. The dataset is conveniently accessible through TensorFlow’s Keras API, facilitating easy integration into deep learning workflows.

**PART 01: Building Blocks**

Apply:
* A manual convolution using NumPy (for intuition)
* A ReLU function manually
* Max pooling manually


**PART 02: Build a CNN**

This CNN includes:

* Convolution + ReLU
* Max Pooling
* Flatten
* Dense layers
* Output layer with softmax

**Part 03: Visulaize and interpret**

Here, we,
* Visualize filters/feature maps from the first convolution layer.
* Show how the network progressively focuses on shapes, edges, etc.


**Part 04: Experiment**

Explore and work aound:
* Change number of filters, kernel size, or activation function.
* Observe how performance or feature maps change.
* Try adding an extra convolution + pooling layer.
"""

# ------------------------------------------------------------------------------
#                                 PART 01
# ------------------------------------------------------------------------------
import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.datasets import fashion_mnist

# Load Fashion MNIST data
(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()
image = train_images[0]  # Use the first image

# --- Utility: Normalize output to [0, 1] for better visualization ---
def normalize(img):
    img = img - np.min(img)
    max_val = np.max(img)
    if max_val != 0:
        img = img / max_val
    return img

# --- Manual Convolution ---
def manual_convolution(image, kernel, stride=1):
    kh, kw = kernel.shape
    h, w = image.shape
    out_h = (h - kh) // stride + 1
    out_w = (w - kw) // stride + 1
    output = np.zeros((out_h, out_w))

    for y in range(out_h):
        for x in range(out_w):
            patch = image[y*stride:y*stride+kh, x*stride:x*stride+kw]
            output[y, x] = np.sum(patch * kernel)
    return output

# --- Manual ReLU ---
def manual_relu(feature_map):
    return np.maximum(0, feature_map)

# --- Manual Max Pooling ---
def manual_max_pooling(feature_map, size=2, stride=2):
    h, w = feature_map.shape
    out_h = (h - size) // stride + 1
    out_w = (w - size) // stride + 1
    pooled = np.zeros((out_h, out_w))

    for y in range(out_h):
        for x in range(out_w):
            patch = feature_map[y*stride:y*stride+size, x*stride:x*stride+size]
            pooled[y, x] = np.max(patch)
    return pooled

# Define edge detection kernel
kernel = np.array([
    [-1, 0, 1],
    [-2, 0, 2],
    [-1, 0, 1]
])

# --- Apply Steps ---
conv_output = manual_convolution(image, kernel)
relu_output = manual_relu(conv_output)
pooled_output = manual_max_pooling(relu_output)

# --- Plot All Steps ---
plt.figure(figsize=(12, 4))

plt.subplot(1, 4, 1)
plt.imshow(image, cmap='gray')
plt.title("Original")

plt.subplot(1, 4, 2)
plt.imshow(normalize(conv_output), cmap='gray')
plt.title("Convolution")

plt.subplot(1, 4, 3)
plt.imshow(normalize(relu_output), cmap='gray')
plt.title("ReLU")

plt.subplot(1, 4, 4)
plt.imshow(normalize(pooled_output), cmap='gray')
plt.title("Max Pool")

plt.tight_layout()
plt.show()

# ------------------------------------------------------------------------------
#                                 PART 02
# ------------------------------------------------------------------------------

import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.datasets import fashion_mnist
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense
from tensorflow.keras.utils import to_categorical

# 1. Load and preprocess data
(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()

# Normalize images to [0,1] and reshape for CNN input (28x28x1)
x_train = x_train.astype('float32') / 255.0
x_test = x_test.astype('float32') / 255.0

x_train = np.expand_dims(x_train, -1)
x_test = np.expand_dims(x_test, -1)

# One-hot encode labels
y_train = to_categorical(y_train, 10)
y_test = to_categorical(y_test, 10)

# 2. Define Mini CNN Model
model = Sequential([
    Conv2D(16, (3, 3), activation='relu', input_shape=(28, 28, 1)),
    MaxPooling2D(pool_size=(2, 2)),

    Conv2D(32, (3, 3), activation='relu'),
    MaxPooling2D(pool_size=(2, 2)),

    Flatten(),
    Dense(64, activation='relu'),
    Dense(10, activation='softmax')
])

# 3. Compile model
model.compile(optimizer='adam',
              loss='categorical_crossentropy',
              metrics=['accuracy'])

# 4. Train model
model.fit(x_train, y_train, epochs=5, batch_size=64, validation_split=0.1)

# 5. Evaluate model
test_loss, test_acc = model.evaluate(x_test, y_test)
print(f"Test Accuracy: {test_acc:.4f}")

# ------------------------------------------------------------------------------
#                                 PART 03
# ------------------------------------------------------------------------------

import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.datasets import fashion_mnist
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense
from tensorflow.keras.utils import to_categorical

# Load and preprocess Fashion MNIST data
(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()
x_train = x_train.astype("float32") / 255.0
x_train = np.expand_dims(x_train, -1)
y_train = to_categorical(y_train, 10)

# ---------------------------------------
# Build model using Functional API
# ---------------------------------------
input_layer = Input(shape=(28, 28, 1))
x = Conv2D(16, (3, 3), activation='relu', name="conv1")(input_layer)
x = MaxPooling2D((2, 2))(x)
x = Conv2D(32, (3, 3), activation='relu')(x)
x = MaxPooling2D((2, 2))(x)
x = Flatten()(x)
x = Dense(64, activation='relu')(x)
output_layer = Dense(10, activation='softmax')(x)

model = Model(inputs=input_layer, outputs=output_layer)

# Compile and train briefly
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
model.fit(x_train[:10000], y_train[:10000], epochs=2, batch_size=64)

# -------------------------------
# 🔍 Part A: Visualize Filters
# -------------------------------
filters, biases = model.get_layer("conv1").get_weights()
filters = (filters - filters.min()) / (filters.max() - filters.min())  # Normalize

plt.figure(figsize=(8, 4))
for i in range(6):
    f = filters[:, :, 0, i]
    plt.subplot(1, 6, i+1)
    plt.imshow(f, cmap='gray')
    plt.title(f'Filter {i+1}')
    plt.axis('off')
plt.suptitle("First Conv Layer Filters")
plt.tight_layout()
plt.show()

# -------------------------------
# 🔍 Part B: Visualize Feature Maps
# -------------------------------
# New model that outputs after conv1
feature_extractor = Model(inputs=model.input, outputs=model.get_layer("conv1").output)

# Pick one image
sample_img = x_train[0:1]
feature_maps = feature_extractor.predict(sample_img)

plt.figure(figsize=(12, 4))
for i in range(6):
    plt.subplot(1, 6, i+1)
    plt.imshow(feature_maps[0, :, :, i], cmap='gray')
    plt.title(f'Map {i+1}')
    plt.axis('off')
plt.suptitle("Feature Maps after Conv1")
plt.tight_layout()
plt.show()

# ------------------------------------------------------------------------------
#                                 PART 04
# ------------------------------------------------------------------------------

# Experiment with the code
# Can we improve the accuracy?

import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.datasets import fashion_mnist
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense
from tensorflow.keras.utils import to_categorical

# 1. Load and preprocess data
(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()

# Normalize images to [0,1] and reshape for CNN input (28x28x1)
x_train = x_train.astype('float32') / 255.0
x_test = x_test.astype('float32') / 255.0

x_train = np.expand_dims(x_train, -1)
x_test = np.expand_dims(x_test, -1)

# One-hot encode labels
y_train = to_categorical(y_train, 10)
y_test = to_categorical(y_test, 10)

# 2. Define Mini CNN Model
model = Sequential([
    Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),
    MaxPooling2D(pool_size=(2, 2)),

    Conv2D(64, (3, 3), activation='relu'),
    MaxPooling2D(pool_size=(2, 2)),

    Conv2D(64, (3, 3), activation='relu'),
    MaxPooling2D(pool_size=(2, 2)),

    Flatten(),
    Dense(64, activation='relu'),
    Dense(10, activation='softmax')
])

# 3. Compile model
model.compile(optimizer='adam',
              loss='categorical_crossentropy',
              metrics=['accuracy'])

# 4. Train model
model.fit(x_train, y_train, epochs=5, batch_size=64, validation_split=0.1)

# 5. Evaluate model
test_loss, test_acc = model.evaluate(x_test, y_test)
print(f"Test Accuracy: {test_acc:.4f}")

# Using CNN for time series data

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense

# Download Air Passengers data CSV
url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/airline-passengers.csv'
data = pd.read_csv(url, usecols=[1])
passengers = data.values.astype(float)

# Normalize data
scaler = MinMaxScaler(feature_range=(0, 1))
passengers_scaled = scaler.fit_transform(passengers)

# Create sequences for supervised learning
def create_sequences(data, seq_length):
    X, y = [], []
    for i in range(len(data) - seq_length):
        X.append(data[i:i+seq_length])
        y.append(data[i+seq_length])
    return np.array(X), np.array(y)

SEQ_LENGTH = 12  # Use 12 months to predict next month
X, y = create_sequences(passengers_scaled, SEQ_LENGTH)

# Reshape X for Conv1D: (samples, time steps, features)
X = X.reshape((X.shape[0], X.shape[1], 1))

# Split into train and test sets (80% train)
split = int(len(X) * 0.8)
X_train, X_test = X[:split], X[split:]
y_train, y_test = y[:split], y[split:]

# Build CNN model
model = Sequential([
    Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(SEQ_LENGTH, 1)),
    MaxPooling1D(pool_size=2),
    Flatten(),
    Dense(50, activation='relu'),
    Dense(1)
])

model.compile(optimizer='adam', loss='mse')
model.summary()

# Train model
history = model.fit(X_train, y_train, epochs=50, batch_size=16, validation_data=(X_test, y_test), verbose=2)

# Predict on test set
y_pred = model.predict(X_test)

# Inverse transform to original scale
y_test_inv = scaler.inverse_transform(y_test)
y_pred_inv = scaler.inverse_transform(y_pred)

# Plot true vs predicted
plt.figure(figsize=(10,6))
plt.plot(y_test_inv, label='True')
plt.plot(y_pred_inv, label='Predicted')
plt.title("Air Passengers Forecast: True vs Predicted")
plt.xlabel("Time steps")
plt.ylabel("Number of Passengers")
plt.legend()
plt.show()

"""**CNN Architectures**

LeNet, AlexNet, and VGG are landmark CNN architectures that shaped the evolution of deep learning. LeNet (1998), developed by Yann LeCun, was designed for digit recognition and introduced the core CNN structure with convolution, pooling, and fully connected layers. AlexNet (2012) revived deep learning by winning the ImageNet challenge using ReLU activations, dropout, and GPU acceleration. It marked a significant leap in performance. VGG (2014) emphasized simplicity and depth by stacking small 3×3 convolution filters, creating very deep yet uniform networks. Each model reflects a step forward in scalability, performance, and design clarity in visual recognition tasks.
"""

# LeNet Architecture

class LeNet(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv_layers = nn.Sequential(
            nn.Conv2d(1, 6, kernel_size=5),
            nn.Tanh(),
            nn.AvgPool2d(2),
            nn.Conv2d(6, 16, kernel_size=5),
            nn.Tanh(),
            nn.AvgPool2d(2)
        )
        self.fc_layers = nn.Sequential(
            nn.Linear(16 * 4 * 4, 120),
            nn.Tanh(),
            nn.Linear(120, 84),
            nn.Tanh(),
            nn.Linear(84, 10)
        )
    def forward(self, x):
        x = self.conv_layers(x)
        x = x.view(x.size(0), -1)
        return self.fc_layers(x)

# AlexNet Architecture

class AlexNet(nn.Module):
    def __init__(self, num_classes=1000):
        super().__init__()
        self.features = nn.Sequential(
            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),
            nn.ReLU(),
            nn.MaxPool2d(3, 2),
            nn.Conv2d(64, 192, 5, padding=2),
            nn.ReLU(),
            nn.MaxPool2d(3, 2),
            nn.Conv2d(192, 384, 3, padding=1),
            nn.ReLU(),
            nn.Conv2d(384, 256, 3, padding=1),
            nn.ReLU(),
            nn.Conv2d(256, 256, 3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(3, 2),
        )
        self.classifier = nn.Sequential(
            nn.Dropout(),
            nn.Linear(256 * 6 * 6, 4096),
            nn.ReLU(),
            nn.Dropout(),
            nn.Linear(4096, 4096),
            nn.ReLU(),
            nn.Linear(4096, num_classes),
        )

    def forward(self, x):
        x = self.features(x)
        x = x.view(x.size(0), -1)
        return self.classifier(x)

# VGG Architecture

import torch
import torch.nn as nn

class VGG16(nn.Module):
    def __init__(self, num_classes=10):  # Customizable output
        super().__init__()

        self.features = nn.Sequential(
            # Block 1
            nn.Conv2d(3, 64, 3, padding=1),
            nn.ReLU(),
            nn.Conv2d(64, 64, 3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2, 2),

            # Block 2
            nn.Conv2d(64, 128, 3, padding=1),
            nn.ReLU(),
            nn.Conv2d(128, 128, 3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2, 2),

            # Block 3
            nn.Conv2d(128, 256, 3, padding=1),
            nn.ReLU(),
            nn.Conv2d(256, 256, 3, padding=1),
            nn.ReLU(),
            nn.Conv2d(256, 256, 3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2, 2),

            # Block 4
            nn.Conv2d(256, 512, 3, padding=1),
            nn.ReLU(),
            nn.Conv2d(512, 512, 3, padding=1),
            nn.ReLU(),
            nn.Conv2d(512, 512, 3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2, 2),

            # Block 5
            nn.Conv2d(512, 512, 3, padding=1),
            nn.ReLU(),
            nn.Conv2d(512, 512, 3, padding=1),
            nn.ReLU(),
            nn.Conv2d(512, 512, 3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2, 2)
        )

        self.classifier = nn.Sequential(
            nn.Linear(512 * 7 * 7, 4096),
            nn.ReLU(),
            nn.Dropout(),

            nn.Linear(4096, 4096),
            nn.ReLU(),
            nn.Dropout(),

            nn.Linear(4096, num_classes)
        )

    def forward(self, x):
        x = self.features(x)
        x = x.view(x.size(0), -1)  # Flatten
        x = self.classifier(x)
        return x

# Working with CIFAR-10 dataset
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms

# Device config
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# CIFAR-10 dataset (no resizing)
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))
])

trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)
trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True)

testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)
testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False)

# Simple CNN Model
class SimpleCNN(nn.Module):
    def __init__(self):
        super(SimpleCNN, self).__init__()
        self.features = nn.Sequential(
            nn.Conv2d(3, 32, 3, padding=1),  # 32x32x32
            nn.ReLU(),
            nn.MaxPool2d(2,2),               # 32x16x16

            nn.Conv2d(32, 64, 3, padding=1), # 64x16x16
            nn.ReLU(),
            nn.MaxPool2d(2,2),               # 64x8x8

            nn.Conv2d(64, 128, 3, padding=1), # 128x8x8
            nn.ReLU(),
            nn.MaxPool2d(2,2)                # 128x4x4
        )
        self.classifier = nn.Sequential(
            nn.Flatten(),
            nn.Linear(128*4*4, 256),
            nn.ReLU(),
            nn.Linear(256, 10)
        )

    def forward(self, x):
        x = self.features(x)
        x = self.classifier(x)
        return x

model = SimpleCNN().to(device)

criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Training function
def train(epoch):
    model.train()
    running_loss = 0
    correct = 0
    total = 0
    for inputs, labels in trainloader:
        inputs, labels = inputs.to(device), labels.to(device)

        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()
        _, predicted = outputs.max(1)
        total += labels.size(0)
        correct += predicted.eq(labels).sum().item()

    print(f"Epoch {epoch}: Loss={running_loss/len(trainloader):.4f}, Accuracy={100.*correct/total:.2f}%")

# Testing function
def test():
    model.eval()
    correct = 0
    total = 0
    with torch.no_grad():
        for inputs, labels in testloader:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            _, predicted = outputs.max(1)
            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()
    print(f"Test Accuracy: {100.*correct/total:.2f}%")

# Train for 10 epochs
for epoch in range(1, 11):
    train(epoch)
test()

# CIFAR-10 with AlexNet

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
from torchvision.models import alexnet

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# Transform including resize for AlexNet input size
transform = transforms.Compose([
    transforms.Resize(224),
    transforms.ToTensor(),
    transforms.Normalize((0.485,0.456,0.406), (0.229,0.224,0.225))  # Imagenet mean/std
])

trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)
trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)

testset = torchvision.datasets.CIFAR10(root='./data', train=False,download=True, transform=transform)
testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)

# Load pretrained AlexNet
model = alexnet(pretrained=True)

# Replace last layer for CIFAR-10 (10 classes)
model.classifier[6] = nn.Linear(4096, 10)

model = model.to(device)

criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.0001)  # Lower LR for finetuning

def train(epoch):
    model.train()
    running_loss = 0
    correct = 0
    total = 0
    for inputs, labels in trainloader:
        inputs, labels = inputs.to(device), labels.to(device)
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()
        _, predicted = outputs.max(1)
        total += labels.size(0)
        correct += predicted.eq(labels).sum().item()
    print(f"Epoch {epoch}: Loss={running_loss/len(trainloader):.4f}, Accuracy={100.*correct/total:.2f}%")

def test():
    model.eval()
    correct = 0
    total = 0
    with torch.no_grad():
        for inputs, labels in testloader:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            _, predicted = outputs.max(1)
            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()
    print(f"Test Accuracy: {100.*correct/total:.2f}%")

# Train for 10 epochs
for epoch in range(1, 11):
    train(epoch)
test()

"""**Support Vector Machines - Revision**


Let us walk through an example to understand from the very basics and build the story right.


Sheet Link:

https://docs.google.com/spreadsheets/d/16uXYgwFC2-TulHT3Iy0TiACetTx7f2eC7WINDrBubIY/edit?gid=915190021#gid=915190021

"""

# 1. Generate synthetic spiral data
#======================================

import numpy as np
import matplotlib.pyplot as plt

def generate_spiral(n_points, noise=0.5):
    n = np.sqrt(np.random.rand(n_points)) * 780 * (2*np.pi)/360
    d1x = -np.cos(n)*n + np.random.rand(n_points) * noise
    d1y = np.sin(n)*n + np.random.rand(n_points) * noise
    X1 = np.vstack((d1x, d1y)).T
    y1 = np.zeros(n_points)

    d2x = np.cos(n)*n + np.random.rand(n_points) * noise
    d2y = -np.sin(n)*n + np.random.rand(n_points) * noise
    X2 = np.vstack((d2x, d2y)).T
    y2 = np.ones(n_points)

    X = np.vstack((X1, X2))
    y = np.concatenate((y1, y2))
    return X, y

# Generate the spiral data
X, y = generate_spiral(200, noise=0.5)

# Plot the data
plt.figure(figsize=(8, 6))
plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], color='blue', label='Class 0')
plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], color='red', label='Class 1')
plt.title("Spiral Dataset")
plt.xlabel("X1")
plt.ylabel("X2")
plt.legend()
plt.grid(True)
plt.axis('equal')
plt.show()

# 2. Model a Regression
#======================================

import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression

# Spiral data function (as before)
def generate_spiral(n_points, noise=0.5):
    n = np.sqrt(np.random.rand(n_points)) * 780 * (2*np.pi)/360
    d1x = -np.cos(n)*n + np.random.rand(n_points) * noise
    d1y = np.sin(n)*n + np.random.rand(n_points) * noise
    X1 = np.vstack((d1x, d1y)).T
    y1 = np.zeros(n_points)

    d2x = np.cos(n)*n + np.random.rand(n_points) * noise
    d2y = -np.sin(n)*n + np.random.rand(n_points) * noise
    X2 = np.vstack((d2x, d2y)).T
    y2 = np.ones(n_points)

    X = np.vstack((X1, X2))
    y = np.concatenate((y1, y2))
    return X, y

# Generate data
X, y = generate_spiral(200, noise=0.5)

# Fit a Linear classifier
clf = LogisticRegression()
clf.fit(X, y)

# Plot decision boundary
def plot_decision_boundary(clf, X, y):
    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 300),
                         np.linspace(y_min, y_max, 300))
    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)

    plt.figure(figsize=(8, 6))
    plt.contourf(xx, yy, Z, alpha=0.3, cmap='coolwarm')
    plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], c='blue', label='Class 0')
    plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], c='red', label='Class 1')
    plt.title("Linear Classifier on Spiral Dataset (Logistic Regression)")
    plt.xlabel("X1")
    plt.ylabel("X2")
    plt.legend()
    plt.grid(True)
    plt.axis('equal')
    plt.show()

plot_decision_boundary(clf, X, y)

# 3.RBF Kernel Visualization
#======================================

import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

# Generate 2D spiral data
def generate_spiral(n_points, noise=0.5):
    n = np.sqrt(np.random.rand(n_points)) * 780 * (2*np.pi)/360
    d1x = -np.cos(n)*n + np.random.rand(n_points) * noise
    d1y = np.sin(n)*n + np.random.rand(n_points) * noise
    X1 = np.vstack((d1x, d1y)).T
    y1 = np.zeros(n_points)

    d2x = np.cos(n)*n + np.random.rand(n_points) * noise
    d2y = -np.sin(n)*n + np.random.rand(n_points) * noise
    X2 = np.vstack((d2x, d2y)).T
    y2 = np.ones(n_points)

    X = np.vstack((X1, X2))
    y = np.concatenate((y1, y2))
    return X, y

# RBF kernel lifting: z = exp(-gamma * ||x - center||^2)
def rbf_lift(X, gamma=0.1, center=np.array([0, 0])):
    diff = X - center
    squared_dist = np.sum(diff**2, axis=1)
    z = np.exp(-gamma * squared_dist)
    return z

# Generate and transform
X, y = generate_spiral(200, noise=0.5)
Z = rbf_lift(X, gamma = 0.05)

# Plot in 3D
fig = plt.figure(figsize=(10, 7))
ax = fig.add_subplot(111, projection='3d')

ax.scatter(X[y == 0][:, 0], X[y == 0][:, 1], Z[y == 0], c='blue', label='Class 0')
ax.scatter(X[y == 1][:, 0], X[y == 1][:, 1], Z[y == 1], c='red', label='Class 1')

ax.set_title("RBF Kernel Lift to Higher Dimension")
ax.set_xlabel("X1")
ax.set_ylabel("X2")
ax.set_zlabel("RBF Value (Z)")
ax.legend()
plt.tight_layout()
plt.show()

# 4. Apply this on spiral data
#======================================

import numpy as np
import matplotlib.pyplot as plt
from sklearn.svm import SVC
from sklearn.preprocessing import StandardScaler

# Spiral data generator
def generate_spiral(n_points, noise=0.5):
    n = np.sqrt(np.random.rand(n_points)) * 780 * (2*np.pi)/360
    d1x = -np.cos(n)*n + np.random.rand(n_points) * noise
    d1y = np.sin(n)*n + np.random.rand(n_points) * noise
    X1 = np.vstack((d1x, d1y)).T
    y1 = np.zeros(n_points)

    d2x = np.cos(n)*n + np.random.rand(n_points) * noise
    d2y = -np.sin(n)*n + np.random.rand(n_points) * noise
    X2 = np.vstack((d2x, d2y)).T
    y2 = np.ones(n_points)

    X = np.vstack((X1, X2))
    y = np.concatenate((y1, y2))
    return X, y

# Generate data
X, y = generate_spiral(200, noise=0.5)

# Standardize features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Fit SVM with RBF kernel
clf = SVC(kernel='rbf', gamma='auto')
clf.fit(X_scaled, y)

# Plot decision boundary
def plot_svm_decision_boundary(model, X, y):
    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 300),
                         np.linspace(y_min, y_max, 300))
    grid = np.c_[xx.ravel(), yy.ravel()]
    Z = model.predict(grid)
    Z = Z.reshape(xx.shape)

    plt.figure(figsize=(8, 6))
    plt.contourf(xx, yy, Z, alpha=0.3, cmap='coolwarm')
    plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], color='blue', label='Class 0')
    plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], color='red', label='Class 1')
    plt.title("SVM with RBF Kernel on Spiral Data")
    plt.xlabel("X1")
    plt.ylabel("X2")
    plt.legend()
    plt.grid(True)
    plt.axis('equal')
    plt.show()

plot_svm_decision_boundary(clf, X_scaled, y)

# 5. Visualize Margins and Support Vectors
#==========================================

import numpy as np
import matplotlib.pyplot as plt
from sklearn.svm import SVC
from sklearn.preprocessing import StandardScaler

# Spiral data generator
def generate_spiral(n_points, noise=0.5):
    n = np.sqrt(np.random.rand(n_points)) * 780 * (2*np.pi)/360
    d1x = -np.cos(n)*n + np.random.rand(n_points) * noise
    d1y = np.sin(n)*n + np.random.rand(n_points) * noise
    X1 = np.vstack((d1x, d1y)).T
    y1 = np.zeros(n_points)

    d2x = np.cos(n)*n + np.random.rand(n_points) * noise
    d2y = -np.sin(n)*n + np.random.rand(n_points) * noise
    X2 = np.vstack((d2x, d2y)).T
    y2 = np.ones(n_points)

    X = np.vstack((X1, X2))
    y = np.concatenate((y1, y2))
    return X, y

# Generate and scale data
X, y = generate_spiral(200, noise=0.5)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Train SVM with RBF kernel
clf = SVC(kernel='rbf', gamma='auto')
clf.fit(X_scaled, y)

# Plot with support vectors and margins
def plot_svm_rbf_with_margins(model, X, y):
    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 500),
                         np.linspace(y_min, y_max, 500))
    grid = np.c_[xx.ravel(), yy.ravel()]
    Z = model.decision_function(grid)
    Z = Z.reshape(xx.shape)

    plt.figure(figsize=(9, 7))
    # Decision boundary
    plt.contour(xx, yy, Z, levels=[0], linewidths=2, colors='black')
    # Margins
    plt.contour(xx, yy, Z, levels=[-1, 1], linestyles='dashed', colors='grey')
    # Support vectors
    plt.scatter(model.support_vectors_[:, 0], model.support_vectors_[:, 1],
                s=100, linewidth=1, facecolors='none', edgecolors='k', label='Support Vectors')

    # Data points
    plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], color='blue', label='Class 0')
    plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], color='red', label='Class 1')

    plt.title("SVM (RBF Kernel) with Support Vectors and Margins")
    plt.xlabel("X1 (scaled)")
    plt.ylabel("X2 (scaled)")
    plt.legend()
    plt.grid(True)
    plt.axis('equal')
    plt.show()

plot_svm_rbf_with_margins(clf, X_scaled, y)

# 6. Evaluate the model
#======================================

from sklearn.metrics import classification_report, accuracy_score

# Predict on training data
y_pred = clf.predict(X_scaled)

# Accuracy
acc = accuracy_score(y, y_pred)
print(f"Accuracy: {acc:.4f}")

# Precision, Recall, F1-Score
print("\nClassification Report:")
print(classification_report(y, y_pred, target_names=["Class 0", "Class 1"]))

# SMS Spam Colleection DataSet
# Link: https://archive.ics.uci.edu/dataset/228/sms+spam+collection

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.svm import SVC
from sklearn.metrics import classification_report, accuracy_score
import urllib.request
import zipfile
import os

# Download dataset zip if not exists
dataset_url = "https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip"
dataset_path = "smsspamcollection.zip"
dataset_folder = "smsspamcollection"

if not os.path.exists(dataset_path):
    print("Downloading dataset...")
    urllib.request.urlretrieve(dataset_url, dataset_path)

if not os.path.exists(dataset_folder):
    print("Extracting dataset...")
    with zipfile.ZipFile(dataset_path, 'r') as zip_ref:
        zip_ref.extractall(dataset_folder)

# Load dataset
data_file = os.path.join(dataset_folder, "SMSSpamCollection")
df = pd.read_csv(data_file, sep='\t', header=None, names=["label", "message"])

# Encode labels: ham -> 0, spam -> 1
df['label_num'] = df.label.map({'ham': 0, 'spam': 1})

# Split data
X_train, X_test, y_train, y_test = train_test_split(
    df['message'], df['label_num'], test_size=0.2, random_state=42)

# Text vectorization using TF-IDF
vectorizer = TfidfVectorizer(stop_words='english', max_df=0.9)
X_train_tfidf = vectorizer.fit_transform(X_train)
X_test_tfidf = vectorizer.transform(X_test)

# Train Linear SVM
clf = SVC(kernel='linear', C=1)
clf.fit(X_train_tfidf, y_train)

# Predictions
y_pred = clf.predict(X_test_tfidf)

# Evaluation
print("Accuracy:", accuracy_score(y_test, y_pred))
print("\nClassification Report:")
print(classification_report(y_test, y_pred, target_names=['Ham', 'Spam']))

# Exercise
# Pima Indians Diabetes Database
# Link: https://www.kaggle.com/datasets/uciml/pima-indians-diabetes-database
# Exercise Link: https://docs.google.com/spreadsheets/d/1ibWrg2PTpvbQ1PwQvk-rjqEDgYYm6i5vV2D-C9TfHeI/edit?gid=0#gid=0


import pandas as pd
import numpy as np
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score
import matplotlib.pyplot as plt

# Load dataset
url = "https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv"
cols = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness',
        'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age', 'Outcome']
df = pd.read_csv(url, header=None, names=cols)

X = df[['BloodPressure', 'Insulin']].values
y = df['Outcome'].values

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train linear SVM
clf = SVC(kernel='rbf')
clf.fit(X_train, y_train)

# Predict on test set
y_pred = clf.predict(X_test)

# Print accuracy and classification report
print("Accuracy:", accuracy_score(y_test, y_pred))
print("\nClassification Report:")
print(classification_report(y_test, y_pred))

# Prepare grid for decision boundary
fig, axes = plt.subplots(1, 2, figsize=(14, 6))

for ax, show_sv in zip(axes, [False, True]):
    # Plot training points
    ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap='bwr', edgecolors='k', s=50)

    if show_sv:
        # Plot support vectors
        sv = clf.support_vectors_
        ax.scatter(sv[:, 0], sv[:, 1], s=150, facecolors='none', edgecolors='k', label='Support Vectors')

    # Decision boundary + margins
    xlim = ax.get_xlim()
    ylim = ax.get_ylim()
    xx = np.linspace(xlim[0], xlim[1], 200)
    yy = np.linspace(ylim[0], ylim[1], 200)
    YY, XX = np.meshgrid(yy, xx)
    xy = np.vstack([XX.ravel(), YY.ravel()]).T
    Z = clf.decision_function(xy).reshape(XX.shape)

    # Plot decision boundary and margins
    ax.contour(XX, YY, Z, colors='k', levels=[-1, 0, 1], alpha=0.5,
               linestyles=['--', '-', '--'])

    ax.set_xlabel('Parameter 1')
    ax.set_ylabel('Parameter 2')
    title = 'With Support Vectors' if show_sv else 'Without Support Vectors'
    ax.set_title(f'SVM Decision Boundary and Margins\n{title}')
    if show_sv:
        ax.legend()

plt.tight_layout()
plt.show()

"""**Metrics and Clustering Revision**


Sheet Link:

https://docs.google.com/spreadsheets/d/1vjT_HV30xx-DUS30IzVh9maqagWZvJJAMqf6dUPp4S8/edit?gid=1343566680#gid=1343566680
"""

from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score
import numpy as np

# Sample data (binary classification)
y_true = [1, 0, 1, 1, 0, 1, 0, 0, 1, 0]  # Ground truth labels
y_pred = [1, 0, 1, 0, 0, 1, 1, 0, 0, 0]  # Predicted labels
#tp = 3, tn = 4, fn = 2, fp = 1

# Build confusion matrix
cm = confusion_matrix(y_true, y_pred)
tn, fp, fn, tp = cm.ravel()

print("Confusion Matrix:")
print(cm)
print(f"TP={tp}, FP={fp}, FN={fn}, TN={tn}")

# Manually compute metrics
# Manually compute metrics
accuracy = (tp + tn) / (tp + tn + fp + fn)
precision = tp / (tp + fp)
recall = tp / (tp + fn)
f1 = 2 * (precision * recall) / (precision + recall)

print("\nManual Calculations:")
print(f"Accuracy: {accuracy:.2f}")
print(f"Precision: {precision:.2f}")
print(f"Recall: {recall:.2f}")
print(f"F1 Score: {f1:.2f}")

# Using sklearn
print("\nUsing sklearn:")
print(f"Accuracy: {accuracy_score(y_true, y_pred):.2f}")
print(f"Precision: {precision_score(y_true, y_pred):.2f}")
print(f"Recall: {recall_score(y_true, y_pred):.2f}")
print(f"F1 Score: {f1_score(y_true, y_pred):.2f}")

import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_blobs
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score, davies_bouldin_score

# Generate synthetic 2D data
X, _ = make_blobs(n_samples = 100)

# Run KMeans
k = 4
kmeans = KMeans(n_clusters=k)
labels = kmeans.fit_predict(X)

# Compute metrics
sil_score = silhouette_score(X, labels)
db_score = davies_bouldin_score(X, labels)

# Plotting clusters
plt.figure(figsize=(8, 5))
plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', s=50)
plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], c='red', marker='X', s=200, label='Centroids')
plt.title("K-Means Clustering")
plt.legend()
plt.show()

# KMeans for Spotify Data
# Link for dataset: https://www.kaggle.com/datasets/maharshipandya/-spotify-tracks-dataset


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score

# Load dataset
df = pd.read_csv("dataset.csv")

# Select audio features for clustering
features = ['danceability', 'energy', 'speechiness', 'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo']
X = df[features].dropna()

# Standardize the data
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Try different values of k
inertias = []
sil_scores = []
K = range(2, 11)

for k in K:
    kmeans = KMeans(n_clusters=k, random_state=42)
    labels = kmeans.fit_predict(X_scaled)
    inertias.append(kmeans.inertia_)
    sil_scores.append(silhouette_score(X_scaled, labels))

# Plot Elbow & Silhouette
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.plot(K, inertias, 'bo-')
plt.title("Elbow Method")
plt.xlabel("No. of Clusters (k)")
plt.ylabel("Inertia")

plt.subplot(1, 2, 2)
plt.plot(K, sil_scores, 'go-')
plt.title("Silhouette Score")
plt.xlabel("No. of Clusters (k)")
plt.ylabel("Score")

plt.tight_layout()
plt.show()

import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_circles
from sklearn.cluster import KMeans, DBSCAN
from sklearn.preprocessing import StandardScaler

# Generate non-linear circular data
X, _ = make_circles(n_samples=500, factor=0.5, noise=0.9, random_state=0)

# Scale the data
X_scaled = StandardScaler().fit_transform(X)

# Apply K-Means
kmeans = KMeans(n_clusters=2, random_state=42)
kmeans_labels = kmeans.fit_predict(X_scaled)

# Apply DBSCAN
dbscan = DBSCAN(eps=0.3, min_samples=5)
dbscan_labels = dbscan.fit_predict(X_scaled)

# Plot results
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=kmeans_labels, cmap='viridis', s=40)
plt.title("K-Means")

plt.subplot(1, 2, 2)
plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=dbscan_labels, cmap='plasma', s=40)
plt.title("DBSCAN")

plt.tight_layout()
plt.show()

# DBSCAN on Digits Dataset
from sklearn.datasets import load_digits
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import DBSCAN
import matplotlib.pyplot as plt

digits = load_digits()
X = digits.data

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Reduce to 2D with PCA
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)

dbscan = DBSCAN(eps=0.3, min_samples=5)
labels = dbscan.fit_predict(X_pca)

plt.scatter(X_pca[:, 0], X_pca[:, 1], c=labels, cmap='tab10', edgecolor='k')
plt.title('DBSCAN Clustering on Digits Dataset (PCA reduced)')
plt.xlabel('PCA 1')
plt.ylabel('PCA 2')
plt.show()

"""**Final Exercise**


For the given scenarios, identify the right technique. Write your name and technique.


Link:
https://docs.google.com/spreadsheets/d/1RIebaSeYRxVevw5YyWy0rbrOnvnwPOOnMZaLZMsNUzY/edit?gid=0#gid=0
"""